
@misc{__,
  howpublished = {https://www.stata.com/meeting/italy14/abstracts/materials/it14\_haghish.pdf}
}

@misc{_Figueiredo_2021,
  title = {Figueiredo, {{Martinovic}}, {{Rees}}, and {{Licata}}: {{Collective Memories}} and {{Present}}-{{Day Intergroup Relations}}: {{Introduction}} to the {{Special Thematic Section}}},
  shorttitle = {Figueiredo, {{Martinovic}}, {{Rees}}, and {{Licata}}},
  year = {2021},
  month = jun,
  doi = {10.5964/jspp.v5i2.895},
  howpublished = {https://jspp.psychopen.eu/index.php/jspp/article/download/4995/4995.html?inline=1}
}

@misc{_impact_2021,
  title = {The Impact of a Multicultural Exchange between Indigenous and Non-Indigenous History Teachers for Students' Attitudes: Preliminary Evidence from a Pilot Study in {{Chile}}: {{Multicultural Education Review}}: {{Vol}} 12, {{No}} 3},
  year = {2021},
  month = jun,
  howpublished = {https://www.tandfonline.com/doi/abs/10.1080/2005615X.2020.1808927}
}

@misc{_Increasing_,
  title = {Increasing the {{Credibility}} of {{Political Science Research}}: {{A Proposal}} for {{Journal Reforms}}-{{Web}} of {{Science Core Collection}}},
  howpublished = {https://www-webofscience-com.uchile.idm.oclc.org/wos/woscc/full-record/WOS:000359291900014}
}

@misc{_Retraction_,
  title = {Retraction {{Watch}}},
  journal = {Retraction Watch},
  abstract = {Tracking retractions as a window into the scientific process},
  howpublished = {https://retractionwatch.com/},
  language = {en-US}
}

@misc{_Web_,
  title = {Web of {{Science}}},
  howpublished = {https://www-webofscience-com.uchile.idm.oclc.org/wos/}
}

@article{abdill_tracking_2019,
  title = {Tracking the Popularity and Outcomes of All {{bioRxiv}} Preprints},
  author = {Abdill, Richard J. and Blekhman, Ran},
  year = {2019},
  journal = {bioRxiv},
  pages = {515643},
  doi = {10.1101/515643},
  abstract = {Researchers in the life sciences are posting work to preprint servers at an unprecedented and increasing rate, sharing papers online before (or instead of) publication in peer-reviewed journals. Though the increasing acceptance of preprints is driving policy changes for journals and funders, there is little information about their usage. Here, we collected and analyzed data on all 37,648 preprints uploaded to bioRxiv.org, the largest biology-focused preprint server, in its first five years. We find preprints are being downloaded more than ever before (1.1 million tallied in October 2018 alone) and that the rate of preprints being posted has increased to a recent high of 2,100 per month. We also find that two-thirds of preprints posted before 2017 were later published in peer-reviewed journals, and find a relationship between journal impact factor and preprint downloads. Lastly, we developed Rxivist.org, a web application providing multiple ways of interacting with preprint metadata.}
}

@book{abrilruiz_Manzanas_2019,
  title = {{Manzanas podridas: Malas pr\'acticas de investigaci\'on y ciencia descuidada}},
  shorttitle = {{Manzanas podridas}},
  author = {Abril Ruiz, Angel},
  year = {2019},
  isbn = {978-1-07-075536-6},
  language = {Spanish},
  annotation = {OCLC: 1120499121}
}

@article{aczel_consensusbased_2020,
  title = {A Consensus-Based Transparency Checklist},
  author = {Aczel, Balazs and Szaszi, Barnabas and Sarafoglou, Alexandra and Kekecs, Zoltan and Kucharsk{\'y}, {\v S}imon and Benjamin, Daniel and Chambers, Christopher D. and Fisher, Agneta and Gelman, Andrew and Gernsbacher, Morton A. and Ioannidis, John P. and Johnson, Eric and Jonas, Kai and Kousta, Stavroula and Lilienfeld, Scott O. and Lindsay, D. Stephen and Morey, Candice C. and Munaf{\`o}, Marcus and Newell, Benjamin R. and Pashler, Harold and Shanks, David R. and Simons, Daniel J. and Wicherts, Jelte M. and Albarracin, Dolores and Anderson, Nicole D. and Antonakis, John and Arkes, Hal R. and Back, Mitja D. and Banks, George C. and Beevers, Christopher and Bennett, Andrew A. and Bleidorn, Wiebke and Boyer, Ty W. and Cacciari, Cristina and Carter, Alice S. and Cesario, Joseph and Clifton, Charles and Conroy, Ron{\'a}n M. and Cortese, Mike and Cosci, Fiammetta and Cowan, Nelson and Crawford, Jarret and Crone, Eveline A. and Curtin, John and Engle, Randall and Farrell, Simon and Fearon, Pasco and Fichman, Mark and Frankenhuis, Willem and Freund, Alexandra M. and Gaskell, M. Gareth and {Giner-Sorolla}, Roger and Green, Don P. and Greene, Robert L. and Harlow, Lisa L. and {de la Guardia}, Fernando Hoces and Isaacowitz, Derek and Kolodner, Janet and Lieberman, Debra and Logan, Gordon D. and Mendes, Wendy B. and Moersdorf, Lea and Nyhan, Brendan and Pollack, Jeffrey and Sullivan, Christopher and Vazire, Simine and Wagenmakers, Eric-Jan},
  year = {2020},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {4},
  number = {1},
  pages = {4--6},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0772-6},
  abstract = {We present a consensus-based checklist to improve and document the transparency of research reports in social and behavioural research. An accompanying online application allows users to complete the form and generate a report that they can submit with their manuscript or post to a public repository.},
  copyright = {2019 The Author(s)},
  language = {en},
  keywords = {forrt,herramienta}
}

@misc{agencianacionaldeinvestigacionydesarrollo_consulta_2020,
  title = {Consulta {{P\'ublica}}: {{Pol\'itica Acceso Abierto}} a {{Informaci\'on Cient\'ifica}}},
  author = {{Agencia Nacional de Investigaci{\'o}n y Desarrollo}, (ANID)},
  year = {2020}
}

@article{agnoli_Questionable_2017,
  title = {Questionable Research Practices among Italian Research Psychologists},
  author = {Agnoli, Franca and Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Albiero, Paolo and Cubelli, Roberto},
  year = {2017},
  month = mar,
  journal = {PLOS ONE},
  volume = {12},
  number = {3},
  pages = {e0172792},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0172792},
  abstract = {A survey in the United States revealed that an alarmingly large percentage of university psychologists admitted having used questionable research practices that can contaminate the research literature with false positive and biased findings. We conducted a replication of this study among Italian research psychologists to investigate whether these findings generalize to other countries. All the original materials were translated into Italian, and members of the Italian Association of Psychology were invited to participate via an online survey. The percentages of Italian psychologists who admitted to having used ten questionable research practices were similar to the results obtained in the United States although there were small but significant differences in self-admission rates for some QRPs. Nearly all researchers (88\%) admitted using at least one of the practices, and researchers generally considered a practice possibly defensible if they admitted using it, but Italian researchers were much less likely than US researchers to consider a practice defensible. Participants' estimates of the percentage of researchers who have used these practices were greater than the self-admission rates, and participants estimated that researchers would be unlikely to admit it. In written responses, participants argued that some of these practices are not questionable and they have used some practices because reviewers and journals demand it. The similarity of results obtained in the United States, this study, and a related study conducted in Germany suggest that adoption of these practices is an international phenomenon and is likely due to systemic features of the international research and publication processes.},
  language = {en},
  keywords = {Behavior,Experimental psychology,Italian people,Psychologists,Psychology,Psychometrics,Questionnaires,United States}
}

@article{allen_Open_2019,
  title = {Open Science Challenges, Benefits and Tips in Early Career and Beyond},
  author = {Allen, Christopher and Mehler, David M. A.},
  year = {2019},
  month = may,
  journal = {PLOS Biology},
  volume = {17},
  number = {5},
  pages = {e3000246},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3000246},
  abstract = {The movement towards open science is a consequence of seemingly pervasive failures to replicate previous research. This transition comes with great benefits but also significant challenges that are likely to affect those who carry out the research, usually early career researchers (ECRs). Here, we describe key benefits, including reputational gains, increased chances of publication, and a broader increase in the reliability of research. The increased chances of publication are supported by exploratory analyses indicating null findings are substantially more likely to be published via open registered reports in comparison to more conventional methods. These benefits are balanced by challenges that we have encountered and that involve increased costs in terms of flexibility, time, and issues with the current incentive structure, all of which seem to affect ECRs acutely. Although there are major obstacles to the early adoption of open science, overall open science practices should benefit both the ECR and improve the quality of research. We review 3 benefits and 3 challenges and provide suggestions from the perspective of ECRs for moving towards open science practices, which we believe scientists and institutions at all levels would do well to consider.},
  language = {en},
  keywords = {Careers,Experimental design,Neuroimaging,Open data,Open science,Peer review,Reproducibility,Statistical data}
}

@article{allison_Reproducibility_2018,
  title = {Reproducibility of Research: {{Issues}} and Proposed Remedies},
  shorttitle = {Reproducibility of Research},
  author = {Allison, David B. and Shiffrin, Richard M. and Stodden, Victoria},
  year = {2018},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {11},
  pages = {2561--2562}
}

@book{alperin_indicadores_2014,
  title = {{Indicadores de acceso abierto y comunicaciones acad\'emicas en Am\'erica Latina}},
  shorttitle = {{Indicadores de AA}},
  author = {Alperin, Juan Pablo and Babini, Dominique and Fischman, Gustavo},
  year = {2014},
  edition = {Juan Pablo Alperin},
  volume = {1},
  publisher = {{Consejo Latinoamericano de Ciencias Sociales, CLACSO}},
  address = {{Buenos Aires, Argentina}},
  abstract = {El mundo hoy en d\'ia cuenta con frases como ``muerte de la distancia'' lo que sugiere que la distancia ya no es un factor limitante en la capacidad de las personas para interactuar y comunicarse. Otro aforismo es que el mundo est\'a ``aplanado'' en t\'erminos de oportunidades, que son facilitadas por el avance de las Tecnolog\'ias de la Comunicaci\'on y de la Informaci\'on (TIC) que han permitido la convergencia de los consorcios y recursos de conocimiento de todo el mundo. A medida que las sociedades se van transformando, los paisajes del conocimiento y su interacci\'on dentro y entre las sociedades tambi\'en est\'an cambiando.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  isbn = {978-987-722-042-1},
  language = {es}
}

@article{an_Crisis_2018,
  title = {The {{Crisis}} of {{Reproducibility}}, the {{Denominator Problem}} and the {{Scientific Role}} of {{Multi}}-Scale {{Modeling}}},
  author = {An, Gary},
  year = {2018},
  month = dec,
  journal = {Bulletin of Mathematical Biology},
  volume = {80},
  number = {12},
  pages = {3071--3080},
  issn = {1522-9602},
  doi = {10.1007/s11538-018-0497-0},
  abstract = {The ``Crisis of Reproducibility'' has received considerable attention both within the scientific community and without. While factors associated with scientific culture and practical practice are most often invoked, I propose that the Crisis of Reproducibility is ultimately a failure of generalization with a fundamental scientific basis in the methods used for biomedical research. The Denominator Problem describes how limitations intrinsic to the two primary approaches of biomedical research, clinical studies and preclinical experimental biology, lead to an inability to effectively characterize the full extent of biological heterogeneity, which compromises the task of generalizing acquired knowledge. Drawing on the example of the unifying role of theory in the physical sciences, I propose that multi-scale mathematical and dynamic computational models, when mapped to the modular structure of biological systems, can serve a unifying role as formal representations of what is conserved and similar from one biological context to another. This ability to explicitly describe the generation of heterogeneity from similarity addresses the Denominator Problem and provides a scientific response to the Crisis of Reproducibility.},
  language = {en},
  keywords = {crisis}
}

@article{andrea_Why_2018,
  title = {Why Science's Crisis Should Not Become a Political Battling Ground},
  author = {Andrea, Saltelli},
  year = {2018},
  month = dec,
  journal = {Futures},
  volume = {104},
  pages = {85--90},
  issn = {0016-3287},
  doi = {10.1016/j.futures.2018.07.006},
  abstract = {A science war is in full swing which has taken science's reproducibility crisis as a battleground. While conservatives and corporate interests use the crisis to weaken regulations, their opponent deny the existence of a science's crisis altogether. Thus, for the conservative National Association of Scholars NAS the crisis is real and due to the progressive assault on higher education with ideologies such as ``neo-Marxism, radical feminism, historicism, post-colonialism, deconstructionism, post-modernism, liberation theology''. In the opposite field, some commentators claim that there is no crisis in science and that saying the opposite is irresponsible. These positions are to be seen in the context of the ongoing battle against regulation, of which the new rules proposed at the US Environmental Protection Agency (EPA) are but the last chapter. In this optic, Naomi Oreskes writes on Nature that what constitutes the crisis is the conservatives' attack on science. This evident right-left divide in the reading of the crisis is unhelpful and dangerous to the survival of science itself. An alternative reading ignored by the contendents would suggest that structural contradictions have emerged in modern science, and that addressing these should be the focus of our attention.},
  language = {en},
  keywords = {crisis,Evidence-based policy,History and philosophy of science,Post-normal science,Science and technology studies,Science’s crisis,Science’s reproducibility,Science’s war,Scientism}
}

@article{angell_Publish_1986,
  title = {Publish or {{Perish}}: {{A Proposal}}},
  shorttitle = {Publish or {{Perish}}},
  author = {Angell, Marcia},
  year = {1986},
  month = feb,
  journal = {Annals of Internal Medicine},
  volume = {104},
  number = {2},
  pages = {261--262},
  publisher = {{American College of Physicians}},
  issn = {0003-4819},
  doi = {10.7326/0003-4819-104-2-261},
  keywords = {institutional}
}

@misc{anid_propuesta_2020,
  title = {Propuesta de {{Pol\'itica}} de Acceso Abierto a La Informaci\'on Cient\'ifica y Adatos de Investigaci\'onfinanciados Con Fondos P\'ublicos de La {{ANID}}},
  author = {ANID},
  year = {2020},
  publisher = {{ANID}}
}

@article{anvari_replicability_2018,
  title = {The Replicability Crisis and Public Trust in Psychological Science},
  author = {Anvari, Farid and Lakens, Dani{\"e}l},
  year = {2018},
  month = sep,
  journal = {Comprehensive Results in Social Psychology},
  volume = {3},
  number = {3},
  pages = {266--286},
  publisher = {{Routledge}},
  issn = {2374-3603},
  doi = {10.1080/23743603.2019.1684822},
  abstract = {Replication failures of past findings in several scientific disciplines, including psychology, medicine, and experimental economics, have created a ``crisis of confidence'' among scientists. Psychological science has been at the forefront of tackling these issues, with discussions about replication failures and scientific self-criticisms of questionable research practices (QRPs) increasingly taking place in public forums. How this replicability crisis impacts the public's trust is a question yet to be answered by research. Whereas some researchers believe that the public's trust will be positively impacted or maintained, others believe trust will be diminished. Because it is our field of expertise, we focus on trust in psychological science. We performed a study testing how public trust in past and future psychological research would be impacted by being informed about (i) replication failures (replications group), (ii) replication failures and criticisms of QRPs (QRPs group), and (iii) replication failures, criticisms of QRPs, and proposed reforms (reforms group). Results from a mostly European sample (N = 1129) showed that, compared to a control group, people in the replications, QRPs, and reforms groups self-reported less trust in past research. Regarding trust in future research, the replications and QRPs groups did not significantly differ from the control group. Surprisingly, the reforms group had less trust in future research than the control group. Nevertheless, people in the replications, QRPs, and reforms groups did not significantly differ from the control group in how much they believed future research in psychological science should be supported by public funding. Potential explanations are discussed.},
  keywords = {crisis of confidence,open science,Replicability crisis,reproducibility crisis,trust in science},
  annotation = {\_eprint: https://doi.org/10.1080/23743603.2019.1684822}
}

@article{armeni_widescale_2021,
  title = {Towards Wide-Scale Adoption of Open Science Practices: {{The}} Role of Open Science Communities},
  shorttitle = {Towards Wide-Scale Adoption of Open Science Practices},
  author = {Armeni, Kristijan and Brinkman, Loek and Carlsson, Rickard and Eerland, Anita and Fijten, Rianne and Fondberg, Robin and Heininga, Vera E and Heunis, Stephan and Koh, Wei Qi and Masselink, Maurits and Moran, Niall and Baoill, Andrew {\'O} and Sarafoglou, Alexandra and Schettino, Antonio and Schwamm, Hardy and Sjoerds, Zsuzsika and Teperek, Marta and {van den Akker}, Olmo R and {van't Veer}, Anna and {Zurita-Milla}, Raul},
  year = {2021},
  month = jul,
  journal = {Science and Public Policy},
  number = {scab039},
  issn = {0302-3427},
  doi = {10.1093/scipol/scab039},
  abstract = {Despite the increasing availability of Open Science (OS) infrastructure and the rise in policies to change behaviour, OS practices are not yet the norm. While pioneering researchers are developing OS practices, the majority sticks to status quo. To transition to common practice, we must engage a critical proportion of the academic community. In this transition, OS Communities (OSCs) play a key role. OSCs are bottom-up learning groups of scholars that discuss OS within and across disciplines. They make OS knowledge more accessible and facilitate communication among scholars and policymakers. Over the past two years, eleven OSCs were founded at several Dutch university cities. In other countries, similar OSCs are starting up. In this article, we discuss the pivotal role OSCs play in the large-scale transition to OS. We emphasize that, despite the grassroot character of OSCs, support from universities is critical for OSCs to be viable, effective, and sustainable.}
}

@inproceedings{babini_universidades_2014,
  title = {{Universidades y acceso abierto: hora de tomar protagonismo}},
  booktitle = {{Foro Revista Iberoamericana de Ciencia, Tecnolog\'ia y Sociedad}},
  author = {Babini, Dominique},
  year = {2014},
  pages = {1--3},
  publisher = {{2015}},
  abstract = {Las universidades est\'an en condiciones tener mayor protagonismo en la construcci\'on de un acceso abierto global cooperativo no comercial, sustentable e inclusivo. Pueden: desarrollar sus propios portales con las revistas que publica cada universidad, crear repositorios digitales institucionales que reflejen la propia producci\'on cient\'ifica y acad\'emica de cada instituci\'on disponible gratis en texto completo, participar activamente en los sistemas nacionales de repositorios de sus pa\'ises, aportar una revisi\'on cr\'itica de las actuales modalidades de evaluaci\'on de la investigaci\'on.},
  language = {es}
}

@article{baker_500_2016,
  title = {1,500 Scientists Lift the Lid on Reproducibility},
  author = {Baker, Monya},
  year = {2016},
  month = may,
  journal = {Nature},
  volume = {533},
  number = {7604},
  pages = {452--454},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/533452a},
  abstract = {Survey sheds light on the `crisis' rocking research.},
  copyright = {2016 Nature Publishing Group},
  language = {en},
  keywords = {crisis}
}

@misc{bakker_Ensuring_2018,
  title = {Ensuring the Quality and Specificity of Preregistrations},
  author = {Bakker, Marjan and Veldkamp, Coosje Lisabet Sterre and van Assen, Marcel A. L. M. and Crompvoets, Elise Anne Victoire and Ong, How Hwee and Nosek, Brian A. and Soderberg, Courtney K. and Mellor, David Thomas and Wicherts, Jelte},
  year = {2018},
  month = sep,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/cdgyh},
  abstract = {Researchers face many, often seemingly arbitrary choices in formulating hypotheses, designing protocols, collecting data, analyzing data, and reporting results. Opportunistic use of `researcher degrees of freedom' aimed at obtaining statistical significance increases the likelihood of obtaining and publishing false positive results and overestimated effect sizes. Preregistration is a mechanism for reducing such degrees of freedom by specifying designs and analysis plans before observing the research outcomes. The effectiveness of preregistration may depend, in part, on whether the process facilitates sufficiently specific articulation of such plans. In this preregistered study, we compared two formats of preregistration available on the OSF: Standard Pre-Data Collection Registration and Prereg Challenge registration (now called ``OSF Preregistration'', http://osf.io/prereg/). The Prereg Challenge format was a structured workflow with detailed instructions, and an independent review to confirm completeness; the ``Standard'' format was unstructured with minimal direct guidance to give researchers flexibility for what to pre-specify. Results of comparing random samples of 53 preregistrations from each format indicate that the structured format restricted the opportunistic use of researcher degrees of freedom better (Cliff's Delta = 0.49) than the unstructured format, but neither eliminated all researcher degrees of freedom. We also observed very low concordance among coders about the number of hypotheses (14\%), indicating that they are often not clearly stated. We conclude that effective preregistration is challenging, and registration formats that provide effective guidance may improve the quality of research.},
  keywords = {Meta-science,preregistration,Quantitative Methods,Questionable research practices,researcher degrees of freedom,Social and Behavioral Sciences,Statistical Methods}
}

@misc{bakker_Questionable_2020,
  title = {Questionable and Open Research Practices: Attitudes and Perceptions among Quantitative Communication Researchers},
  shorttitle = {Questionable and Open Research Practices},
  author = {Bakker, Bert N. and Jaidka, Kokil and D{\"o}rr, Timothy and Fasching, Neil and Lelkes, Yphtach},
  year = {2020},
  month = nov,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/7uyn5},
  abstract = {Recent contributions have questioned the credibility of quantitative communication research. While questionable research practices are believed to be widespread, evidence for this claim is primarily derived from other disciplines. Before change in communication research can happen, it is important to document the extent to which QRPs are used and whether researchers are open to the changes proposed by the so-called open science agenda. We conducted a large survey among authors of papers published in the top-20 journals in communication science in the last ten years (N=1039). A non-trivial percent of researchers report using one or more QRPs. While QRPs are generally considered unacceptable, researchers perceive QRPs to be common among their colleagues. At the same time, we find optimism about the use of open science practices in communication research. We end with a series of recommendations outlining what journals, institutions and researchers can do moving forward.},
  keywords = {other,Psychology,Social and Behavioral Sciences}
}

@article{banzato_soberania_2019,
  title = {{Soberan\'ia del conocimiento para superar inequidades: pol\'iticas de Acceso Abierto para revistas cient\'ificas en Am\'erica Latina}},
  author = {Banzato, Guillermo},
  year = {2019},
  journal = {Mecila Working Paper Series},
  volume = {18},
  pages = {1--18},
  abstract = {Desde el comienzo de la era digital, determinadas pol\'iticas de gesti\'on de la ciencia  han incrementado las inequidades en las condiciones de producci\'on del conocimiento  y en las posibilidades de di\'alogo entre los colectivos de investigadores. A fines del  siglo XX y principios del XXI se inici\'o una reacci\'on en las m\'as prestigiosas bibliotecas  y comunidades cient\'ificas de Am\'erica del Norte y Europa Occidental, y Am\'erica Latina  comenz\'o  el  desarrollo  de  sistemas  de  visibilidad  propios,  al  tiempo  que  sucesivas   declaraciones  fueron  definiendo  al  Acceso  Abierto  como  estrategia  para  superar  tales inequidades. En esta direcci\'on, se han desarrollado revistas en Acceso Abierto  cuya  sustentabilidad  est\'a  siendo  puesta  a  prueba.  Este  trabajo  presenta  un  breve   estado de situaci\'on actualizado sobre algunos problemas que enfrentan los autores,  evaluadores y editores latinoamericanos en la gesti\'on y publicaci\'on de los resultados  de las investigaciones. Asimismo, en \'el se argumenta en pro del Acceso Abierto como  herramienta primordial para garantizar la soberan\'ia del conocimiento en el Sur Global,  y  se  sostiene  que  la  propuesta  colaborativa  para  la  construcci\'on  conjunta  de  un   sistema sustentable de edici\'on cient\'ifica en Acceso Abierto puede ayudar a superar  las inequidades en la producci\'on y difusi\'on del conocimiento latinoamericano},
  language = {es}
}

@article{barba_Terminologies_2018,
  title = {Terminologies for {{Reproducible Research}}},
  author = {Barba, Lorena A.},
  year = {2018},
  month = feb,
  journal = {arXiv:1802.03311 [cs]},
  eprint = {1802.03311},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Reproducible research---by its many names---has come to be regarded as a key concern across disciplines and stakeholder groups. Funding agencies and journals, professional societies and even mass media are paying attention, often focusing on the so-called "crisis" of reproducibility. One big problem keeps coming up among those seeking to tackle the issue: different groups are using terminologies in utter contradiction with each other. Looking at a broad sample of publications in different fields, we can classify their terminology via decision tree: they either, A---make no distinction between the words reproduce and replicate, or B---use them distinctly. If B, then they are commonly divided in two camps. In a spectrum of concerns that starts at a minimum standard of "same data+same methods=same results," to "new data and/or new methods in an independent study=same findings," group 1 calls the minimum standard reproduce, while group 2 calls it replicate. This direct swap of the two terms aggravates an already weighty issue. By attempting to inventory the terminologies across disciplines, I hope that some patterns will emerge to help us resolve the contradictions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Digital Libraries}
}

@article{becerrilgarcia_end_2019,
  title = {The {{End}} of a {{Centralized Open Access Project}} and the {{Beginning}} of a {{Community}}-{{Based Sustainable Infrastructure}} for {{Latin America}}},
  author = {Becerril Garc{\'i}a, Arianna and Aguado L{\'o}pez, Eduardo},
  year = {2019},
  journal = {OpenEdition Press},
  pages = {41--55},
  doi = {10.4000/books.oep. 9003.},
  abstract = {The Latin American region has an ecosystem where the nature of publication is conceived as the act of making public, of sharing, not as the publishing industry. International, national and institutional contexts have led to redefine a project\textemdash Redalyc.org\textemdash that began in 2003 and that has already fulfilled its original mission: give visibility to knowledge coming from Latin America and promote qualitative scientific journals. Nevertheless, it has to be transformed from a Latin American platform based in Mexico into a community- based regional infrastructure that continues assessing journals' quality and providing access to full-text, thus allowing visibility for journals and free access to knowledge. It is a framework that generates technology in favor of the empowerment and professionalization of journal editors, making sustainable the editorial task in open access so that Redalyc may sustain itself collectively. This work describes Redalyc's first model, presents the problematic in process and the new business model Redalyc is designing and adopting to operate.}
}

@misc{beigel_america_2021,
  type = {{Scientific Blog}},
  title = {{Am\'erica Latina podr\'ia convertirse en l\'ider mundial de la ciencia abierta no comercial}},
  author = {Beigel, Fernanda},
  year = {2021},
  journal = {The Conversation},
  abstract = {Para hacer frente a los retos del pr\'oximo siglo \textendash desde las pandemias hasta el cambio clim\'atico, pasando por la automatizaci\'on y el big data\textendash, la ciencia debe estar abierta a todas las personas del mundo. La ciudadan\'ia deben tener el mismo acceso a la informaci\'on que los investigadores, y estos necesitan acceder a repositorios de conocimiento de alta calidad e interconectados para avanzar en nuestra comprensi\'on del mundo que nos rodea.},
  copyright = {Creative Commons 3.0},
  language = {es}
}

@article{beigel_relaciones_2018,
  title = {{Las relaciones de poder en la ciencia mundial}},
  shorttitle = {{NUSO}},
  author = {Beigel, Fernanda},
  year = {2018},
  journal = {Nueva Sociedad},
  volume = {274},
  pages = {13--28},
  issn = {0251-3552},
  abstract = {Los rankings universitarios se crearon principalmente para intervenir en los flujos internacionales de estudiantes, pero se convirtieron progresivamente en una fuente directa para reforzar el prestigio de un peque\~no grupo de universidades, de sus principales revistas y editoriales oligop\'olicas. Su aplicaci\'on tiende a volver cada vez m\'as perif\'erica a la ciencia desarrollada en los espacios alejados del circuito mainstream o de corriente principal. Por eso es necesario crear nuevas herramientas de medici\'onde la producci\'on cient\'ifica de la periferia que contemplen las interacciones de sus universidades en sus distintas direcciones, y no solo con los circuitos dominantes.},
  language = {es}
}

@article{benjamin-chung_Internal_2020,
  title = {Internal Replication of Computational Workflows in Scientific Research},
  author = {{Benjamin-Chung}, Jade and Colford, Jr., John M. and Mertens, Andrew and Hubbard, Alan E. and Arnold, Benjamin F.},
  year = {2020},
  month = jun,
  journal = {Gates Open Research},
  volume = {4},
  pages = {17},
  issn = {2572-4754},
  doi = {10.12688/gatesopenres.13108.2},
  abstract = {Failures to reproduce research findings across scientific disciplines from psychology to physics have garnered increasing attention in recent years. External replication of published findings by outside investigators has emerged as a method to detect errors and bias in the published literature. However, some studies influence policy and practice before external replication efforts can confirm or challenge the original contributions. Uncovering and resolving errors before publication would increase the efficiency of the scientific process by increasing the accuracy of published evidence. Here we summarize the rationale and best practices for internal replication, a process in which multiple independent data analysts replicate an analysis and correct errors prior to publication. We explain how internal replication should reduce errors and bias that arise during data analyses and argue that it will be most effective when coupled with pre-specified hypotheses and analysis plans and performed with data analysts masked to experimental group assignments. By improving the reproducibility of published evidence, internal replication should contribute to more rapid scientific advances.},
  language = {en}
}

@article{benning_Registration_2019a,
  title = {The {{Registration Continuum}} in {{Clinical Science}}: {{A Guide Toward Transparent Practices}}},
  shorttitle = {The {{Registration Continuum}} in {{Clinical Science}}},
  author = {Benning, Stephen D. and Bachrach, Rachel L. and Smith, Edward A. and Freeman, Andrew J. and Wright, Aidan G. C.},
  year = {2019},
  month = aug,
  journal = {Journal of Abnormal Psychology},
  volume = {128},
  number = {6},
  pages = {528--540},
  publisher = {{Amer Psychological Assoc}},
  address = {{Washington}},
  issn = {0021-843X},
  doi = {10.1037/abn0000451},
  abstract = {Clinical scientists can use a continuum of registration efforts that vary in their disclosure and timing relative to data collection and analysis. Broadly speaking, registration benefits investigators by offering stronger, more powerful tests of theory with particular methods in tandem with better control of long-run false positive error rates. Registration helps clinical researchers in thinking through tensions between bandwidth and fidelity that surround recruiting participants, defining clinical phenotypes, handling comorbidity, treating missing data. and analyzing rich and complex data. In particular. registration helps record and justify the reasons behind specific study design decisions, though it also provides the opportunity to register entire decision trees with specific endpoints. Creating ever more faithful registrations and standard operating procedures may offer alternative methods of judging a clinical investigator's scientific skill and eminence because study registration increases the transparency of clinical researchers' work.},
  language = {English},
  keywords = {coregistration,credibility,disorders,flexibility,framework,postregistration,preregistration,psychopathology,registered-reports,symptoms,transparency},
  annotation = {WOS:000478024300006}
}

@article{bergh_there_2017,
  title = {Is There a Credibility Crisis in Strategic Management Research? {{Evidence}} on the Reproducibility of Study Findings},
  shorttitle = {Is There a Credibility Crisis in Strategic Management Research?},
  author = {Bergh, Donald D and Sharp, Barton M and Aguinis, Herman and Li, Ming},
  year = {2017},
  month = aug,
  journal = {Strategic Organization},
  volume = {15},
  number = {3},
  pages = {423--436},
  publisher = {{SAGE Publications}},
  issn = {1476-1270},
  doi = {10.1177/1476127017701076},
  abstract = {Recent studies report an inability to replicate previously published research, leading some to suggest that scientific knowledge is facing a credibility crisis. In this essay, we provide evidence on whether strategic management research may itself be vulnerable to these concerns. We conducted a study whereby we attempted to reproduce the empirical findings of 88 articles appearing in the Strategic Management Journal using data reported in the articles themselves. About 70\% of the studies did not disclose enough data to permit independent tests of reproducibility of their findings. Of those that could be retested, almost one-third reported hypotheses as statistically significant which were no longer so and far more significant results were found to be non-significant in the reproductions than in the opposite direction. Collectively, incomplete reporting practices, disclosure errors, and possible opportunism limit the reproducibility of most studies. Until disclosure standards and requirements change to include more complete reporting and facilitate tests of reproducibility, the strategic management field appears vulnerable to a credibility crisis.},
  language = {en},
  keywords = {crisis,knowledge credibility,replication,reproducibility}
}

@article{bergkvist_Preregistration_2020a,
  title = {Preregistration as a Way to Limit Questionable Research Practice in Advertising Research},
  author = {Bergkvist, Lars},
  year = {2020},
  month = oct,
  journal = {International Journal of Advertising},
  volume = {39},
  number = {7},
  pages = {1172--1180},
  publisher = {{Routledge Journals, Taylor \& Francis Ltd}},
  address = {{Abingdon}},
  issn = {0265-0487},
  doi = {10.1080/02650487.2020.1753441},
  abstract = {This paper discusses two phenomena that threaten the credibility of scientific research and suggests an approach to limiting the extent of their use in advertising research. HARKing (hypothesizing after the results are known) refers to when hypotheses are formulated or modified after the results of a study are known. P-hacking refers to various practices (e.g., adding respondents, introducing control variables) that increase the likelihood of obtaining statistically significant results from a study. Both of these practices increase the risk of false positives (Type I errors) in research results and it is in the interest of the advertising research field that they are limited. Voluntary preregistration, where researchers commit to and register their research design and analytical approach before conducting the study, is put forward as a means to limiting both HARKing and p-hacking.},
  language = {English},
  keywords = {HARKing,journals,methodology,P-hacking,preregistration,publication bias,questionable research practice,replication},
  annotation = {WOS:000559843700001}
}

@article{berlin_declaracion_2003,
  title = {La {{Declaraci\'on}} de {{Berl\'in}} Sobre Acceso Abierto},
  author = {Berl{\'i}n},
  year = {2003},
  series = {Sociedad {{Max Planck}}},
  volume = {1},
  number = {2},
  pages = {152--154}
}

@misc{bethesda_declaracion_2003,
  title = {Declaraci\'on de {{Bethesda}} Sobre Publicaci\'on de Acceso Abierto},
  author = {Bethesda},
  year = {2003}
}

@article{bishop_Problems_2016,
  title = {Problems in Using P-Curve Analysis and Text-Mining to Detect Rate of p-Hacking and Evidential Value},
  author = {Bishop, Dorothy V. M. and Thompson, Paul A.},
  year = {2016},
  month = feb,
  journal = {Peerj},
  volume = {4},
  pages = {e1715},
  publisher = {{Peerj Inc}},
  address = {{London}},
  issn = {2167-8359},
  doi = {10.7717/peerj.1715},
  abstract = {Background. The p-curve is a plot of the distribution of p-values reported in a set of scientific studies. Comparisons between ranges of p-values have been used to evaluate fields of research in terms of the extent to which studies have genuine evidential value, and the extent to which they suffer from bias in the selection of variables and analyses for publication, p-hacking. Methods. p-hacking can take various forms. Here we used R code to simulate the use of ghost variables, where an experimenter gathers data on several dependent variables but reports only those with statistically significant effects. We also examined a text-mined dataset used by Head et al. (2015) and assessed its suitability for investigating p-hacking. Results. We show that when there is ghost p-hacking, the shape of the p-curve depends on whether dependent variables are intercorrelated. For uncorrelated variables, simulated p-hacked data do not give the "p-hacking bump" just below .05 that is regarded as evidence of p-hacking, though there is a negative skew when simulated variables are inter-correlated. The way p-curves vary according to features of underlying data poses problems when automated text mining is used to detect p-values in heterogeneous sets of published papers. Conclusions. The absence of a bump in the p-curve is not indicative of lack of p-hacking. Furthermore, while studies with evidential value will usually generate a right-skewed p-curve, we cannot treat a right-skewed p-curve as an indicator of the extent of evidential value, unless we have a model specific to the type of p-values entered into the analysis. We conclude that it is not feasible to use the p-curve to estimate the extent of p-hacking and evidential value unless there is considerable control over the type of data entered into the analysis. In particular, p-hacking with ghost variables is likely to be missed.},
  language = {English},
  keywords = {Correlation,Ghost variables,p-curve,p-hacking,Power,prevalence,publication,Reproducibility,Simulation,Text-mining},
  annotation = {WOS:000370984200010}
}

@article{bishop_Rein_2019,
  title = {Rein in the Four Horsemen of Irreproducibility},
  author = {Bishop, Dorothy},
  year = {2019},
  month = apr,
  journal = {Nature},
  volume = {568},
  number = {7753},
  pages = {435--435},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-019-01307-2},
  abstract = {Dorothy Bishop describes how threats to reproducibility, recognized but unaddressed for decades, might finally be brought under control.},
  copyright = {2021 Nature},
  language = {en},
  keywords = {forrt}
}

@book{bjork_developing_2014,
  title = {Developing an {{Effective}}  {{Market}} for {{Open Access}}  {{Article Processing Charges}}},
  author = {Bj{\"o}rk, Bo-Christer and Solomon, David},
  year = {2014},
  publisher = {{Weolcome Trust}}
}

@article{bjork_gold_2017,
  title = {Gold, {{Green}} and {{Black Open Access}}},
  author = {Bj{\"o}rk, Bo-Christer},
  year = {2017},
  journal = {Learned Publishing},
  volume = {30},
  number = {2},
  pages = {173--175},
  doi = {10.1002/leap.1096},
  abstract = {Universal open access (OA) to scholarly research publications is deceptively simple as a concept. Any scientific publications, whether found via a Google keyword search, or by trying to access a citation would be just one click away. But the path to get there from the current subscription-dominated journal pub- lishing model has proved to be complex and filled with obstacles. Since the terms gold and green OA were coined almost 15 years ago, much of the debate inside the OA movement has been focused on the relative merits of these two paths (Harnad et al., 2004)},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  language = {English}
}

@misc{blanca_informatica_2019,
  type = {{Course}},
  title = {{Inform\'atica para las ciencias de la vida: Unix y Python}},
  author = {Blanca, Jos{\'e}},
  year = {2019},
  journal = {Bioinformatics at COMAV},
  abstract = {The COMAV institute is devoted to the preservation of the horticultural plant variability and to the plant breeding on horticultural species. To accomplish these goals we have developed some software that it might be of some use to other researchers like ngs\_backbone or sff\_extract.},
  copyright = {Creative Commons 3.0},
  language = {es}
}

@article{blincoe_Research_2020,
  title = {Research {{Preregistration}} as a {{Teaching}} and {{Learning Tool}} in {{Undergraduate Psychology Courses}}},
  author = {Blincoe, Sarai and Buchert, Stephanie},
  year = {2020},
  month = mar,
  journal = {Psychology Learning and Teaching-Plat},
  volume = {19},
  number = {1},
  pages = {107--115},
  publisher = {{Sage Publications Ltd}},
  address = {{London}},
  issn = {1475-7257},
  doi = {10.1177/1475725719875844},
  abstract = {The preregistration of research plans and hypotheses may prevent publication bias and questionable research practices. We incorporated a modified version of the preregistration process into an undergraduate capstone research course. Students completed a standard preregistration form during the planning stages of their research projects as well as surveys about their knowledge of preregistration. Based on survey results, our senior-level psychology students lacked knowledge of importance of the preregistration movement in the sciences but could anticipate some of its benefits. Our review of the completed preregistration assignment suggested that students struggle with data analysis decision-making but generally perceive preregistration as a helpful planning tool. We discuss the value of a preregistration assignment for generating discussions of research practice and ethics.},
  language = {English},
  keywords = {Preregistration,questionable research practices,undergraduate   preregistration assignment},
  annotation = {WOS:000488433900001}
}

@misc{boai_diez_2012,
  title = {{Diez a\~nos desde la Budapest Open Access Initiative: hacia lo abierto por defecto}},
  author = {BOAI, Budapest Open Access Initiative},
  year = {2012},
  collaborator = {Melero, Remedios and Babini, Dominique},
  language = {Traducido}
}

@misc{boai_iniciativa_2002,
  title = {{Iniciativa de Budapest para el Acceso Abierto}},
  author = {BOAI, Budapest Open Access Initiative},
  year = {2002},
  language = {Traducido}
}

@article{bohannon_who_2016,
  title = {Who's Downloading Pirated Papers? {{Everyone}}},
  author = {Bohannon, John},
  year = {2016},
  journal = {American Association for the Advancement of Science},
  volume = {352},
  number = {6285},
  pages = {508--512},
  issn = {1095-9203},
  doi = {10.1126/science.352.6285.508},
  copyright = {Creative Commons Attribution 4.0 International, Open Access}
}

@article{bowers_How_2016,
  title = {How to Improve Your Relationship with Your Future Self},
  author = {Bowers, Jake and Voors, Maarten},
  year = {2016},
  month = dec,
  journal = {Revista de ciencia pol\'itica (Santiago)},
  volume = {36},
  number = {3},
  pages = {829--848},
  issn = {0718-090X},
  doi = {10.4067/S0718-090X2016000300011}
}

@article{breznau_does_2021,
  title = {Does {{Sociology Need Open Science}}?},
  author = {Breznau, Nate},
  year = {2021},
  month = mar,
  journal = {Societies},
  volume = {11},
  number = {1},
  pages = {9},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/soc11010009},
  abstract = {Reliability, transparency, and ethical crises pushed many social science disciplines toward dramatic changes, in particular psychology and more recently political science. This paper discusses why sociology should also change. It reviews sociology as a discipline through the lens of current practices, definitions of sociology, positions of sociological associations, and a brief consideration of the arguments of three highly influential yet epistemologically diverse sociologists: Weber, Merton, and Habermas. It is a general overview for students and sociologists to quickly familiarize themselves with the state of sociology or explore the idea of open science and its relevance to their discipline.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  language = {en},
  keywords = {crisis of science,Habermas,Merton,open science,p-hacking,publication bias,replication,research ethics,revisado,science community,sociology legitimation,transparency,Weber}
}

@misc{breznau_observing_2021,
  title = {Observing {{Many Researchers Using}} the {{Same Data}} and {{Hypothesis Reveals}} a {{Hidden Universe}} of {{Uncertainty}}},
  author = {Breznau, Nate and Rinke, Eike Mark and Wuttke, Alexander and Adem, Muna and Adriaans, Jule and {Alvarez-Benjumea}, Amalia and Andersen, Henrik Kenneth and Auer, Daniel and Azevedo, Flavio and Bahnsen, Oke and Balzer, Dave and Bauer, Gerrit and Bauer, Paul C. and Baumann, Markus and Baute, Sharon and Benoit, Verena and Bernauer, Julian and Berning, Carl and Berthold, Anna and Bethke, Felix and Biegert, Thomas and Blinzler, Katharina and Blumenberg, Johannes and Bobzien, Licia and Bohman, Andrea and Bol, Thijs and Bostic, Amie and Brzozowska, Zuzanna and Burgdorf, Katharina and Burger, Kaspar and Busch, Kathrin and Castillo, Juan Carlos and Chan, Nathan and Christmann, Pablo and Connelly, Roxanne and Czymara, Christian S. and Damian, Elena and Ecker, Alejandro and Edelmann, Achim and Eger, Maureen A. and Ellerbrock, Simon and Forke, Anna and Forster, Andrea and Gaasendam, Chris and Gavras, Konstantin and Gayle, Vernon and Gessler, Theresa and Gnambs, Timo and Godefroidt, Am{\'e}lie and Gr{\"o}mping, Max and Gro{\ss}, Martin and Gruber, Stefan and Gummer, Tobias and Hadjar, Andreas and Heisig, Jan Paul and Hellmeier, Sebastian and Heyne, Stefanie and Hirsch, Magdalena and Hjerm, Mikael and Hochman, Oshrat and H{\"o}vermann, Andreas and Hunger, Sophia and Hunkler, Christian and Huth, Nora and Ignacz, Zsofia and Jacobs, Laura and Jacobsen, Jannes and Jaeger, Bastian and Jungkunz, Sebastian and Jungmann, Nils and Kauff, Mathias and Kleinert, Manuel and Klinger, Julia and Kolb, Jan-Philipp and Ko{\l}czy{\'n}ska, Marta and Kuk, John Seungmin and Kuni{\ss}en, Katharina and Sinatra, Dafina Kurti and Greinert, Alexander and Lersch, Philipp M. and L{\"o}bel, Lea-Maria and Lutscher, Philipp and Mader, Matthias and Madia, Joan and Malancu, Natalia and Maldonado, Luis and Marahrens, Helge and Martin, Nicole and Martinez, Paul and Mayerl, Jochen and Mayorga, Oscar Jose and McManus, Patricia and Wagner, Kyle and Meeusen, Cecil and Meierrieks, Daniel and Mellon, Jonathan and Merhout, Friedolin and Merk, Samuel and Meyer, Daniel and Micheli, Leticia and Mijs, Jonathan J. B. and Moya, Crist{\'o}bal and Neunhoeffer, Marcel and N{\"u}st, Daniel and Nyg{\aa}rd, Olav and Ochsenfeld, Fabian and Otte, Gunnar and Pechenkina, Anna and Prosser, Christopher and Raes, Louis and Ralston, Kevin and Ramos, Miguel and Roets, Arne and Rogers, Jonathan and Ropers, Guido and Samuel, Robin and Sand, Gregor and Schachter, Ariela and Schaeffer, Merlin and Schieferdecker, David and Schlueter, Elmar and Schmidt, Katja M. and Schmidt, Regine and {Schmidt-Catran}, Alexander and Schmiedeberg, Claudia and Schneider, J{\"u}rgen and Schoonvelde, Martijn and {Schulte-Cloos}, Julia and Schumann, Sandy and Schunck, Reinhard and Schupp, J{\"u}rgen and Seuring, Julian and Silber, Henning and Sleegers, Willem and Sonntag, Nico and Staudt, Alexander and Steiber, Nadia and Steiner, Nils and Sternberg, Sebastian and Stiers, Dieter and Stojmenovska, Dragana and Storz, Nora and Striessnig, Erich and Stroppe, Anne-Kathrin and Teltemann, Janna and Tibajev, Andrey and Tung, Brian B. and Vagni, Giacomo and Assche, Jasper Van and van der Linden, Meta and van der Noll, Jolanda and Hootegem, Arno Van and Vogtenhuber, Stefan and Voicu, Bogdan and Wagemans, Fieke and Wehl, Nadja and Werner, Hannah and Wiernik, Brenton M. and Winter, Fabian and Wolf, Christof and Yamada, Yuki and Zhang, Nan and Ziller, Conrad and Zins, Stefan and {\.Z}{\'o}{\l}tak, Tomasz and Nguyen, Hung H. V.},
  year = {2021},
  month = mar,
  institution = {{MetaArXiv}},
  doi = {10.31222/osf.io/cd5j9},
  abstract = {How does noise generated by researcher decisions undermine the credibility of science? We test this by observing all decisions made among 73 research teams as they independently conduct studies on the same hypothesis with identical starting data. We find excessive variation of outcomes. When combined, the 107 observed research decisions taken across teams explained at most 2.6\% of the total variance in effect sizes and 10\% of the deviance in subjective conclusions. Expertise, prior beliefs and attitudes of the researchers explain even less. Each model deployed to test the hypothesis was unique, which highlights a vast universe of research design variability that is normally hidden from view and suggests humility when presenting and interpreting scientific findings.},
  keywords = {Analytical Flexibility,Crowdsourced Replication Initiative,Crowdsourcing,Economics,Garden of Forking Paths,Immigration,Many Analysts,Meta-Science,Noise,Other Social and Behavioral Sciences,Political Science,Psychology,Researcher Degrees of Freedom,Researcher Variability,Social and Behavioral Sciences,Social Policy,Sociology}
}

@misc{breznau_Open_,
  type = {Billet},
  title = {Open Science in Sociology. {{What}}, Why and Now.},
  author = {Breznau, Nate},
  journal = {Crowdid},
  abstract = {WHAT By now you've heard the term ``open science''. Although it has no global definition, its advocates tend toward certain agreements. Most definitions focus on the practical aspects of accessibility. ``\ldots the practice of science in such a way that others can collaborate and contribute, where research data, lab notes and other research processes are freely \ldots{} Continue reading Open science in sociology. What, why and now.},
  language = {en-US}
}

@techreport{brodeur_Methods_2018,
  type = {{{IZA Discussion Paper}}},
  title = {Methods {{Matter}}: {{P}}-{{Hacking}} and {{Causal Inference}} in {{Economics}}},
  shorttitle = {Methods {{Matter}}},
  author = {Brodeur, Abel and Cook, Nikolai and Heyes, Anthony},
  year = {2018},
  month = aug,
  number = {11796},
  institution = {{Institute of Labor Economics (IZA)}},
  abstract = {The economics 'credibility revolution' has promoted the identification of causal relationships using difference-in-differences (DID), instrumental variables (IV), randomized control trials (RCT) and regression discontinuity design (RDD) methods. The extent to which a reader should trust claims about the statistical significance of results proves very sensitive to method. Applying multiple methods to 13,440 hypothesis tests reported in 25 top economics journals in 2015, we show that selective publication and p-hacking is a substantial problem in research employing DID and (in particular) IV. RCT and RDD are much less problematic. Almost 25\% of claims of marginally significant results in IV papers are misleading.},
  keywords = {causal inference,p-curves,p-hacking,practices,publication bias,research methods}
}

@article{brodeur_Star_2016,
  title = {Star {{Wars}}: {{The Empirics Strike Back}}},
  shorttitle = {Star {{Wars}}},
  author = {Brodeur, Abel and L{\'e}, Mathias and Sangnier, Marc and Zylberberg, Yanos},
  year = {2016},
  month = jan,
  journal = {American Economic Journal: Applied Economics},
  volume = {8},
  number = {1},
  pages = {1--32},
  issn = {1945-7782},
  doi = {10.1257/app.20150044},
  abstract = {Using 50,000 tests published in the AER, JPE, and QJE, we identify a residual in the distribution of tests that cannot be explained solely by journals favoring rejection of the null hypothesis. We observe a two-humped camel shape with missing p-values between 0.25 and 0.10 that can be retrieved just after the 0.05 threshold and represent 10-20 percent of marginally rejected tests. Our interpretation is that researchers inflate the value of just-rejected tests by choosing "significant" specifications. We propose a method to measure this residual and describe how it varies by article and author characteristics. (JEL A11, C13)},
  language = {en},
  keywords = {Market for Economists; Estimation: General,Role of Economics,Role of Economists}
}

@misc{budapestopenaccessinitiative_diez_2012,
  title = {{Diez a\~nos desde la Budapest Open Access Initiative: hacia lo abierto por defecto}},
  author = {Budapest Open Access Initiative},
  year = {12 de Septiembre, 2012},
  journal = {BOAI},
  language = {Traducido}
}

@misc{budapestopenaccessinitiative_iniciativa_2002,
  title = {{Iniciativa de Budapest para el Aceso Abierto}},
  author = {Budapest Open Access Initiative},
  year = {14 de Febrero, 2002},
  journal = {Budapest Open Access Initiative},
  language = {Traducido}
}

@article{burlig_Improving_2018,
  title = {Improving Transparency in Observational Social Science Research: {{A}} Pre-Analysis Plan Approach},
  shorttitle = {Improving Transparency in Observational Social Science Research},
  author = {Burlig, Fiona},
  year = {2018},
  month = jul,
  journal = {Economics Letters},
  volume = {168},
  pages = {56--60},
  issn = {0165-1765},
  doi = {10.1016/j.econlet.2018.03.036},
  abstract = {Social science research has undergone a credibility revolution, but these gains are at risk due to problematic research practices. Existing research on transparency has centered around randomized controlled trials, which constitute only a small fraction of research in economics. In this paper, I highlight three scenarios in which study preregistration can be credibly applied in non-experimental settings: cases where researchers collect their own data; prospective studies; and research using restricted-access data.},
  language = {en},
  keywords = {Confidential data,Observational research,Pre-registration,Transparency}
}

@misc{businessmanagementink_protecting_2016,
  title = {Protecting {{Students}}' {{Intellectual Property}}},
  author = {Business \& Management INK},
  year = {2016},
  journal = {social science space}
}

@article{buttner_Are_2020,
  title = {Are Questionable Research Practices Facilitating New Discoveries in Sport and Exercise Medicine? {{The}} Proportion of Supported Hypotheses Is Implausibly High},
  shorttitle = {Are Questionable Research Practices Facilitating New Discoveries in Sport and Exercise Medicine?},
  author = {Buttner, Fionn and Toomey, Elaine and McClean, Shane and Roe, Mark and Delahunt, Eamonn},
  year = {2020},
  month = nov,
  journal = {British Journal of Sports Medicine},
  volume = {54},
  number = {22},
  pages = {1365--1371},
  publisher = {{Bmj Publishing Group}},
  address = {{London}},
  issn = {0306-3674},
  doi = {10.1136/bjsports-2019-101863},
  abstract = {Questionable research practices (QRPs) are intentional and unintentional practices that can occur when designing, conducting, analysing, and reporting research, producing biased study results. Sport and exercise medicine (SEM) research is vulnerable to the same QRPs that pervade the biomedical and psychological sciences, producing false-positive results and inflated effect sizes. Approximately 90\% of biomedical research reports supported study hypotheses, provoking suspicion about the field-wide presence of systematic biases to facilitate study findings that confirm researchers' expectations. In this education review, we introduce three common QRPs (ie, HARKing, P-hacking and Cherry-picking), perform a cross-sectional study to assess the proportion of original SEM research that reports supported study hypotheses, and draw attention to existing solutions and resources to overcome QRPs that manifest in exploratory research. We hypothesised that {$>$}= 85\% of original SEM research studies would report supported study hypotheses. Two independent assessors systematically identified, screened, included, and extracted study data from original research articles published between 1 January 2019 and 31 May 2019 in the British Journal of Sports Medicine, Sports Medicine, the American Journal of Sports Medicine, and the Journal of Orthopaedic \& Sports Physical Therapy. We extracted data relating to whether studies reported that the primary hypothesis was supported or rejected by the results. Study hypotheses, methodologies, and analysis plans were preregistered at the Open Science Framework. One hundred and twenty-nine original research studies reported at least one study hypothesis, of which 106 (82.2\%) reported hypotheses that were supported by study results. Of 106 studies reporting that primary hypotheses were supported by study results, 75 (70.8\%) studies reported that the primary hypothesis was fully supported by study results. The primary study hypothesis was partially supported by study results in 28 (26.4\%) studies. We detail open science practices and resources that aim to safe-guard against QRPs that bely the credibility and replicability of original research findings.},
  language = {English},
  keywords = {education,harking,incentives,methodological,publication decisions,publish,registered-reports,replicability,research,science,sport,statistics,tests,truth},
  annotation = {WOS:000584953300013}
}

@article{button_Power_2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn3475},
  abstract = {Low statistical power undermines the purpose of scientific research; it reduces the chance of detecting a true effect.Perhaps less intuitively, low power also reduces the likelihood that a statistically significant result reflects a true effect.Empirically, we estimate the median statistical power of studies in the neurosciences is between {$\sim$}8\% and {$\sim$}31\%.We discuss the consequences of such low statistical power, which include overestimates of effect size and low reproducibility of results.There are ethical dimensions to the problem of low power; unreliable research is inefficient and wasteful.Improving reproducibility in neuroscience is a key priority and requires attention to well-established, but often ignored, methodological principles.We discuss how problems associated with low power can be addressed by adopting current best-practice and make clear recommendations for how to achieve this.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  language = {en},
  keywords = {practices}
}

@article{byington_Solutions_2017,
  title = {Solutions to the {{Credibility Crisis}} in {{Management Science}}},
  author = {Byington, Eliza and Felps, Will},
  year = {2017},
  month = mar,
  journal = {Academy of Management Learning and Education, The},
  volume = {16},
  pages = {142--162},
  doi = {10.5465/amle.2015.0035},
  abstract = {This article argues much academic misconduct can be explained as the result of social dilemmas occurring at two levels of Management science. First, the career benefits associated with engaging in Noncredible Research Practices (NCRPs) (e.g. data manipulation, fabricating results, data hoarding, undisclosed HARKing) result in many academics choosing self-interest over collective welfare. These perverse incentives derive from journal gatekeepers who are pressed into a similar social dilemma. Namely, an individual journal's status (i.e. its ``impact factor'') is likely to suffer from unilaterally implementing practices that help ensure the credibility of Management science claims (e.g. dedicating journal space to strict replications, crowd-sourcing replications, data submission requirements, in-house analysis checks, registered reports, Open Practice badges). Fortunately, research on social dilemmas and collective action offers solutions. For example, journal editors could pledge to publish a certain number of credibility boosting articles contingent on a proportion of their ``peer'' journals doing the same. Details for successful implementation of conditional pledges, other social dilemma solutions \textendash{} including actions for Management academics who support changes in journal practices (e.g. reviewer boycotts / buycotts), and insights on credibility supportive journal practices from other fields are provided.},
  keywords = {crisis}
}

@article{caldwell_Moving_2020,
  title = {Moving {{Sport}} and {{Exercise Science Forward}}: {{A Call}} for the {{Adoption}} of {{More Transparent Research Practices}}},
  shorttitle = {Moving {{Sport}} and {{Exercise Science Forward}}},
  author = {Caldwell, Aaron R. and Vigotsky, Andrew D. and Tenan, Matthew S. and Radel, R{\'e}mi and Mellor, David T. and Kreutzer, Andreas and Lahart, Ian M. and Mills, John P. and Boisgontier, Matthieu P. and Boardley, Ian and Bouza, Brooke and Cheval, Boris and Chow, Zad Rafi and Contreras, Bret and Dieter, Brad and Halperin, Israel and Haun, Cody and Knudson, Duane and Lahti, Johan and Miller, Matthew and Morin, Jean-Benoit and Naughton, Mitchell and Neva, Jason and Nuckols, Greg and Peters, Sue and Roberts, Brandon and {Rosa-Caldwell}, Megan and Schmidt, Julia and Schoenfeld, Brad J. and Severin, Richard and Skarabot, Jakob and Steele, James and Twomey, Rosie and Zenko, Zachary and Lohse, Keith R. and Nunan, David and {Consortium for Transparency in Exercise Science (COTES) Collaborators}},
  year = {2020},
  month = mar,
  journal = {Sports Medicine},
  volume = {50},
  number = {3},
  pages = {449--459},
  issn = {1179-2035},
  doi = {10.1007/s40279-019-01227-1},
  abstract = {The primary means of disseminating sport and exercise science research is currently through journal articles. However, not all studies, especially those with null findings, make it to formal publication. This publication bias towards positive findings may contribute to questionable research practices. Preregistration is a solution to prevent the publication of distorted evidence resulting from this system. This process asks authors to register their hypotheses and methods before data collection on a publicly available repository or by submitting a Registered Report. In the Registered Report format, authors submit a stage 1 manuscript to a participating journal that includes an introduction, methods, and any pilot data indicating the exploratory or confirmatory nature of the study. After a stage 1 peer review, the manuscript can then be offered in-principle acceptance, rejected, or sent back for revisions to improve the quality of the study. If accepted, the project is guaranteed publication, assuming the authors follow the data collection and analysis protocol. After data collection, authors re-submit a stage 2 manuscript that includes the results and discussion, and the study is evaluated on clarity and conformity with the planned analysis. In its final form, Registered Reports appear almost identical to a typical publication, but give readers confidence that the hypotheses and main analyses are less susceptible to bias from questionable research practices. From this perspective, we argue that inclusion of Registered Reports by researchers and journals will improve the transparency, replicability, and trust in sport and exercise science research. The preprint version of this work is available on SportR\$\$\textbackslash chi \$\$iv: https://osf.io/preprints/sportrxiv/fxe7a/.},
  language = {en},
  keywords = {transparency}
}

@article{caldwell_Moving_2020a,
  title = {Moving {{Sport}} and {{Exercise Science Forward}}: {{A Call}} for the {{Adoption}} of {{More Transparent Research Practices}}},
  shorttitle = {Moving {{Sport}} and {{Exercise Science Forward}}},
  author = {Caldwell, Aaron R. and Vigotsky, Andrew D. and Tenan, Matthew S. and Radel, Remi and Mellor, David T. and Kreutzer, Andreas and Lahart, Ian M. and Mills, John P. and Boisgontier, Matthieu P. and Boardley, Ian and Bouza, Brooke and Cheval, Boris and Chow, Zad Rafi and Contreras, Bret and Dieter, Brad and Halperin, Israel and Haun, Cody and Knudson, Duane and Lahti, Johan and Lohse, Keith and Miller, Matthew and Morin, Jean-Benoit and Naughton, Mitchell and Neva, Jason and Nuckols, Greg and Nunan, David and Peters, Sue and Roberts, Brandon and {Rosa-Caldwell}, Megan and Schmidt, Julia and Schoenfeld, Brad J. and Severin, Richard and Skarabot, Jakob and Steele, James and Twomey, Rosie and Zenko, Zachary},
  year = {2020},
  month = mar,
  journal = {Sports Medicine},
  volume = {50},
  number = {3},
  pages = {449--459},
  publisher = {{Adis Int Ltd}},
  address = {{Northcote}},
  issn = {0112-1642},
  doi = {10.1007/s40279-019-01227-1},
  abstract = {The primary means of disseminating sport and exercise science research is currently through journal articles. However, not all studies, especially those with null findings, make it to formal publication. This publication bias towards positive findings may contribute to questionable research practices. Preregistration is a solution to prevent the publication of distorted evidence resulting from this system. This process asks authors to register their hypotheses and methods before data collection on a publicly available repository or by submitting a Registered Report. In the Registered Report format, authors submit a stage 1 manuscript to a participating journal that includes an introduction, methods, and any pilot data indicating the exploratory or confirmatory nature of the study. After a stage 1 peer review, the manuscript can then be offered in-principle acceptance, rejected, or sent back for revisions to improve the quality of the study. If accepted, the project is guaranteed publication, assuming the authors follow the data collection and analysis protocol. After data collection, authors re-submit a stage 2 manuscript that includes the results and discussion, and the study is evaluated on clarity and conformity with the planned analysis. In its final form, Registered Reports appear almost identical to a typical publication, but give readers confidence that the hypotheses and main analyses are less susceptible to bias from questionable research practices. From this perspective, we argue that inclusion of Registered Reports by researchers and journals will improve the transparency, replicability, and trust in sport and exercise science research. The preprint version of this work is available on SportRiv: https://osf.io/ prepr ints/sport rxiv/fxe7a/.},
  language = {English},
  keywords = {associations,health,increase,model,power,publication bias,registered-reports,true},
  annotation = {WOS:000511041300002}
}

@article{camerer_Evaluating_2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {9},
  pages = {637--644},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  abstract = {Being able to replicate scientific findings is crucial for scientific progress1\textendash 15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516\textendash 36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
  copyright = {2018 The Author(s)},
  language = {en},
  keywords = {crisis}
}

@article{campbell_Enhancing_2014,
  title = {Enhancing Transparency of the Research Process to Increase Accuracy of Findings: {{A}} Guide for Relationship Researchers},
  shorttitle = {Enhancing Transparency of the Research Process to Increase Accuracy of Findings},
  author = {Campbell, Lorne and Loving, Timothy J. and Lebel, Etienne P.},
  year = {2014},
  journal = {Personal Relationships},
  volume = {21},
  number = {4},
  pages = {531--545},
  issn = {1475-6811},
  doi = {10.1111/pere.12053},
  abstract = {The purpose of this paper is to extend to the field of relationship science, recent discussions and suggested changes in open research practises. We demonstrate different ways that greater transparency of the research process in our field will accelerate scientific progress by increasing accuracy of reported research findings. Importantly, we make concrete recommendations for how relationship researchers can transition to greater disclosure of research practices in a manner that is sensitive to the unique design features of methodologies employed by relationship scientists. We discuss how to implement these recommendations for four different research designs regularly used in relationship research and practical limitations regarding implementing our recommendations and provide potential solutions to these problems.},
  language = {en},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/pere.12053}
}

@article{card_Role_2011,
  title = {The {{Role}} of {{Theory}} in {{Field Experiments}}},
  author = {Card, David and DellaVigna, Stefano and Malmendier, Ulrike},
  year = {2011},
  month = sep,
  journal = {Journal of Economic Perspectives},
  volume = {25},
  number = {3},
  pages = {39--62},
  issn = {0895-3309},
  doi = {10.1257/jep.25.3.39},
  abstract = {studies that estimate structural parameters in a completely specified model. We also classify laboratory experiments published in these journals over the same period and find that economic theory has played a more central role in the laboratory than in the field. Finally, we discuss in detail three sets of field experiments\textemdash on gift exchange, on charitable giving, and on negative income tax\textemdash that illustrate both the benefits and the potential costs of a tighter link between experimental design and theoretical underpinnings.},
  language = {en},
  keywords = {Field Experiments}
}

@article{carey_Fraud_2011,
  title = {Fraud {{Case Seen}} as a {{Red Flag}} for {{Psychology Research}}},
  author = {Carey, Benedict},
  year = {2011},
  month = nov,
  journal = {The New York Times},
  issn = {0362-4331},
  abstract = {A Dutch scholar was found to have falsified findings in dozens of papers, in a field that critics say is vulnerable to such abuses.},
  chapter = {Health},
  language = {en-US},
  keywords = {Falsification of Data,Frauds and Swindling,Psychology and Psychologists,Research,Stapel; Diederik}
}

@article{carrier_Facing_2017,
  title = {Facing the {{Credibility Crisis}} of {{Science}}: {{On}} the {{Ambivalent Role}} of {{Pluralism}} in {{Establishing Relevance}} and {{Reliability}}},
  shorttitle = {Facing the {{Credibility Crisis}} of {{Science}}},
  author = {Carrier, Martin},
  year = {2017},
  month = may,
  journal = {Perspectives on Science},
  volume = {25},
  number = {4},
  pages = {439--464},
  issn = {1063-6145},
  doi = {10.1162/POSC_a_00249},
  abstract = {Science at the interface with society is regarded with mistrust among parts of the public. Scientific judgments on matters of practical concern are not infrequently suspected of being incompetent and biased. I discuss two proposals for remedying this deficiency. The first aims at strengthening the independence of science and suggests increasing the distance to political and economic powers. The drawback is that this runs the risk of locking science in an academic ivory tower. The second proposal favors ``counter-politicization'' in that research is strongly focused on projects ``in the public interest,'' that is, on projects whose expected results will benefit all those concerned by these results. The disadvantage is that the future use of research findings cannot be delineated reliably in advance. I argue that the underlying problem is the perceived lack of relevance and reliability and that pluralism is an important step toward its solution. Pluralism serves to stimulate a more inclusive research agenda and strengthens the well-testedness of scientific approaches. However, pluralism also prevents the emergence of clear-cut practical suggestions. Accordingly, pluralism is part of the solution to the credibility crisis of science, but also part of the problem. In order for science to be suitable as a guide for practice, the leeway of scientific options needs to be narrowed \textendash{} in spite of uncertainty in epistemic respect. This reduction can be achieved by appeal to criteria that do not focus on the epistemic credentials of the suggestions but on their appropriateness in practical respect.},
  keywords = {crisis}
}

@article{chambers_Registered_2013,
  title = {Registered {{Reports}}: {{A}} New Publishing Initiative at~{{Cortex}}},
  shorttitle = {Registered {{Reports}}},
  author = {Chambers, Christopher D.},
  year = {2013},
  month = mar,
  journal = {Cortex},
  volume = {49},
  number = {3},
  pages = {609--610},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2012.12.016},
  language = {en},
  keywords = {forrt,reports}
}

@misc{chambers_Registered_2014,
  title = {Registered {{Reports}}: {{A}} Step Change in Scientific Publishing},
  author = {Chambers, Christopher D.},
  year = {2014},
  journal = {Reviewers' Update},
  abstract = {Professor Chris Chambers, Registered Reports Editor of the Elsevier journal Cortex and one of the concept's founders, on how the initiative combats publication bias},
  howpublished = {https://www.elsevier.com/connect/reviewers-update/registered-reports-a-step-change-in-scientific-publishing},
  language = {en},
  keywords = {forrt,reports}
}

@article{chambers_Registered_2015,
  title = {Registered Reports: Realigning Incentives in Scientific Publishing},
  shorttitle = {Registered Reports},
  author = {Chambers, Christopher D. and Dienes, Zoltan and McIntosh, Robert D. and Rotshtein, Pia and Willmes, Klaus},
  year = {2015},
  month = may,
  journal = {Cortex; a Journal Devoted to the Study of the Nervous System and Behavior},
  volume = {66},
  pages = {A1-2},
  issn = {1973-8102},
  doi = {10.1016/j.cortex.2015.03.022},
  language = {eng},
  pmid = {25892410},
  keywords = {Biomedical Research,Editorial Policies,forrt,Humans,Motivation,Peer Review; Research,Publication Bias,Publishing,reports,Reproducibility of Results}
}

@article{chambers_Ten_2015,
  title = {Ten {{Reasons Why Journals Must Review Manuscripts Before Results Are Known}}},
  author = {Chambers, Christopher D.},
  year = {2015},
  month = jan,
  journal = {Addiction},
  volume = {110},
  number = {1},
  pages = {10--11},
  publisher = {{Wiley}},
  address = {{Hoboken}},
  issn = {0965-2140},
  doi = {10.1111/add.12728},
  language = {English},
  keywords = {False positives,incentives,publication bias,questionable research practices,registered reports,registered-reports,reproducibility,study pre-registration,truth},
  annotation = {WOS:000346699700004}
}

@techreport{chin_Improving_2019,
  type = {{{SSRN Scholarly Paper}}},
  title = {Improving {{Expert Evidence}}: {{The Role}} of {{Open Science}} and {{Transparency}}},
  shorttitle = {Improving {{Expert Evidence}}},
  author = {Chin, Jason and Growns, Bethany and Mellor, David},
  year = {2019},
  month = feb,
  number = {ID 3345225},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  doi = {10.2139/ssrn.3345225},
  abstract = {Both science and expert evidence law are undergoing significant changes. In this article, the authors compare these two movements \textendash{} the open science movement and the evidence-based evidence movement. The open science movement encompasses the recent discovery of many irreproducible findings in science and the subsequent move towards more transparent methods. The evidence-based evidence movement is the discovery that many forms of expert evidence are unreliable, and that they have contributed to wrongful convictions. The authors identify similarities between these movements, which suggest how courts and legal actors may learn from the open science movement to produce more accurate results. Expert witnesses should comport themselves as rigorous open scientists to produce evidence that is more susceptible to evaluation. Parties should be subjected to more specific and rigorous disclosure requirements because research has shown that even leading scientists find it easy to discount and suppress findings that do not support their hypotheses. And trial judges, as gatekeepers, should not defer to the generally accepted practices that have proven insufficient in the mainstream sciences.},
  language = {en},
  keywords = {Bethany Growns,David Mellor,Improving Expert Evidence: The Role of Open Science and Transparency,Jason Chin,SSRN}
}

@article{chin_Questionable_2021,
  title = {Questionable {{Research Practices}} and {{Open Science}} in {{Quantitative Criminology}}},
  author = {Chin, Jason M. and Pickett, Justin T. and Vazire, Simine and Holcombe, Alex O.},
  year = {2021},
  journal = {Journal of Quantitative Criminology},
  issn = {0748-4518},
  doi = {10.1007/s10940-021-09525-6},
  abstract = {Questionable research practices (QRPs) lead to incorrect research results and contribute to irreproducibility in science. Researchers and institutions have proposed open science practices (OSPs) to improve the detectability of QRPs and the credibility of science. We examine the prevalence of QRPs and OSPs in criminology, and researchers' opinions of those practices. We administered an anonymous survey to authors of articles published in criminology journals. Respondents self-reported their own use of 10 QRPs and 5 OSPs. They also estimated the prevalence of use by others, and reported their attitudes toward the practices. QRPs and OSPs are both common in quantitative criminology, about as common as they are in other fields. Criminologists who responded to our survey support using QRPs in some circumstances, but are even more supportive of using OSPs. We did not detect a significant relationship between methodological training and either QRP or OSP use. Support for QRPs is negatively and significantly associated with support for OSPs. Perceived prevalence estimates for some practices resembled a uniform distribution, suggesting criminologists have little knowledge of the proportion of researchers that engage in certain questionable practices. Most quantitative criminologists in our sample have used QRPs, and many have used multiple QRPs. Moreover, there was substantial support for QRPs, raising questions about the validity and reproducibility of published criminological research. We found promising levels of OSP use, albeit at levels lagging what researchers endorse. The findings thus suggest that additional reforms are needed to decrease QRP use and increase the use of OSPs.},
  language = {en}
}

@article{chopik_Relationship_2020,
  title = {Relationship Science and the Credibility Revolution: {{An}} Introduction to the First Part of the Special Issue},
  shorttitle = {Relationship Science and the Credibility Revolution},
  author = {Chopik, William J. and Chartier, Christopher R. and Campbell, Lorne and Donnellan, M. Brent},
  year = {2020},
  month = mar,
  journal = {Personal Relationships},
  volume = {27},
  number = {1},
  pages = {132--137},
  publisher = {{Wiley}},
  address = {{Hoboken}},
  issn = {1350-4126},
  doi = {10.1111/pere.12312},
  abstract = {In the past 10 years, the field of relationship science-like many other fields-has been exposed to dramatic changes in how scientists approach the research process. Relationship science has been at the forefront of many recent changes in the field, whether it be high profile replication attempts or broader discussions about how to increase rigor and reproducibility. A major goal of this special issue was to provide an opportunity for relationship scientists to engage with these issues and reforms. The first four articles in this special issue represent a sampling of different approaches relationship researchers have used to enhance the credibility of their work.},
  language = {English},
  keywords = {credibility revolution,history,incentives,increase,personal relationships,registered reports,registered-reports,replication,special issue,truth},
  annotation = {WOS:000518878700007}
}

@article{christensen_Transparency_2018,
  title = {Transparency, {{Reproducibility}}, and the {{Credibility}} of {{Economics Research}}},
  author = {Christensen, Garret and Miguel, Edward},
  year = {2018},
  month = sep,
  journal = {Journal of Economic Literature},
  volume = {56},
  number = {3},
  pages = {920--980},
  issn = {0022-0515},
  doi = {10.1257/jel.20171350},
  abstract = {There is growing interest in enhancing research transparency and reproducibility in economics and other scientific fields. We survey existing work on these topics within economics, and discuss the evidence suggesting that publication bias, inability to replicate, and specification searching remain widespread in the discipline. We next discuss recent progress in this area, including through improved research design, study registration and pre-analysis plans, disclosure standards, and open sharing of data and materials, drawing on experiences in both economics and other social sciences. We discuss areas where consensus is emerging on new practices, as well as approaches that remain controversial, and speculate about the most effective ways to make economics research more credible in the future.},
  language = {en},
  keywords = {Market for Economists; Methodological Issues: General; Higher Education,Research Institutions,Role of Economics,Role of Economists}
}

@book{christensen_Transparent_2019,
  title = {Transparent and Reproducible Social Science Research: How to Do Open Science},
  shorttitle = {Transparent and Reproducible Social Science Research},
  author = {Christensen, Garret S. and Freese, Jeremy and Miguel, Edward},
  year = {2019},
  publisher = {{University of California Press}},
  address = {{Oakland, California}},
  abstract = {"Social science practitioners have recently witnessed numerous episodes of influential research that fell apart upon close scrutiny. These instances have spurred suspicions that other published results may contain errors or may at least be less robust than they appear. In response, an influential movement has emerged across the social sciences for greater research transparency, openness, and reproducibility. Transparent and Reproducible Social Science Research crystallizes the new insights, practices, and methods of this rising interdisciplinary field"--Provided by publisher},
  isbn = {978-0-520-96923-0},
  lccn = {Q180.55.S7},
  keywords = {Reproducible research,Research,Social sciences,transparency}
}

@article{chuard_Evidence_2019,
  title = {Evidence That Nonsignificant Results Are Sometimes Preferred: {{Reverse P}}-Hacking or Selective Reporting?},
  shorttitle = {Evidence That Nonsignificant Results Are Sometimes Preferred},
  author = {Chuard, Pierre J. C. and Vrtilek, Milan and Head, Megan L. and Jennions, Michael D.},
  year = {2019},
  month = jan,
  journal = {Plos Biology},
  volume = {17},
  number = {1},
  pages = {e3000127},
  publisher = {{Public Library Science}},
  address = {{San Francisco}},
  issn = {1544-9173},
  doi = {10.1371/journal.pbio.3000127},
  abstract = {There is increased concern about poor scientific practices arising from an excessive focus on P-values. Two particularly worrisome practices are selective reporting of significant results and 'P-hacking'. The latter is the manipulation of data collection, usage, or analyses to obtain statistically significant outcomes. Here, we introduce the novel, to our knowledge, concepts of selective reporting of nonsignificant results and 'reverse P-hacking' whereby researchers ensure that tests produce a nonsignificant result. We test whether these practices occur in experiments in which researchers randomly assign subjects to treatment and control groups to minimise differences in confounding variables that might affect the focal outcome. By chance alone, 5\% of tests for a group difference in confounding variables should yield a significant result (P {$<$} 0.05). If researchers less often report significant findings and/or reverse P-hack to avoid significant outcomes that undermine the ethos that experimental and control groups only differ with respect to actively manipulated variables, we expect significant results from tests for group differences to be under-represented in the literature. We surveyed the behavioural ecology literature and found significantly more nonsignificant P-values reported for tests of group differences in potentially confounding variables than the expected 95\% (P = 0.005; N = 250 studies). This novel, to our knowledge, publication bias could result from selective reporting of nonsignificant results and/or from reverse P-hacking. We encourage others to test for a bias toward publishing nonsignificant results in the equivalent context in their own research discipline.},
  language = {English},
  keywords = {ecology,randomized controlled-trials,values},
  annotation = {WOS:000457596000029}
}

@article{chubin_Open_1985,
  title = {Open {{Science}} and {{Closed Science}}: {{Tradeoffs}} in a {{Democracy}}},
  shorttitle = {Open {{Science}} and {{Closed Science}}},
  author = {Chubin, Daryl E.},
  year = {1985},
  month = apr,
  journal = {Science, Technology, \& Human Values},
  volume = {10},
  number = {2},
  pages = {73--80},
  publisher = {{SAGE Publications Inc}},
  issn = {0162-2439},
  doi = {10.1177/016224398501000211},
  language = {en}
}

@misc{conicyt_chile_2017,
  title = {Chile y {{Argentina}} Son Destacados Como Ejemplos de Pol\'iticas de Acceso Abierto a La Informaci\'on},
  author = {CONICYT},
  year = {9 de Enero, 2017}
}

@article{crane_Impact_2018,
  title = {The {{Impact}} of {{P}}-Hacking on "{{Redefine Statistical Significance}}"},
  author = {Crane, Harry},
  year = {2018},
  journal = {Basic and Applied Social Psychology},
  volume = {40},
  number = {4},
  pages = {219--235},
  publisher = {{Routledge Journals, Taylor \& Francis Ltd}},
  address = {{Abingdon}},
  issn = {0197-3533},
  doi = {10.1080/01973533.2018.1474111},
  abstract = {In their proposal to "redefine statistical significance," Benjamin et al. claim that lowering the default cutoff for statistical significance from .05 to .005 would "immediately improve the reproducibility of scientific research in many fields." Benjamin et al. assert specifically that false positive rates would fall below 10\% and replication rates would double under the lower cutoff. I analyze these claims here, showing how the failure to account for P-hacking and other widespread reporting issues leads to exaggerated and misleading conclusions about the potential impact of the .005 proposal.},
  language = {English},
  keywords = {prevalence},
  annotation = {WOS:000454439800005}
}

@misc{creativecommons_cc_2019,
  type = {{{FAQ}}},
  title = {About {{CC Licenses}}},
  shorttitle = {{{CC Licenses}}},
  author = {Creative Commons, CC},
  year = {2019},
  journal = {What we do},
  abstract = {Creative Commons licenses give everyone from individual creators to large institutions a standardized way to grant the public permission to use their creative work under copyright law. From the reuser's perspective, the presence of a Creative Commons license on a copyrighted work answers the question, ``What can I do with this work?''},
  copyright = {Creative Commons 3.0},
  language = {English}
}

@misc{creativecommons_sobre_2017,
  title = {Sobre Las Licencias: {{Lo}} Que Nuestras Licencias Hacen},
  author = {Creative Commons},
  year = {7 de Noviembre, 2017}
}

@misc{cruwell_Easy_2018,
  title = {7 {{Easy Steps}} to {{Open Science}}: {{An Annotated Reading List}}},
  shorttitle = {7 {{Easy Steps}} to {{Open Science}}},
  author = {Cr{\"u}well, Sophia and van Doorn, Johnny and Etz, Alexander and Makel, Matthew C. and Moshontz, Hannah and Niebaum, Jesse and Orben, Amy and Parsons, Sam and {Schulte-Mecklenbeck}, Michael},
  year = {2018},
  month = nov,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/cfzyx},
  abstract = {The Open Science movement is rapidly changing the scientific landscape. Because exact definitions are often lacking and reforms are constantly evolving, accessible guides to open science are needed. This paper provides an introduction to open science and related reforms in the form of an annotated reading list of seven peer-reviewed articles, following the format of Etz et al. (2018). Written for researchers and students - particularly in psychological science - it highlights and introduces seven topics: understanding open science; open access; open data, materials, and code; reproducible analyses; preregistration and registered reports; replication research; and teaching open science. For each topic, we provide a detailed summary of one particularly informative and actionable article and suggest several further resources. Supporting a broader understanding of open science issues, this overview should enable researchers to engage with, improve, and implement current open, transparent, reproducible, replicable, and cumulative scientific practices.},
  keywords = {Meta-science,Meta-Science,Open Access,Open Science,other,Psychology,Reproducibility,revisado,Social and Behavioral Sciences,Transparency}
}

@article{dal-re_Making_2014,
  title = {Making {{Prospective Registration}} of {{Observational Research}} a {{Reality}}},
  author = {{Dal-R{\'e}}, Rafael and Ioannidis, John P. and Bracken, Michael B. and Buffler, Patricia A. and Chan, An-Wen and Franco, Eduardo L. and Vecchia, Carlo La and Weiderpass, Elisabete},
  year = {2014},
  month = feb,
  journal = {Science Translational Medicine},
  volume = {6},
  number = {224},
  pages = {224cm1-224cm1},
  publisher = {{American Association for the Advancement of Science}},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.3007513},
  abstract = {The vast majority of health-related observational studies are not prospectively registered and the advantages of registration have not been fully appreciated. Nonetheless, international standards require approval of study protocols by an independent ethics committee before the study can begin. We suggest that there is an ethical and scientific imperative to publicly preregister key information from newly approved protocols, which should be required by funders. Ultimately, more complete information may be publicly available by disclosing protocols, analysis plans, data sets, and raw data. Key information about human observational studies should be publicly available before the study is initiated. Key information about human observational studies should be publicly available before the study is initiated.},
  chapter = {Commentary},
  copyright = {Copyright \textcopyright{} 2014, American Association for the Advancement of Science},
  language = {en},
  pmid = {24553383},
  keywords = {reports}
}

@article{derond_Publish_2005,
  title = {Publish or {{Perish}}: {{Bane}} or {{Boon}} of {{Academic Life}}?},
  shorttitle = {Publish or {{Perish}}},
  author = {De Rond, Mark and Miller, Alan N.},
  year = {2005},
  month = dec,
  journal = {Journal of Management Inquiry},
  volume = {14},
  number = {4},
  pages = {321--329},
  publisher = {{SAGE Publications Inc}},
  issn = {1056-4926},
  doi = {10.1177/1056492605276850},
  abstract = {There are few more familiar aphorisms in the academic community than ``publish or perish.'' Venerated by many and dreaded by more, this phenomenon is the subject of the authors' essay. Here they consider the publish or perish principle that has come to characterize life at many business schools. They explain when and why it began and suggest reasons for its persistence. This exercise elicits questions that appear as relatively neglected but are integral to our profession, namely, the effect of publish or perish on the creativity, intellectual lives, morale, and psychological and emotional states of faculty.},
  language = {en},
  keywords = {business schools,institutional,publish,research,tenure}
}

@article{diaz_Mala_2018,
  title = {{Mala conducta cient\'ifica en la publicaci\'on}},
  author = {D{\'i}az, Rosa Mar{\'i}a Lam},
  year = {2018},
  month = jan,
  journal = {Revista Cubana de Hematolog\'ia, Inmunolog\'ia y Hemoterapia},
  volume = {34},
  number = {1},
  issn = {1561-2996},
  abstract = {La publicaci\'on en revistas cient\'ificas constituye la forma m\'as aceptada para validar una investigaci\'on debido a que pasa por un riguroso proceso de revisi\'on por expertos, que deciden entre lo publicable y lo no publicable con vista a garantizar la calidad de los trabajos. A pesar de esto con frecuencia aparecen pr\'acticas incorrectas relacionadas con la \'etica durante la publicaci\'on, que se conocen como mala conducta cient\'ifica. Las manifestaciones de mala conducta cient\'ifica van desde el fraude cient\'ifico hasta una variedad de faltas que se cometen en el proceso de publicaci\'on. El fraude cient\'ifico incluye la invenci\'on, la falsificaci\'on y el plagio. Las faltas en el proceso de publicaci\'on incluyen la autor\'ia ficticia, la autor\'ia fantasma, la publicaci\'on duplicada, la publicaci\'on fragmentada o publicaci\'on salami, la publicaci\'on inflada, el autoplagio, la incorrecci\'on de citas bibliogr\'aficas, los sesgos de publicaci\'on y la publicaci\'on anticipada.},
  copyright = {Copyright (c) 2018 Revista Cubana de Hematolog\'ia, Inmunolog\'ia y Hemoterapia},
  language = {es},
  keywords = {autoría,conflicto de intereses,ética,fraude científico,investigación,plagio,practices,publicación}
}

@article{doucouliagos_Are_2013,
  title = {Are {{All Economic Facts Greatly Exaggerated}}? {{Theory Competition}} and {{Selectivity}}},
  shorttitle = {Are {{All Economic Facts Greatly Exaggerated}}?},
  author = {Doucouliagos, Chris and Stanley, T. D.},
  year = {2013},
  journal = {Journal of Economic Surveys},
  volume = {27},
  number = {2},
  pages = {316--339},
  issn = {1467-6419},
  doi = {10.1111/j.1467-6419.2011.00706.x},
  abstract = {Abstract..There is growing concern and mounting evidence of selectivity in empirical economics. Most empirical economic literatures have a truncated distribution of results. The aim of this paper is to explore the link between publication selectivity and theory contests. This link is confirmed through the analysis of 87 distinct empirical economics literatures, involving more than three and a half thousand separate empirical studies, using objective measures of both selectivity and contests. Our meta\textendash meta-analysis shows that publication selection is widespread, but not universal. It distorts scientific inference with potentially adverse effects on policy making, but competition and debate between rival theories reduces this selectivity and thereby improves economic inference.},
  language = {en},
  keywords = {Contested theory,Empirical economics,Meta-analysis,Publication selectivity},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-6419.2011.00706.x}
}

@article{dutilh_Flexible_,
  title = {Flexible yet Fair: Blinding Analyses in Experimental Psychology},
  shorttitle = {Flexible yet Fair},
  author = {Dutilh, Gilles and Sarafoglou, Alexandra and Wagenmakers, Eric-Jan},
  journal = {Synthese},
  publisher = {{Springer}},
  address = {{Dordrecht}},
  issn = {0039-7857},
  doi = {10.1007/s11229-019-02456-7},
  abstract = {The replicability of findings in experimental psychology can be improved by distinguishing sharply between hypothesis-generating research and hypothesis-testing research. This distinction can be achieved by preregistration, a method that has recently attracted widespread attention. Although preregistration is fair in the sense that it inoculates researchers against hindsight bias and confirmation bias, preregistration does not allow researchers to analyze the data flexibly without the analysis being demoted to exploratory. To alleviate this concern we discuss how researchers may conduct blinded analyses (MacCoun and Perlmutter in Nature 526:187-189, 2015). As with preregistration, blinded analyses break the feedback loop between the analysis plan and analysis outcome, thereby preventing cherry-picking and significance seeking. However, blinded analyses retain the flexibility to account for unexpected peculiarities in the data. We discuss different methods of blinding, offer recommendations for blinding of popular experimental designs, and introduce the design for an online blinding protocol.},
  language = {English},
  keywords = {explanation,Preregistration,registered-reports,replication,Replication crisis,Scientific learning,social-psychology,trials},
  annotation = {WOS:000575377000001}
}

@article{dutilh_Seven_2016,
  title = {Seven {{Selfish Reasons}} for {{Preregistration}}},
  author = {Dutilh, Eric-Jan Wagenmakers {and} Gilles},
  year = {2016},
  month = oct,
  journal = {APS Observer},
  volume = {29},
  number = {9},
  abstract = {Psychological scientists Eric-Jan Wagenmakers and Gilles Dutilh present an illustrated guide to the career benefits of submitting your research plans before beginning your data collection.},
  language = {en-US}
}

@article{earth_Reproducibility_2019,
  title = {Reproducibility and {{Replicability}} in {{Science}}},
  author = {on Earth, Division and on Behavioral, Cognitive Board},
  year = {2019},
  journal = {undefined},
  abstract = {Semantic Scholar extracted view of \&quot;Reproducibility and Replicability in Science\&quot; by Division on Earth et al.},
  language = {en}
}

@article{editors_Observational_2014,
  title = {Observational {{Studies}}: {{Getting Clear}} about {{Transparency}}},
  shorttitle = {Observational {{Studies}}},
  author = {Editors, The PLOS Medicine},
  year = {2014},
  month = aug,
  journal = {PLOS Medicine},
  volume = {11},
  number = {8},
  pages = {e1001711},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1001711},
  abstract = {The PLOS Medicine Editors endorse four measures to ensure transparency in the analysis and reporting of observational studies. Please see later in the article for the Editors' Summary},
  language = {en},
  keywords = {Clinical trial reporting,Clinical trials,Cross-sectional studies,Health care policy,Medical journals,Observational studies,Research reporting guidelines,Science policy}
}

@article{editors_Observational_2014a,
  title = {Observational {{Studies}}: {{Getting Clear}} about {{Transparency}}},
  shorttitle = {Observational {{Studies}}},
  author = {Editors, The PLOS Medicine},
  year = {2014},
  month = aug,
  journal = {PLOS Medicine},
  volume = {11},
  number = {8},
  pages = {e1001711},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1001711},
  abstract = {The PLOS Medicine Editors endorse four measures to ensure transparency in the analysis and reporting of observational studies. Please see later in the article for the Editors' Summary},
  language = {en},
  keywords = {Clinical trial reporting,Clinical trials,Cross-sectional studies,Health care policy,Medical journals,Observational studies,Research reporting guidelines,Science policy}
}

@article{editors_Transparency_2015,
  title = {Transparency in {{Reporting Observational Studies}}: {{Reflections}} after a {{Year}}},
  shorttitle = {Transparency in {{Reporting Observational Studies}}},
  author = {Editors, The PLOS Medicine},
  year = {2015},
  month = oct,
  journal = {PLOS Medicine},
  volume = {12},
  number = {10},
  pages = {e1001896},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1001896},
  abstract = {The PLOS Medicine Editors take stock of changes in the reporting of observational studies following our new transparency guidelines from August 2014.},
  language = {en},
  keywords = {Cohort studies,Diagnostic medicine,Health care policy,Observational studies,Open access medical journals,Peer review,Reflection,Water resources}
}

@article{elliott_Taxonomy_2020,
  title = {A {{Taxonomy}} of {{Transparency}} in {{Science}}},
  author = {Elliott, Kevin C.},
  year = {2020},
  journal = {Canadian Journal of Philosophy},
  pages = {1--14},
  publisher = {{Cambridge University Press}},
  issn = {0045-5091, 1911-0820},
  doi = {10.1017/can.2020.21},
  abstract = {Both scientists and philosophers of science have recently emphasized the importance of promoting transparency in science. For scientists, transparency is a way to promote reproducibility, progress, and trust in research. For philosophers of science, transparency can help address the value-ladenness of scientific research in a responsible way. Nevertheless, the concept of transparency is a complex one. Scientists can be transparent about many different things, for many different reasons, on behalf of many different stakeholders. This paper proposes a taxonomy that clarifies the major dimensions along which approaches to transparency can vary. By doing so, it provides several insights that philosophers and other science studies scholars can pursue. In particular, it helps address common objections to pursuing transparency in science, it clarifies major forms of transparency, and it suggests avenues for further research on this topic.},
  language = {en},
  keywords = {herramienta,open science,research ethics,science communication,transparency,value judgments,values and science}
}

@misc{engzell_Improving_2020,
  title = {Improving {{Social Science}}: {{Lessons}} from the {{Open Science Movement}}},
  shorttitle = {Improving {{Social Science}}},
  author = {Engzell, Per and Rohrer, Julia M.},
  year = {2020},
  month = apr,
  institution = {{SocArXiv}},
  doi = {10.31235/osf.io/6whjt},
  abstract = {The transdisciplinary movement towards greater research transparency opens the door for a meta-scientific exchange between different social sciences. In the spirit of such an exchange, we offer some lessons inspired by ongoing debates in psychology, highlighting the broad benefits of open science but also potential pitfalls, as well as practical challenges in the implementation that have not yet been fully resolved. Our discussion is aimed towards political scientists but relevant for population sciences more broadly.},
  keywords = {credibility,meta-science,open science,replication,reproducibility,Social and Behavioral Sciences,transparency}
}

@article{erdfelder_Detecting_2019,
  title = {Detecting {{Evidential Value}} and P-{{Hacking With}} the p-{{Curve Tool A Word}} of {{Caution}}},
  author = {Erdfelder, Edgar and Heck, Daniel W.},
  year = {2019},
  month = oct,
  journal = {Zeitschrift Fur Psychologie-Journal of Psychology},
  volume = {227},
  number = {4},
  pages = {249--260},
  publisher = {{Hogrefe \& Huber Publishers}},
  address = {{Gottingen}},
  issn = {2190-8370},
  doi = {10.1027/2151-2604/a000383},
  abstract = {Simonsohn, Nelson, and Simmons (2014a) proposed p-curve - the distribution of statistically significant p-values for a set of studies - as a toot to assess the evidential value of these studies. They argued that, whereas right-skewed p-curves indicate true underlying effects, left-skewed p-curves indicate selective reporting of significant results when there is no true effect ("p-hacking"). We first review previous research showing that, in contrast to the first claim, null effects may produce right-skewed p-curves under some conditions. We then question the second claim by showing that not only selective reporting but also selective nonreporting of significant results due to a significant outcome of a more popular alternative test of the same hypothesis may produce left-skewed p-curves, even if all studies reflect true effects. Hence, just as right-skewed p-curves do not necessarily imply evidential value, left-skewed p-curves do not necessarily imply p- hacking and absence of true effects in the studies involved.},
  language = {English},
  keywords = {ancova,decisions,failure,false-positive results,p-curve,p-hacking,psychology,publication bias,replication,robust,science,selection,tests},
  annotation = {WOS:000503843900003}
}

@article{fanelli_How_2009,
  title = {How {{Many Scientists Fabricate}} and {{Falsify Research}}? {{A Systematic Review}} and {{Meta}}-{{Analysis}} of {{Survey Data}}},
  shorttitle = {How {{Many Scientists Fabricate}} and {{Falsify Research}}?},
  author = {Fanelli, Daniele},
  year = {2009},
  month = may,
  journal = {PLOS ONE},
  volume = {4},
  number = {5},
  pages = {e5738},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0005738},
  abstract = {The frequency with which scientists fabricate and falsify data, or commit other forms of scientific misconduct is a matter of controversy. Many surveys have asked scientists directly whether they have committed or know of a colleague who committed research misconduct, but their results appeared difficult to compare and synthesize. This is the first meta-analysis of these surveys. To standardize outcomes, the number of respondents who recalled at least one incident of misconduct was calculated for each question, and the analysis was limited to behaviours that distort scientific knowledge: fabrication, falsification, ``cooking'' of data, etc\ldots{} Survey questions on plagiarism and other forms of professional misconduct were excluded. The final sample consisted of 21 surveys that were included in the systematic review, and 18 in the meta-analysis. A pooled weighted average of 1.97\% (N = 7, 95\%CI: 0.86\textendash 4.45) of scientists admitted to have fabricated, falsified or modified data or results at least once \textendash a serious form of misconduct by any standard\textendash{} and up to 33.7\% admitted other questionable research practices. In surveys asking about the behaviour of colleagues, admission rates were 14.12\% (N = 12, 95\% CI: 9.91\textendash 19.72) for falsification, and up to 72\% for other questionable research practices. Meta-regression showed that self reports surveys, surveys using the words ``falsification'' or ``fabrication'', and mailed surveys yielded lower percentages of misconduct. When these factors were controlled for, misconduct was reported more frequently by medical/pharmacological researchers than others. Considering that these surveys ask sensitive questions and have other limitations, it appears likely that this is a conservative estimate of the true prevalence of scientific misconduct.},
  language = {en},
  keywords = {Deception,important,Medical journals,Medicine and health sciences,Metaanalysis,practices,Scientific misconduct,Scientists,Social research,Surveys}
}

@article{fanelli_Opinion_2018,
  title = {Opinion: {{Is}} Science Really Facing a Reproducibility Crisis, and Do We Need It To?},
  shorttitle = {Opinion},
  author = {Fanelli, Daniele},
  year = {2018},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {11},
  pages = {2628--2631},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708272114},
  abstract = {Efforts to improve the reproducibility and integrity of science are typically justified by a narrative of crisis, according to which most published results are unreliable due to growing problems with research and publication practices. This article provides an overview of recent evidence suggesting that this narrative is mistaken, and argues that a narrative of epochal changes and empowerment of scientists would be more accurate, inspiring, and compelling.},
  language = {en},
  keywords = {crisis,forrt}
}

@misc{fernandez_derechos_2009,
  title = {Derechos de {{Autor}}},
  author = {Fern{\'a}ndez, Juan Carlos},
  year = {2009},
  journal = {Derechos de Autor en plataformas e-learning}
}

@inproceedings{fernandez_derechos_2018,
  title = {Derechos de Autor y Ciencia Abierta: El Papel de La Biblioteca Universitaria},
  booktitle = {{{VIII Conferencia Biredial}}-{{ISTEC}}},
  author = {Fern{\'a}ndez, Juan Carlos and Graziosi, Eduardo and Mart{\'i}nez, Daniel},
  year = {2018},
  address = {{Lima - Per\'u}}
}

@inproceedings{fernandezmolina_derechos_2018,
  title = {Derechos de Autor y Ciencia Abierta: El Papel de La Biblioteca Universitaria},
  shorttitle = {Derechos de Autor y Ciencia Abierta},
  booktitle = {{{VIII Conferencia Internacional}} Sobre {{Bibliotecas}} y {{Repositorios Digitales BIREDIAL}}-{{ISTEC}} ({{Lima}}, 2018)},
  author = {Fern{\'a}ndez Molina, Juan Carlos and Graziosi Silva, Eduardo and Mart{\'i}nez {\'A}vila, Daniel},
  year = {2018}
}

@incollection{fidler_Reproducibility_2021,
  title = {Reproducibility of {{Scientific Results}}},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Fidler, Fiona and Wilcox, John},
  editor = {Zalta, Edward N.},
  year = {2021},
  edition = {Summer 2021},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  abstract = {The terms ``reproducibility crisis'' and ``replicationcrisis'' gained currency in conversation and in print over thelast decade (e.g., Pashler \& Wagenmakers 2012), as disappointingresults emerged from large scale reproducibility projects in variousmedical, life and behavioural sciences (e.g., Open ScienceCollaboration, OSC 2015). In 2016, a poll conducted by the journalNature reported that more than half (52\%) of scientistssurveyed believed science was facing a ``replicationcrisis'' (Baker 2016). More recently, some authors have moved tomore positive terms for describing this episode in science; forexample, Vazire (2018) refers instead to a ``credibilityrevolution'' highlighting the improved methods and open sciencepractices it has motivated., The crisis often refers collectively to at least the following things:, The associated open science reform movement aims to rectify conditionsthat led to the crisis. This is done by promoting activities such asdata sharing and public pre-registration of studies, and by advocatingstricter editorial policies around statistical reporting includingpublishing replication studies and statistically non-significantresults., This review consists of four distinct parts. First, we look at theterm ``reproducibility'' and related terms like``repeatability'' and ``replication'', presentingsome definitions and conceptual discussion about the epistemicfunction of different types of replication studies. Second, wedescribe the meta-science research that has established andcharacterised the reproducibility crisis, including large scalereplication projects and surveys of questionable research practices invarious scientific communities. Third, we look at attempts to addressepistemological questions about the limitations of replication, andwhat value it holds for scientific inquiry and the accumulation ofknowledge. The fourth and final part describes some of the manyinitiatives the open science reform movement has proposed (and in manycases implemented) to improve reproducibility in science. In addition,we reflect there on the values and norms which those reforms embody,noting their relevance to the debate about the role of values in thephilosophy of science.}
}

@article{fiedler_Questionable_2016,
  title = {Questionable {{Research Practices Revisited}}},
  author = {Fiedler, Klaus and Schwarz, Norbert},
  year = {2016},
  month = jan,
  journal = {Social Psychological and Personality Science},
  volume = {7},
  number = {1},
  pages = {45--52},
  publisher = {{SAGE Publications Inc}},
  issn = {1948-5506},
  doi = {10.1177/1948550615612150},
  abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes. We conclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
  language = {en},
  keywords = {ethics/morality,language,research methods,research practices,survey methodology}
}

@article{field_effect_2020,
  title = {The Effect of Preregistration on Trust in Empirical Research Findings: Results of a Registered Report},
  shorttitle = {The Effect of Preregistration on Trust in Empirical Research Findings},
  author = {Field, Sarahanne M. and Wagenmakers, E.-J. and Kiers, Henk A. L. and Hoekstra, Rink and Ernst, Anja F. and {van Ravenzwaaij}, Don},
  year = {2020},
  month = apr,
  journal = {Royal Society Open Science},
  volume = {7},
  number = {4},
  pages = {181351},
  publisher = {{Royal Soc}},
  address = {{London}},
  issn = {2054-5703},
  doi = {10.1098/rsos.181351},
  abstract = {The crisis of confidence has undermined the trust that researchers place in the findings of their peers. In order to increase trust in research, initiatives such as preregistration have been suggested, which aim to prevent various questionable research practices. As it stands, however, no empirical evidence exists that preregistration does increase perceptions of trust. The picture may be complicated by a researcher's familiarity with the author of the study, regardless of the preregistration status of the research. This registered report presents an empirical assessment of the extent to which preregistration increases the trust of 209 active academics in the reported outcomes, and how familiarity with another researcher influences that trust. Contrary to our expectations, we report ambiguous Bayes factors and conclude that we do not have strong evidence towards answering our research questions. Our findings are presented along with evidence that our manipulations were ineffective for many participants, leading to the exclusion of 68\% of complete datasets, and an underpowered design as a consequence. We discuss other limitations and confounds which may explain why the findings of the study deviate from a previously conducted pilot study. We reflect on the benefits of using the registered report submission format in light of our results. The OSF page for this registered report and its pilot can be found here: http://dx.doi.org.uchile.idm.oclc.org/10.17605/OSF.IO/B3K75.},
  language = {English},
  keywords = {credibility,preregistration,questionable   research practice,registered reporting,trustworthiness},
  annotation = {WOS:000523781500001}
}

@incollection{figueiredo_Data_2020,
  title = {Data {{Collection With Indigenous People}}: {{Fieldwork Experiences From Chile}}},
  shorttitle = {Data {{Collection With Indigenous People}}},
  booktitle = {Researching {{Peace}}, {{Conflict}}, and {{Power}} in the {{Field}}: {{Methodological Challenges}} and {{Opportunities}}},
  author = {Figueiredo, Ana and Rocha, Carolina and Montagna, Pietro},
  editor = {Acar, Yasemin G{\"u}ls{\"u}m and Moss, Sigrun Marie and Ulu{\u g}, {\"O}zden Melis},
  year = {2020},
  series = {Peace {{Psychology Book Series}}},
  pages = {105--127},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-44113-5_7},
  abstract = {At present, the Mapuche are the largest indigenous group living in Chile and, up until the present day, they are considered a disadvantaged group in Chilean society in terms of poverty, education and discrimination indicators. In recent decades, this group has been involved in a violent conflict with the Chilean state, forestry and hydroelectric industries and big landowners due mainly to territorial claims of the ancestral land that is currently inhabited and exploited by these different actors. In the present chapter, we narrate the process of data collection with indigenous participants within the framework of a three-year long project about representations of history and present-day intergroup relations between the Mapuche and the non-indigenous majority in Chile. We focus on the challenges that data collection involved by highlighting the process of participant recruitment and trust issues revolving around data collection, as well as retribution practices. Moreover, we also highlight the pros and cons of having non-indigenous Chilean and international researchers conducting fieldwork in this context. Another aspect we address is how methodological approaches may influence the data quality and participants' degree of involvement with the project, by highlighting how these issues interconnect with cultural differences and this indigenous group's worldview and cultural practices. We hope this chapter may provide significant insights on how to deal with some of the difficulties that data collection with indigenous people may involve.},
  isbn = {978-3-030-44113-5},
  language = {en},
  keywords = {Chile,Fieldwork,Mapuche,Qualitative research,Quantitative research}
}

@article{figueiredo_Groupbased_2015,
  title = {Group-Based {{Compunction}} and {{Anger}}: {{Their Antecedents}} and {{Consequences}} in {{Relation}} to {{Colonial Conflicts}}},
  shorttitle = {Group-Based {{Compunction}} and {{Anger}}},
  author = {Figueiredo, Ana and Doosje, Bertjan and Valentim, Joaquim Pires},
  year = {2015},
  journal = {International Journal of Conflict and Violence (IJCV)},
  volume = {9},
  pages = {90--105},
  issn = {1864-1385},
  doi = {10.4119/ijcv-3070},
  copyright = {Copyright (c) 2016 International Journal of Conflict and Violence},
  language = {en}
}

@incollection{figueiredo_Representations_2019,
  title = {Representations of History and Present-Day Intergroup Relations between Indigenous and Non-Indigenous People: {{The Mapuche}} in {{Chile}}},
  shorttitle = {Representations of History and Present-Day Intergroup Relations between Indigenous and Non-Indigenous People},
  author = {Figueiredo, Ana and Rocha, Carolina and Montagna, Pietro and Ferreiro, Trinidad and Guerrero, Catarina and Varela O'Reilly, Micaela and Garc{\'i}a, Bernardita and Mu{\~n}oz, Loreto and Schmidt, Magdalena and Cornejo, Marcela and Licata, Laurent},
  year = {2019},
  month = jan,
  pages = {79--104},
  isbn = {978-1-5361-6164-9}
}

@article{figueiredo_Too_2015,
  title = {Too Long to Compensate? {{Time}} Perceptions, Emotions, and Compensation for Colonial Conflicts},
  shorttitle = {Too Long to Compensate?},
  author = {Figueiredo, Ana Mateus and Valentim, Joaquim Pires and Doosje, Bertjan},
  year = {2015},
  journal = {Peace and Conflict: Journal of Peace Psychology},
  volume = {21},
  number = {3},
  pages = {500--504},
  publisher = {{Educational Publishing Foundation}},
  address = {{US}},
  issn = {1532-7949(ELECTRONIC),1078-1919(PRINT)},
  doi = {10.1037/pac0000114},
  abstract = {In the present article we analyze the role of perceptions of time and ingroup-focused compunction and anger on the desire to compensate the outgroup in relation to historical colonial conflicts. Furthermore, we analyze the relationships between the aforementioned variables and perceptions of the past as being violent and perceptions that compensation has been enough. By means of multiple group structural equation modeling using 1 Portuguese sample (N = 170) and 1 Dutch sample (N = 238), we were able to show that perceptions of the time passed between the negative events and the present day are negatively related to compensatory behavioral intentions. Furthermore, the belief that past compensation has been enough is negatively related to ingroup-focused anger and compunction. Anger (Portuguese sample only) and compunction are positively associated with intentions of compensation. The implications of our results for the field of intergroup relations are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Conflict,Emotions,History,Ingroup Outgroup,Time Perception}
}

@article{findley_Can_2016,
  title = {Can {{Results}}-{{Free Review Reduce Publication Bias}}? {{The Results}} and {{Implications}} of a {{Pilot Study}}},
  shorttitle = {Can {{Results}}-{{Free Review Reduce Publication Bias}}?},
  author = {Findley, Michael G. and Jensen, Nathan M. and Malesky, Edmund J. and Pepinsky, Thomas B.},
  year = {2016},
  month = nov,
  journal = {Comparative Political Studies},
  volume = {49},
  number = {13},
  pages = {1667--1703},
  publisher = {{Sage Publications Inc}},
  address = {{Thousand Oaks}},
  issn = {0010-4140},
  doi = {10.1177/0010414016655539},
  abstract = {In 2015, Comparative Political Studies embarked on a landmark pilot study in research transparency in the social sciences. The editors issued an open call for submissions of manuscripts that contained no mention of their actual results, incentivizing reviewers to evaluate manuscripts based on their theoretical contributions, research designs, and analysis plans. The three papers in this special issue are the result of this process that began with 19 submissions. In this article, we describe the rationale for this pilot, expressly articulating the practices of preregistration and results-free review. We document the process of carrying out the special issue with a discussion of the three accepted papers, and critically evaluate the role of both preregistration and results-free review. Our main conclusions are that results-free review encourages much greater attention to theory and research design, but that it raises thorny problems about how to anticipate and interpret null findings. We also observe that as currently practiced, results-free review has a particular affinity with experimental and cross-case methodologies. Our lack of submissions from scholars using qualitative or interpretivist research suggests limitations to the widespread use of results-free review.},
  language = {English},
  keywords = {anonymous incorporation,debt,experimental research,field experiment,growth,journals,null   hypothesis,political-science,preregistration,qualitative methods,quantitative methods,registration,results-free review,transparency,trials},
  annotation = {WOS:000386853700001}
}

@article{flier_Faculty_2017,
  title = {Faculty Promotion Must Assess Reproducibility},
  author = {Flier, Jeffrey},
  year = {2017},
  month = sep,
  journal = {Nature},
  volume = {549},
  number = {7671},
  pages = {133--133},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/549133a},
  abstract = {Research institutions should explicitly seek job candidates who can be frankly self-critical of their work, says Jeffrey Flier.},
  copyright = {2017 Nature Publishing Group},
  language = {en},
  keywords = {forrt}
}

@article{flier_Faculty_2017a,
  title = {Faculty Promotion Must Assess Reproducibility},
  author = {Flier, Jeffrey},
  year = {2017},
  month = sep,
  journal = {Nature},
  volume = {549},
  number = {7671},
  pages = {133--133},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/549133a},
  abstract = {Research institutions should explicitly seek job candidates who can be frankly self-critical of their work, says Jeffrey Flier.},
  copyright = {2017 Nature Publishing Group},
  language = {en},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Comments \& Opinion Subject\_term: Careers;Lab life;Research data;Research management Subject\_term\_id: careers;lab-life;research-data;research-management}
}

@article{flournoy_Improving_2020,
  title = {Improving Practices and Inferences in Developmental Cognitive Neuroscience},
  author = {Flournoy, John C. and Vijayakumar, Nandita and Cheng, Theresa W. and Cosme, Danielle and Flannery, Jessica E. and Pfeifer, Jennifer H.},
  year = {2020},
  month = oct,
  journal = {Developmental Cognitive Neuroscience},
  volume = {45},
  pages = {100807},
  publisher = {{Elsevier Sci Ltd}},
  address = {{Oxford}},
  issn = {1878-9293},
  doi = {10.1016/j.dcn.2020.100807},
  abstract = {The past decade has seen growing concern about research practices in cognitive neuroscience, and psychology more broadly, that shake our confidence in many inferences in these fields. We consider how these issues affect developmental cognitive neuroscience, with the goal of progressing our field to support strong and defensible inferences from our neurobiological data. This manuscript focuses on the importance of distinguishing between confirmatory versus exploratory data analysis approaches in developmental cognitive neuroscience. Regarding confirmatory research, we discuss problems with analytic flexibility, appropriately instantiating hypotheses, and controlling the error rate given how we threshold data and correct for multiple comparisons. To counterbalance these concerns with confirmatory analyses, we present two complementary strategies. First, we discuss the advantages of working within an exploratory analysis framework, including estimating and reporting effect sizes, using parcellations, and conducting specification curve analyses. Second, we summarize defensible approaches for null hypothesis significance testing in confirmatory analyses, focusing on transparent and reproducible practices in our field. Specific recommendations are given, and templates, scripts, or other resources are hyperlinked, whenever possible.},
  language = {English},
  keywords = {Exploratory,extent,Inference,parcellation,Parcellations,Preregistration,primer,Reproducibility,size,tests,Thresholding},
  annotation = {WOS:000579499600015}
}

@article{forstmeier_Detecting_2017,
  title = {Detecting and Avoiding Likely False-Positive Findings - a Practical Guide},
  author = {Forstmeier, Wolfgang and Wagenmakers, Eric-Jan and Parker, Timothy H.},
  year = {2017},
  month = nov,
  journal = {Biological Reviews},
  volume = {92},
  number = {4},
  pages = {1941--1968},
  publisher = {{Wiley}},
  address = {{Hoboken}},
  issn = {1464-7931},
  doi = {10.1111/brv.12315},
  abstract = {Recently there has been a growing concern that many published research findings do not hold up in attempts to replicate them. We argue that this problem may originate from a culture of you can publish if you found a significant effect'. This culture creates a systematic bias against the null hypothesis which renders meta-analyses questionable and may even lead to a situation where hypotheses become difficult to falsify. In order to pinpoint the sources of error and possible solutions, we review current scientific practices with regard to their effect on the probability of drawing a false-positive conclusion. We explain why the proportion of published false-positive findings is expected to increase with (i) decreasing sample size, (ii) increasing pursuit of novelty, (iii) various forms of multiple testing and researcher flexibility, and (iv) incorrect P-values, especially due to unaccounted pseudoreplication, i.e. the non-independence of data points (clustered data). We provide examples showing how statistical pitfalls and psychological traps lead to conclusions that are biased and unreliable, and we show how these mistakes can be avoided. Ultimately, we hope to contribute to a culture of you can publish if your study is rigorous'. To this end, we highlight promising strategies towards making science more objective. Specifically, we enthusiastically encourage scientists to preregister their studies (including a priori hypotheses and complete analysis plans), to blind observers to treatment groups during data collection and analysis, and unconditionally to report all results. Also, we advocate reallocating some efforts away from seeking novelty and discovery and towards replicating important research findings of one's own and of others for the benefit of the scientific community as a whole. We believe these efforts will be aided by a shift in evaluation criteria away from the current system which values metrics of impact' almost exclusively and towards a system which explicitly values indices of scientific rigour.},
  language = {English},
  keywords = {behavioral ecology,confirmation bias,effect sizes,evolution,HARKing,hindsight bias,incentives,metaanalyses,overfitting,P-hacking,power,preregistration,publication bias,replication,researcher degrees of freedom,statistical power,tests,transparency,truth,Type   I error},
  annotation = {WOS:000412314400005}
}

@misc{fortney_social_2015,
  title = {A Social Networking Site Is Not an Open Access Repository},
  author = {Fortney, Katie and Gonder, Justin},
  year = {2015},
  journal = {Office of Scholary Communication University of California}
}

@article{franca_Reproducibility_2019,
  title = {Reproducibility Crisis, the Scientific Method, and the Quality of Published Studies: {{Untangling}} the Knot},
  shorttitle = {Reproducibility Crisis, the Scientific Method, and the Quality of Published Studies},
  author = {Fran{\c c}a, Thiago F. A. and Monserrat, Jos{\'e} Maria},
  year = {2019},
  month = oct,
  journal = {Learned Publishing},
  volume = {32},
  number = {4},
  pages = {406--408},
  issn = {0953-1513, 1741-4857},
  doi = {10.1002/leap.1250},
  language = {en},
  keywords = {crisis}
}

@article{franco_Publication_2014,
  title = {Publication Bias in the Social Sciences: {{Unlocking}} the File Drawer},
  shorttitle = {Publication Bias in the Social Sciences},
  author = {Franco, A. and Malhotra, N. and Simonovits, G.},
  year = {2014},
  month = sep,
  journal = {Science},
  volume = {345},
  number = {6203},
  pages = {1502--1505},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1255484},
  language = {en},
  keywords = {practices}
}

@article{frankenhuis_Open_2018,
  title = {Open {{Science Is Liberating}} and {{Can Foster Creativity}}},
  author = {Frankenhuis, Willem E. and Nettle, Daniel},
  year = {2018},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {13},
  number = {4},
  pages = {439--447},
  publisher = {{Sage Publications Ltd}},
  address = {{London}},
  issn = {1745-6916},
  doi = {10.1177/1745691618767878},
  abstract = {Some scholars think that Open Science practices constrain researchers in ways that reduce their creativity, arguing, for instance, that preregistration discourages data exploration and so stifles discovery. In this article, we argue the opposite: Open Science practices are liberating and can foster creativity. Open Science practices are liberating because they (a) enable us to explore data transparently and comfortably; (b) reward quality, which is under our control, rather than outcomes, which are not; and (c) reduce the choke hold of needing to find "positive" results for career advancement. Open Science practices can foster creativity because they cultivate an open and flexible mind-set, create a more collaborative and constructive climate, and generate more accurate information and make it more accessible. In sum, Open Science liberates researchers more than it constrains them.},
  language = {English},
  keywords = {competition,creativity,doubt,information,knowledge,open science,preregistration,psychological science,registered-reports,replicability,replication,reproducibility,scientific utopia,standard,uncertainty},
  annotation = {WOS:000438605100005}
}

@article{fraser_Questionable_2018,
  title = {Questionable Research Practices in Ecology and Evolution},
  author = {Fraser, Hannah and Parker, Tim and Nakagawa, Shinichi and Barnett, Ashley and Fidler, Fiona},
  year = {2018},
  month = jul,
  journal = {PLOS ONE},
  volume = {13},
  number = {7},
  pages = {e0200303},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0200303},
  abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64\% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42\% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51\% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
  language = {en},
  keywords = {Behavioral ecology,Community ecology,Evolutionary biology,Evolutionary ecology,Evolutionary rate,Psychology,Publication ethics,Statistical data}
}

@article{freese_Replication_2017,
  title = {Replication in {{Social Science}}},
  author = {Freese, Jeremy and Peterson, David},
  year = {2017},
  month = jul,
  journal = {Annual Review of Sociology},
  volume = {43},
  number = {1},
  pages = {147--165},
  publisher = {{Annual Reviews}},
  issn = {0360-0572},
  doi = {10.1146/annurev-soc-060116-053450},
  abstract = {Across the medical and social sciences, new discussions about replication have led to transformations in research practice. Sociologists, however, have been largely absent from these discussions. The goals of this review are to introduce sociologists to these developments, synthesize insights from science studies about replication in general, and detail the specific issues regarding replication that occur in sociology. The first half of the article argues that a sociologically sophisticated understanding of replication must address both the ways that replication rules and conventions evolved within an epistemic culture and how those cultures are shaped by specific research challenges. The second half outlines the four main dimensions of replicability in quantitative sociology\textemdash verifiability, robustness, repeatability, and generalizability\textemdash and discusses the specific ambiguities of interpretation that can arise in each. We conclude by advocating some commonsense changes to promote replication while acknowledging the epistemic diversity of our field.}
}

@article{frey_Publishing_2003,
  title = {Publishing as {{Prostitution}}? \textendash{} {{Choosing Between One}}'s {{Own Ideas}} and {{Academic Success}}},
  shorttitle = {Publishing as {{Prostitution}}?},
  author = {Frey, Bruno S.},
  year = {2003},
  month = jul,
  journal = {Public Choice},
  volume = {116},
  number = {1},
  pages = {205--223},
  issn = {1573-7101},
  doi = {10.1023/A:1024208701874},
  abstract = {Survival in academia depends on publications in refereedjournals. Authors only get their papers accepted if theyintellectually prostitute themselves by slavishly followingthe demands made by anonymous referees who have no propertyrights to the journals they advise. Intellectual prostitutionis neither beneficial to suppliers nor consumers. But it isavoidable. The editor (with property rights to the journal)should make the basic decision of whether a paper is worthpublishing or not. The referees should only offer suggestionsfor improvement. The author may disregard this advice. Thisreduces intellectual prostitution and produces more originalpublications.},
  language = {en},
  keywords = {institutional}
}

@article{friese_pHacking_2020,
  title = {P-{{Hacking}} and {{Publication Bias Interact}} to {{Distort Meta}}-{{Analytic Effect Size Estimates}}},
  author = {Friese, Malte and Frankenbach, Julius},
  year = {2020},
  month = aug,
  journal = {Psychological Methods},
  volume = {25},
  number = {4},
  pages = {456--471},
  publisher = {{Amer Psychological Assoc}},
  address = {{Washington}},
  issn = {1082-989X},
  doi = {10.1037/met0000246},
  abstract = {Science depends on trustworthy evidence. Thus, a biased scientific record is of questionable value because it impedes scientific progress, and the public receives advice on the basis of unreliable evidence that has the potential to have far-reaching detrimental consequences. Meta-analysis is a technique that can be used to summarize research evidence. However, meta-analytic effect size estimates may themselves be biased, threatening the validity and usefulness of meta-analyses to promote scientific progress. Here, we offer a large-scale simulation study to elucidate how p-hacking and publication bias distort meta-analytic effect size estimates under a broad array of circumstances that reflect the reality that exists across a variety of research areas. The results revealed that, first, very high levels of publication bias can severely distort the cumulative evidence. Second, p-hacking and publication bias interact: At relatively high and low levels of publication bias, p-hacking does comparatively little harm, but at medium levels of publication bias, p-hacking can considerably contribute to bias, especially when the true effects are very small or are approaching zero. Third, p-hacking can severely increase the rate of false positives. A key implication is that, in addition to preventing p-hacking, policies in research institutions, funding agencies, and scientific journals need to make the prevention of publication bias a top priority to ensure a trustworthy base of evidence. Translational Abstract In recent years, the trustworthiness of psychological science has been questioned. A major concern is that many research findings are less robust than the published evidence suggests. Several reasons may contribute to this state of affairs. Two prominently discussed reasons are that (a) researchers use questionable research practices (so called p-hacking) when they analyze the data of their empirical studies, and (b) studies that revealed results consistent with expectations are more likely published than studies that "failed" (publication bias). The present large-scale simulation study estimates the extent to which meta-analytic effect sizes are biased by different degrees of p-hacking and publication bias, considering several factors of influence that may impact on this bias (e.g., the true effect of the phenomenon of interest). Results show that both p-hacking and publication bias contribute to a potentially severely biased impression of the overall evidence. This is especially the case when the true effect that is investigated is very small or does not exist at all. Severe publication bias alone can exert considerable bias; p-hacking exerts considerable bias only when there is also publication bias. However, p-hacking can severely increase the rate of false positives, that is, findings that suggest that a study found a real effect when, in reality, no effect exists. A key implication of the present study is that, in addition to preventing p-hacking, policies in research institutions, funding agencies, and scientific journals need to make the prevention of publication bias a top priority to ensure a trustworthy base of evidence.},
  language = {English},
  keywords = {curve,decisions,incentives,meta-analysis,meta-regression,metascience,p-hacking,prevalence,psychological-research,publication bias,registered-reports,robust,science,tests},
  annotation = {WOS:000563801800004}
}

@article{friese_pHacking_2020a,
  title = {P-{{Hacking}} and {{Publication Bias Interact}} to {{Distort Meta}}-{{Analytic Effect Size Estimates}}},
  author = {Friese, Malte and Frankenbach, Julius},
  year = {2020},
  month = aug,
  journal = {Psychological Methods},
  volume = {25},
  number = {4},
  pages = {456--471},
  publisher = {{Amer Psychological Assoc}},
  address = {{Washington}},
  issn = {1082-989X},
  doi = {10.1037/met0000246},
  abstract = {Science depends on trustworthy evidence. Thus, a biased scientific record is of questionable value because it impedes scientific progress, and the public receives advice on the basis of unreliable evidence that has the potential to have far-reaching detrimental consequences. Meta-analysis is a technique that can be used to summarize research evidence. However, meta-analytic effect size estimates may themselves be biased, threatening the validity and usefulness of meta-analyses to promote scientific progress. Here, we offer a large-scale simulation study to elucidate how p-hacking and publication bias distort meta-analytic effect size estimates under a broad array of circumstances that reflect the reality that exists across a variety of research areas. The results revealed that, first, very high levels of publication bias can severely distort the cumulative evidence. Second, p-hacking and publication bias interact: At relatively high and low levels of publication bias, p-hacking does comparatively little harm, but at medium levels of publication bias, p-hacking can considerably contribute to bias, especially when the true effects are very small or are approaching zero. Third, p-hacking can severely increase the rate of false positives. A key implication is that, in addition to preventing p-hacking, policies in research institutions, funding agencies, and scientific journals need to make the prevention of publication bias a top priority to ensure a trustworthy base of evidence. Translational Abstract In recent years, the trustworthiness of psychological science has been questioned. A major concern is that many research findings are less robust than the published evidence suggests. Several reasons may contribute to this state of affairs. Two prominently discussed reasons are that (a) researchers use questionable research practices (so called p-hacking) when they analyze the data of their empirical studies, and (b) studies that revealed results consistent with expectations are more likely published than studies that "failed" (publication bias). The present large-scale simulation study estimates the extent to which meta-analytic effect sizes are biased by different degrees of p-hacking and publication bias, considering several factors of influence that may impact on this bias (e.g., the true effect of the phenomenon of interest). Results show that both p-hacking and publication bias contribute to a potentially severely biased impression of the overall evidence. This is especially the case when the true effect that is investigated is very small or does not exist at all. Severe publication bias alone can exert considerable bias; p-hacking exerts considerable bias only when there is also publication bias. However, p-hacking can severely increase the rate of false positives, that is, findings that suggest that a study found a real effect when, in reality, no effect exists. A key implication of the present study is that, in addition to preventing p-hacking, policies in research institutions, funding agencies, and scientific journals need to make the prevention of publication bias a top priority to ensure a trustworthy base of evidence.},
  language = {English},
  keywords = {curve,decisions,incentives,meta-analysis,meta-regression,metascience,p-hacking,prevalence,psychological-research,publication bias,registered-reports,robust,science,tests},
  annotation = {WOS:000563801800004}
}

@article{frost_Calibrate_2020,
  title = {Calibrate Your Confidence in Research Findings: {{A}} Tutorial on Improving Research Methods and Practices},
  shorttitle = {Calibrate Your Confidence in Research Findings},
  author = {Frost, Aline da Silva and Ledgerwood, Alison},
  year = {2020},
  journal = {Journal of Pacific Rim Psychology},
  volume = {14},
  pages = {e14},
  publisher = {{Cambridge Univ Press}},
  address = {{New York}},
  issn = {1834-4909},
  doi = {10.1017/prp.2020.7},
  abstract = {This article provides an accessible tutorial with concrete guidance for how to start improving research methods and practices in your lab. Following recent calls to improve research methods and practices within and beyond the borders of psychological science, resources have proliferated across book chapters, journal articles, and online media. Many researchers are interested in learning more about cutting-edge methods and practices but are unsure where to begin. In this tutorial, we describe specific tools that help researchers calibrate their confidence in a given set of findings. In Part I, we describe strategies for assessing the likely statistical power of a study, including when and how to conduct different types of power calculations, how to estimate effect sizes, and how to think about power for detecting interactions. In Part II, we provide strategies for assessing the likely type I error rate of a study, including distinguishing clearly between data-independent ("confirmatory") and data-dependent ("exploratory") analyses and thinking carefully about different forms and functions of preregistration.},
  language = {English},
  keywords = {crisis,incentives,open science,positive predictive value,preanalysis plan,preregistration,publication bias,reliability,replicability,sample-size,science,special section,statistical power,trade-off},
  annotation = {WOS:000527960000001}
}

@article{gall_credibility_2017,
  title = {The Credibility Crisis in Research: {{Can}} Economics Tools Help?},
  shorttitle = {The Credibility Crisis in Research},
  author = {Gall, Thomas and Ioannidis, John P. A. and Maniadis, Zacharias},
  year = {2017},
  month = apr,
  journal = {PLOS Biology},
  volume = {15},
  number = {4},
  pages = {e2001846},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2001846},
  abstract = {The issue of nonreplicable evidence has attracted considerable attention across biomedical and other sciences. This concern is accompanied by an increasing interest in reforming research incentives and practices. How to optimally perform these reforms is a scientific problem in itself, and economics has several scientific methods that can help evaluate research reforms. Here, we review these methods and show their potential. Prominent among them are mathematical modeling and laboratory experiments that constitute affordable ways to approximate the effects of policies with wide-ranging implications.},
  language = {en},
  keywords = {crisis,Economic models,Economics,Experimental economics,Game theory,Health economics,Labor economics,Mathematical modeling,Randomized controlled trials}
}

@article{gerber_Publication_2008,
  title = {Publication {{Bias}} in {{Empirical Sociological Research}}: {{Do Arbitrary Significance Levels Distort Published Results}}?},
  shorttitle = {Publication {{Bias}} in {{Empirical Sociological Research}}},
  author = {Gerber, Alan S. and Malhotra, Neil},
  year = {2008},
  month = aug,
  journal = {Sociological Methods \& Research},
  volume = {37},
  number = {1},
  pages = {3--30},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124108318973},
  abstract = {Despite great attention to the quality of research methods in individual studies, if publication decisions of journals are a function of the statistical significance of research findings, the published literature as a whole may not produce accurate measures of true effects. This article examines the two most prominent sociology journals (the American Sociological Review and the American Journal of Sociology) and another important though less influential journal (The Sociological Quarterly) for evidence of publication bias. The effect of the .05 significance level on the pattern of published findings is examined using a ``caliper'' test, and the hypothesis of no publication bias can be rejected at approximately the 1 in 10 million level. Findings suggest that some of the results reported in leading sociology journals may be misleading and inaccurate due to publication bias. Some reasons for publication bias and proposed reforms to reduce its impact on research are also discussed.},
  language = {en},
  keywords = {caliper test,hypothesis testing,meta-analysis,practices,publication bias}
}

@article{gerber_Statistical_2008,
  title = {Do {{Statistical Reporting Standards Affect What Is Published}}? {{Publication Bias}} in {{Two Leading Political Science Journals}}},
  shorttitle = {Do {{Statistical Reporting Standards Affect What Is Published}}?},
  author = {Gerber, Alan and Malhotra, Neil},
  year = {2008},
  month = oct,
  journal = {Quarterly Journal of Political Science},
  volume = {3},
  number = {3},
  pages = {313--326},
  publisher = {{Now Publishers, Inc.}},
  issn = {1554-0626, 1554-0634},
  doi = {10.1561/100.00008024},
  abstract = {Do Statistical Reporting Standards Affect What Is Published? Publication Bias in Two Leading Political Science Journals},
  language = {English},
  keywords = {practices}
}

@article{geukes_Ways_2016,
  title = {{Ways Out of the Crisis of Confidence: Individual Steps Toward a Reliable and Open Science}},
  shorttitle = {{Ways Out of the Crisis of Confidence}},
  author = {Geukes, Katharina and Schoenbrodt, Felix D. and Utesch, Till and Geukes, Sebastian and Back, Mitja D.},
  year = {2016},
  journal = {Zeitschrift Fur Sportpsychologie},
  volume = {23},
  number = {3},
  pages = {99--109},
  publisher = {{Hogrefe \& Huber Publishers}},
  address = {{Gottingen}},
  issn = {1612-5010},
  doi = {10.1026/1612-5010/a000167},
  abstract = {Psychology faces a so-called crisis of confidence as does sport psychology (see title of this special issue). While the debate on its causes and consequences is lively, the deduction of individual opportunities to collectively increase trust is missing. We propose ways out of this crisis and above all describe individual steps toward a reliable and open science. Reliable science refers to the publication of robust effects, as well as to direct and conceptual replications, and open science refers to transparency regarding the design (preregistration), the conducting (open material), and the analysis (open data, reproducible code) of scientific studies. The commitment to reliable and open science wilt change our behavior in the diverse roles within the scientific system (e.g., as researchers, reviewers, supervisors, editors, or members of commissions). In this sense, we consider the current discussion as a chance to enhance the trustworthiness of our findings and to ultimately create justified confidence.},
  language = {German},
  keywords = {crisis of confidence,exercise,incentives,open science,preregistration,psychological-research,recommendations,registered-reports,replicability,replication,research practices,special section,sport,truth},
  annotation = {WOS:000392884700005}
}

@article{gilbert_Comment_2016,
  title = {Comment on "{{Estimating}} the Reproducibility of Psychological Science"},
  author = {Gilbert, D. T. and King, G. and Pettigrew, S. and Wilson, T. D.},
  year = {2016},
  month = mar,
  journal = {Science},
  volume = {351},
  number = {6277},
  pages = {1037--1037},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aad7243},
  language = {en}
}

@article{goodman_What_2016,
  title = {What Does Research Reproducibility Mean?},
  author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
  year = {2016},
  month = jun,
  journal = {Science Translational Medicine},
  volume = {8},
  number = {341},
  pages = {341ps12-341ps12},
  publisher = {{American Association for the Advancement of Science}},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.aaf5027},
  abstract = {{$<$}p{$>$}The language and conceptual framework of ``research reproducibility'' are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for ``truth.''{$<$}/p{$>$}},
  chapter = {Perspective},
  copyright = {Copyright \textcopyright{} 2016, American Association for the Advancement of Science},
  language = {en},
  pmid = {27252173}
}

@article{gorman_Systems_2019,
  title = {A {{Systems Approach}} to {{Understanding}} and {{Improving Research Integrity}}},
  author = {Gorman, Dennis M. and Elkins, Amber D. and Lawley, Mark},
  year = {2019},
  month = feb,
  journal = {Science and Engineering Ethics},
  volume = {25},
  number = {1},
  pages = {211--229},
  publisher = {{Springer}},
  address = {{Dordrecht}},
  issn = {1353-3452},
  doi = {10.1007/s11948-017-9986-z},
  abstract = {Concern about the integrity of empirical research has arisen in recent years in the light of studies showing the vast majority of publications in academic journals report positive results, many of these results are false and cannot be replicated, and many positive results are the product of data dredging and the application of flexible data analysis practices coupled with selective reporting. While a number of potential solutions have been proposed, the effects of these are poorly understood and empirical evaluation of each would take many years. We propose that methods from the systems sciences be used to assess the effects, both positive and negative, of proposed solutions to the problem of declining research integrity such as study registration, Registered Reports, and open access to methods and data. In order to illustrate the potential application of systems science methods to the study of research integrity, we describe three broad types of models: one built on the characteristics of specific academic disciplines; one a diffusion of research norms model conceptualizing researchers as susceptible, infected and recovered; and one conceptualizing publications as a product produced by an industry comprised of academics who respond to incentives and disincentives.},
  language = {English},
  keywords = {aversion,bias,diffusion,dynamics,impact,medicine,Open data,programs,publication,Publish or perish,registered reports,Registered reports,Research ethics,science,System dynamics,Systems thinking},
  annotation = {WOS:000461310400012}
}

@article{grant_Lack_2018,
  title = {Lack of Preregistered Analysis Plans Allows Unacceptable Data Mining for and Selective Reporting of Consensus in {{Delphi}} Studies},
  author = {Grant, Sean and Booth, Marika and Khodyakov, Dmitry},
  year = {2018},
  month = jul,
  journal = {Journal of Clinical Epidemiology},
  volume = {99},
  pages = {96--105},
  publisher = {{Elsevier Science Inc}},
  address = {{New York}},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2018.03.007},
  abstract = {Objectives: To empirically demonstrate how undisclosed analytic flexibility provides substantial latitude for data mining and selective reporting of consensus in Delphi processes. Study Design and Setting: Pooling data across eight online modified-Delphi panels, we first calculated the percentage of items reaching consensus according to descriptive analysis procedures commonly used in health research but selected post hoc in this article. We then examined the variability of items reaching consensus across panels. Results: Pooling all panel data, the percentage of items reaching consensus ranged from 0\% to 84\%, depending on the analysis procedure. Comparing data across panels, variability in the percentage of items reaching consensus for each analysis procedure ranged from 0 (i.e., all panels had the same percentage of items reaching consensus for a given analysis procedure) to 83 (i.e., panels had a range of 11\% to 94\% of items reaching consensus for a given analysis procedure). Of 200 total panel-by-analysis-procedure configurations, four configurations (2\%) had all items and 64 (32\%) had no items reaching consensus. Conclusion: Undisclosed analytic flexibility makes it unacceptably easy to data mine for and selectively report consensus in Delphi processes. As a solution, we recommend prospective, complete registration of preanalysis plans for consensus-oriented Delphi processes in health research. (C) 2018 Elsevier Inc. All rights reserved.},
  language = {English},
  keywords = {bias,clinical-trials,Consensus,consort,criteria,Delphi process,Expert panel,future,Open science framework,Preanalysis plan,Preregistration,registration,stability},
  annotation = {WOS:000437393100011}
}

@article{greiff_Introducing_2020,
  title = {Introducing {{New Open Science Practices}} at {{EJPA}}},
  author = {Greiff, Samuel and {van der Westhuizen}, Lindie and Mund, Marcus and Rauthmann, John F. and Wetzel, Eunike},
  year = {2020},
  month = sep,
  journal = {European Journal of Psychological Assessment},
  volume = {36},
  number = {5},
  pages = {717--720},
  publisher = {{Hogrefe Publishing Corp}},
  address = {{Boston}},
  issn = {1015-5759},
  doi = {10.1027/1015-5759/a000628},
  language = {English},
  keywords = {registered reports},
  annotation = {WOS:000595143100001}
}

@article{gundersen_fundamental_2021,
  title = {The Fundamental Principles of Reproducibility},
  author = {Gundersen, Odd Erik},
  year = {2021},
  month = may,
  journal = {Philosophical Transactions of the Royal Society a-Mathematical Physical and Engineering Sciences},
  volume = {379},
  number = {2197},
  pages = {20200210},
  publisher = {{Royal Soc}},
  address = {{London}},
  issn = {1364-503X},
  doi = {10.1098/rsta.2020.0210},
  abstract = {Reproducibility is a confused terminology. In this paper, I take a fundamental view on reproducibility rooted in the scientific method. The scientific method is analysed and characterized in order to develop the terminology required to define reproducibility. Furthermore, the literature on reproducibility and replication is surveyed, and experiments are modelled as tasks and problem solving methods. Machine learning is used to exemplify the described approach. Based on the analysis, reproducibility is defined and three different degrees of reproducibility as well as four types of reproducibility are specified. This article is part of the theme issue 'Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification in silico'.},
  language = {English},
  keywords = {go,level,machine learning,problem solving methods,registered-reports,scientific method},
  annotation = {WOS:000636259600012}
}

@article{guttinger_limits_2020,
  title = {The Limits of Replicability},
  author = {Guttinger, Stephan},
  year = {2020},
  month = jan,
  journal = {European Journal for Philosophy of Science},
  volume = {10},
  number = {2},
  pages = {10},
  issn = {1879-4920},
  doi = {10.1007/s13194-019-0269-1},
  abstract = {Discussions about a replicability crisis in science have been driven by the normative claim that all of science should be replicable and the empirical claim that most of it isn't. Recently, such crisis talk has been challenged by a new localism, which argues a) that serious problems with replicability are not a general occurrence in science and b) that replicability itself should not be treated as a universal standard. The goal of this article is to introduce this emerging strand of the debate and to discuss some of its implications and limitations. I will in particular highlight the issue of demarcation that localist accounts have to address, i.e. the question of how we can distinguish replicable science from disciplines where replicability does not apply.},
  language = {en}
}

@article{hales_Improving_2019,
  title = {Improving {{Psychological Science}} through {{Transparency}} and {{Openness}}: {{An Overview}}},
  shorttitle = {Improving {{Psychological Science}} through {{Transparency}} and {{Openness}}},
  author = {Hales, Andrew H. and Wesselmann, Eric D. and Hilgard, Joseph},
  year = {2019},
  month = mar,
  journal = {Perspectives on Behavior Science},
  volume = {42},
  number = {1},
  pages = {13--31},
  issn = {2520-8977},
  doi = {10.1007/s40614-018-00186-8},
  abstract = {The ability to independently verify and replicate observations made by other researchers is a hallmark of science. In this article, we provide an overview of recent discussions concerning replicability and best practices in mainstream psychology with an emphasis on the practical benefists to both researchers and the field as a whole. We first review challenges individual researchers face in producing research that is both publishable and reliable. We then suggest methods for producing more accurate research claims, such as transparently disclosing how results were obtained and analyzed, preregistering analysis plans, and publicly posting original data and materials. We also discuss ongoing changes at the institutional level to incentivize stronger research. These include officially recognizing open science practices at the journal level, disconnecting the publication decision from the results of a study, training students to conduct replications, and publishing replications. We conclude that these open science practices afford exciting low-cost opportunities to improve the quality of psychological science.},
  language = {en}
}

@article{hales_Improving_2019a,
  title = {Improving {{Psychological Science}} through {{Transparency}} and {{Openness}}: {{An Overview}}},
  shorttitle = {Improving {{Psychological Science}} through {{Transparency}} and {{Openness}}},
  author = {Hales, Andrew H. and Wesselmann, Eric D. and Hilgard, Joseph},
  year = {2019},
  month = mar,
  journal = {Perspectives on Behavior Science},
  volume = {42},
  number = {1},
  pages = {13--31},
  publisher = {{Springer International Publishing Ag}},
  address = {{Cham}},
  issn = {2520-8969},
  doi = {10.1007/s40614-018-00186-8},
  abstract = {The ability to independently verify and replicate observations made by other researchers is a hallmark of science. In this article, we provide an overview of recent discussions concerning replicability and best practices in mainstream psychology with an emphasis on the practical benefists to both researchers and the field as a whole. We first review challenges individual researchers face in producing research that is both publishable and reliable. We then suggest methods for producing more accurate research claims, such as transparently disclosing how results were obtained and analyzed, preregistering analysis plans, and publicly posting original data and materials. We also discuss ongoing changes at the institutional level to incentivize stronger research. These include officially recognizing open science practices at the journal level, disconnecting the publication decision from the results of a study, training students to conduct replications, and publishing replications. We conclude that these open science practices afford exciting low-cost opportunities to improve the quality of psychological science.},
  language = {English},
  keywords = {behavior analysis,consequences,incentives,Meta-analysis,metaanalysis,power,Preregistration,recommendations,rejection,replicability,replication,Replication,Reproducibility,statistical-inference},
  annotation = {WOS:000464867500002}
}

@article{hartgerink_Reanalyzing_2017,
  title = {Reanalyzing {{Head}} et al. (2015): Investigating the Robustness of Widespread p-Hacking},
  shorttitle = {Reanalyzing {{Head}} et al. (2015)},
  author = {Hartgerink, Chris H. J.},
  year = {2017},
  month = mar,
  journal = {Peerj},
  volume = {5},
  pages = {e3068},
  publisher = {{Peerj Inc}},
  address = {{London}},
  issn = {2167-8359},
  doi = {10.7717/peerj.3068},
  abstract = {Heal et al. (2015) provided a large collection of p-values that, from their perspective, indicates widespread statistical significance seeking (i.e., p-hacking). This paper inspects this result for robustness. Theoretically, the p-value distribution should be a smooth, decreasing function, but the distribution of reported p-values shows systematically more reported p-values for .01, .02, .03, .04, and .05 than p-values reported to three decimal places, due to apparent tendencies to round p-values to two decimal places. Head et al. (2015) correctly argue that an aggregate p-value distribution could show a bump below .05 when left-skew p-hacking occurs frequently. Moreover, the elimination of p = .045 and p = .05, as done in the original paper, is debatable. Given that eliminating p = .045 is a result of the need for symmetric bins and systematically more p-values are reported to two decimal places than to three decimal places, I did not exclude p = .045 and p = .05. I conducted Fishers method .04 {$<$} p {$<$} .05 and reanalyzed the data by adjusting the bin selection to .03875 {$<$} p {$<$}= .04 versus .04875 {$<$} p {$<$}= .05. Results of the reanalysis indicate that no evidence for left-skew p-hacking remains when we look at the entire range between .04 {$<$} p {$<$} .05 or when we inspect the second-decimal. Taking into account reporting tendencies when selecting the bins to compare is especially important because this dataset does not allow for the recalculation of the p-values. Moreover, inspecting the bins that include two-decimal reported p-values potentially increases sensitivity if strategic rounding down of p-values as a form of p-hacking is widespread. Given the far-reaching implications of supposed widespread p-hacking throughout the sciences Head et al. (2015), it is important that these findings are robust to data analysis choices if the conclusion is to be considered unequivocal. Although no evidence of widespread left-skew p-hacking is found in this reanalysis, this does not mean that there is no p-hacking at all. These results nuance the conclusion by Head et al. (2015), indicating that the results are not robust and that the evidence for widespread left-skew p-hacking is ambiguous at best.},
  language = {English},
  keywords = {curve analysis,Nhst,P-hacking,P-values,Qrps,Questionable research practices,Reanalysis,values},
  annotation = {WOS:000396903800009}
}

@article{haven_Preregistering_2019,
  title = {Preregistering Qualitative Research},
  author = {Haven, Tamarinde L. and Grootel, Dr Leonie Van},
  year = {2019},
  month = apr,
  journal = {Accountability in Research},
  volume = {26},
  number = {3},
  pages = {229--244},
  publisher = {{Taylor \& Francis}},
  issn = {0898-9621},
  doi = {10.1080/08989621.2019.1580147},
  abstract = {The threat to reproducibility and awareness of current rates of research misbehavior sparked initiatives to better academic science. One initiative is preregistration of quantitative research. We investigate whether the preregistration format could also be used to boost the credibility of qualitative research. A crucial distinction underlying preregistration is that between prediction and postdiction. In qualitative research, data are used to decide which way interpretation should move forward, using data to generate hypotheses and new research questions. Qualitative research is thus a real-life example of postdiction research. Some may object to the idea of preregistering qualitative studies because qualitative research generally does not test hypotheses, and because qualitative research design is typically flexible and subjective. We rebut these objections, arguing that making hypotheses explicit is just one feature of preregistration, that flexibility can be tracked using preregistration, and that preregistration would provide a check on subjectivity. We then contextualize preregistrations alongside another initiative to enhance credibility in qualitative research: the confirmability audit. Besides, preregistering qualitative studies is practically useful to combating dissemination bias and could incentivize qualitative researchers to report constantly on their study's development. We conclude with suggested modifications to the Open Science Framework preregistration form to tailor it for qualitative studies.},
  pmid = {30741570},
  keywords = {Preregistration,qualitative research,reports,transparency},
  annotation = {\_eprint: https://doi.org/10.1080/08989621.2019.1580147}
}

@article{haven_Preregistering_2020,
  title = {Preregistering {{Qualitative Research}}: {{A Delphi Study}}},
  shorttitle = {Preregistering {{Qualitative Research}}},
  author = {Haven, Tamarinde L. and Errington, Timothy M. and Gleditsch, Kristian Skrede and {van Grootel}, Leonie and Jacobs, Alan M. and Kern, Florian G. and Pineiro, Rafael and Rosenblatt, Fernando and Mokkink, Lidwine B.},
  year = {2020},
  month = dec,
  journal = {International Journal of Qualitative Methods},
  volume = {19},
  pages = {1609406920976417},
  publisher = {{Sage Publications Inc}},
  address = {{Thousand Oaks}},
  issn = {1609-4069},
  doi = {10.1177/1609406920976417},
  abstract = {Preregistrations-records made a priori about study designs and analysis plans and placed in open repositories-are thought to strengthen the credibility and transparency of research. Different authors have put forth arguments in favor of introducing this practice in qualitative research and made suggestions for what to include in a qualitative preregistration form. The goal of this study was to gauge and understand what parts of preregistration templates qualitative researchers would find helpful and informative. We used an online Delphi study design consisting of two rounds with feedback reports in between. In total, 48 researchers participated (response rate: 16\%). In round 1, panelists considered 14 proposed items relevant to include in the preregistration form, but two items had relevance scores just below our predefined criterion (68\%) with mixed argument and were put forth again. We combined items where possible, leading to 11 revised items. In round 2, panelists agreed on including the two remaining items. Panelists also converged on suggested terminology and elaborations, except for two terms for which they provided clear arguments. The result is an agreement-based form for the preregistration of qualitative studies that consists of 13 items. The form will be made available as a registration option on Open Science Framework (osf.io). We believe it is important to assure that the strength of qualitative research, which is its flexibility to adapt, adjust and respond, is not lost in preregistration. The preregistration should provide a systematic starting point.},
  language = {English},
  keywords = {consensus,criteria,guidelines,preregistration,psychology,qualitative research,science,standards,transparency},
  annotation = {WOS:000599964600001}
}

@article{head_Extent_2015,
  title = {The {{Extent}} and {{Consequences}} of {{P}}-{{Hacking}} in {{Science}}},
  author = {Head, Megan L. and Holman, Luke and Lanfear, Rob and Kahn, Andrew T. and Jennions, Michael D.},
  year = {2015},
  month = mar,
  journal = {PLOS Biology},
  volume = {13},
  number = {3},
  pages = {e1002106},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002106},
  abstract = {A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as ``p-hacking,'' occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.},
  language = {en},
  keywords = {Bibliometrics,Binomials,Medicine and health sciences,Metaanalysis,practices,Publication ethics,Reproducibility,Statistical data,Test statistics}
}

@article{head_Extent_2015a,
  title = {The {{Extent}} and {{Consequences}} of {{P}}-{{Hacking}} in {{Science}}},
  author = {Head, Megan L. and Holman, Luke and Lanfear, Rob and Kahn, Andrew T. and Jennions, Michael D.},
  year = {2015},
  month = mar,
  journal = {Plos Biology},
  volume = {13},
  number = {3},
  pages = {e1002106},
  publisher = {{Public Library Science}},
  address = {{San Francisco}},
  issn = {1544-9173},
  doi = {10.1371/journal.pbio.1002106},
  abstract = {A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as "p-hacking," occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.},
  language = {English},
  keywords = {clinical-research,competition,confidence,extra-pair paternity,false discovery rate,metaanalysis,operational sex-ratio,publication bias,replication,wise false},
  annotation = {WOS:000352095700015}
}

@article{hernandez_open_2007,
  title = {{{OPEN ACCESS}}: {{EL PAPEL DE LAS BIBLIOTECAS EN LOS REPOSITORIOS INSTITUCIONALES DE ACCESO ABIERTO}}},
  author = {Hern{\'a}ndez, Tony and Rodr{\'i}guez, David and {Bueno De la Fuente}, Gema},
  year = {2007},
  volume = {10},
  pages = {185--204},
  abstract = {En Espa\~na, como en muchos otros pa\'ises, el n\'umero de repositorios insti-tucionales  ha  ido  creciendo  paulatinamente  en  los  \'ultimos  tres  a\~nos.  En  febrero  de  2007 estos repositorios contienen ya m\'as de 30000 documentos en acceso abierto, es decir, disponibles a texto completo de forma gratuita, y con posibilidad de descarga, impresi\'on o copia sin coste a\~nadido. La pr\'actica totalidad de estos repositorios est\'an siendo  gestionados  por  los  servicios  de  biblioteca  de  las  distintas  instituciones  que  los albergan. Este art\'iculo explica las razones de la crisis del modelo tradicional de comunicaci\'on cient\'ifica, iniciado en la era de lo impreso y que se ha trasladado a la era digital, la alternativa que representa el modelo basado en el acceso abierto, y el importante papel que las bibliotecas pueden jugar, un reto y una oportunidad que no deben perder, en la construcci\'on de colecciones digitales propias}
}

@article{hernandez_por_2016,
  title = {\textquestiondown{{Por}} Qu\'e Open Access?},
  author = {Hern{\'a}ndez, Enrique},
  year = {2016},
  volume = {28},
  number = {1},
  issn = {2448-8771}
}

@article{hitzig_Problem_2020,
  title = {The {{Problem}} of {{New Evidence}}: {{P}}-{{Hacking}} and {{Pre}}-{{Analysis Plans}}},
  shorttitle = {The {{Problem}} of {{New Evidence}}},
  author = {Hitzig, Zoe and Stegenga, Jacob},
  year = {2020},
  month = dec,
  journal = {Diametros},
  volume = {17},
  number = {66},
  pages = {10--33},
  publisher = {{Jagiellonian Univ, Inst Philosophy}},
  address = {{Krakow}},
  issn = {1733-5566},
  doi = {10.33392/diam.1587},
  abstract = {We provide a novel articulation of the epistemic peril of p-hacking using three resources from philosophy: predictivism, Bayesian confirmation theory, and model selection theory. We defend a nuanced position on p-hacking: p-hacking is sometimes, but not always, epistemically pernicious. Our argument requires a novel understanding of Bayesianism, since a standard criticism of Bayesian confirmation theory is that it cannot represent the influence of biased methods. We then turn to pre-analysis plans, a methodological device used to mitigate p-hacking. Some say that pre-analysis plans are epistemically meritorious while others deny this, and in practice pre-analysis plans are often violated. We resolve this debate with a modest defence of pre-analysis plans. Further, we argue that pre-analysis plans can be epistemically relevant even if the plan is not strictly followed-and suggest that allowing for flexible pre-analysis plans may be the best available policy option.},
  language = {English},
  keywords = {Bayesian confirmation theory,credibility,old evidence,p-hacking,pre-analysis plans,prediction,predictivism,registered-reports,replication crisis,science,transparency},
  annotation = {WOS:000600025300002}
}

@article{hollenbeck_Harking_2017,
  title = {Harking, {{Sharking}}, and {{Tharking}}: {{Making}} the {{Case}} for {{Post Hoc Analysis}} of {{Scientific Data}}},
  shorttitle = {Harking, {{Sharking}}, and {{Tharking}}},
  author = {Hollenbeck, John R. and Wright, Patrick M.},
  year = {2017},
  month = jan,
  journal = {Journal of Management},
  volume = {43},
  number = {1},
  pages = {5--18},
  publisher = {{SAGE Publications Inc}},
  issn = {0149-2063},
  doi = {10.1177/0149206316679487},
  abstract = {In this editorial we discuss the problems associated with HARKing (Hypothesizing After Results Are Known) and draw a distinction between Sharking (Secretly HARKing in the Introduction section) and Tharking (Transparently HARKing in the Discussion section). Although there is never any justification for the process of Sharking, we argue that Tharking can promote the effectiveness and efficiency of both scientific inquiry and cumulative knowledge creation. We argue that the discussion sections of all empirical papers should include a subsection that reports post hoc exploratory data analysis. We explain how authors, reviewers, and editors can best leverage post hoc analyses in the spirit of scientific discovery in a way that does not bias parameter estimates and recognizes the lack of definitiveness associated with any single study or any single replication. We also discuss why the failure to Thark in high-stakes contexts where data is scarce and costly may also be unethical.},
  language = {en},
  keywords = {macro topics,micro topics,philosophy of science,practices,research design,research methods,statistical methods}
}

@article{horbach_changing_2018,
  title = {The Changing Forms and Expectations of Peer Review},
  author = {Horbach, S. P. J. M. and Halffman, W.},
  year = {2018},
  month = sep,
  journal = {Research Integrity and Peer Review},
  volume = {3},
  number = {1},
  pages = {8},
  publisher = {{Bmc}},
  address = {{London}},
  doi = {10.1186/s41073-018-0051-5},
  abstract = {The quality and integrity of the scientific literature have recently become the subject of heated debate. Due to an apparent increase in cases of scientific fraud and irreproducible research, some have claimed science to be in a state of crisis. A key concern in this debate has been the extent to which science is capable of self-regulation. Among various mechanisms, the peer review system in particular is considered an essential gatekeeper of both quality and sometimes even integrity in science.However, the allocation of responsibility for integrity to the peer review system is fairly recent and remains controversial. In addition, peer review currently comes in a wide variety of forms, developed in the expectation they can address specific problems and concerns in science publishing. At present, there is a clear need for a systematic analysis of peer review forms and the concerns underpinning them, especially considering a wave of experimentation fuelled by internet technologies and their promise to improve research integrity and reporting.We describe the emergence of current peer review forms by reviewing the scientific literature on peer review and by adding recent developments based on information from editors and publishers. We analyse the rationale for developing new review forms and discuss how they have been implemented in the current system. Finally, we give a systematisation of the range of discussed peer review forms. We pay detailed attention to the emergence of the expectation that peer review can maintain 'the integrity of science's published record', demonstrating that this leads to tensions in the academic debate about the responsibilities and abilities of the peer review system.},
  language = {English},
  keywords = {credibility,exchange,fraud,Innovation,journals,Peer review,plagiarism,publication bias,registered-reports,science,Scientific integrity,scientific misconduct,Scientific misconduct,Scientific publishing},
  annotation = {WOS:000561282300001}
}

@misc{horgan_Psychology_,
  title = {Psychology's {{Credibility Crisis}}: The {{Bad}}, the {{Good}} and the {{Ugly}}},
  shorttitle = {Psychology's {{Credibility Crisis}}},
  author = {Horgan, John},
  journal = {Scientific American},
  doi = {10.1038/scientificamericanmind0716-18},
  abstract = {As more studies are called into question and researchers bicker over methodology, the field is showing a healthy willingness to face its problems\&nbsp;},
  howpublished = {https://www.scientificamerican.com/article/psychology-s-credibility-crisis-the-bad-the-good-and-the-ugly/},
  language = {en},
  keywords = {crisis}
}

@article{ioannidis_Power_2017,
  title = {The {{Power}} of {{Bias}} in {{Economics Research}}},
  author = {Ioannidis, John P. A. and Stanley, T. D. and Doucouliagos, Hristos},
  year = {2017},
  journal = {The Economic Journal},
  volume = {127},
  number = {605},
  pages = {F236-F265},
  issn = {1468-0297},
  doi = {10.1111/ecoj.12461},
  abstract = {We investigate two critical dimensions of the credibility of empirical economics research: statistical power and bias. We survey 159 empirical economics literatures that draw upon 64,076 estimates of economic parameters reported in more than 6,700 empirical studies. Half of the research areas have nearly 90\% of their results under-powered. The median statistical power is 18\%, or less. A simple weighted average of those reported results that are adequately powered (power {$\geq$} 80\%) reveals that nearly 80\% of the reported effects in these empirical economics literatures are exaggerated; typically, by a factor of two and with one-third inflated by a factor of four or more.},
  copyright = {\textcopyright{} 2017 Royal Economic Society},
  language = {en},
  keywords = {bias,credibility,empirical economics,practices,publication bias,statistical power},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ecoj.12461}
}

@article{ioannidis_Why_2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  journal = {PLOS Medicine},
  volume = {2},
  number = {8},
  pages = {e124},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  language = {en},
  keywords = {Cancer risk factors,crisis,Finance,Genetic epidemiology,Genetics of disease,Metaanalysis,practices,Randomized controlled trials,Research design,Schizophrenia}
}

@article{janzen_Ulysses_,
  title = {Ulysses' Pact or {{Ulysses}}' Raft: {{Using}} Pre-Analysis Plans in Experimental and Nonexperimental Research},
  shorttitle = {Ulysses' Pact or {{Ulysses}}' Raft},
  author = {Janzen, Sarah A. and Michler, Jeffrey D.},
  journal = {Applied Economic Perspectives and Policy},
  publisher = {{Wiley}},
  address = {{Hoboken}},
  issn = {2040-5790},
  doi = {10.1002/aepp.13133},
  abstract = {Economists have recently adopted preanalysis plans in response to concerns about robustness and transparency in research. The increased use of registered preanalysis plans has raised competing concerns that detailed plans are costly to create, overly restrictive, and limit the type of inspiration that stems from exploratory analysis. We consider these competing views of preanalysis plans, and make a careful distinction between the roles of preanalysis plans and registries, which provide a record of all planned research. We propose a flexible "packraft" preanalysis plan approach that offers benefits for a wide variety of experimental and nonexperimental applications in applied economics.},
  language = {English},
  keywords = {error,hypothesis registry,preanalysis plan,preregistration,research ethics,transparency},
  annotation = {WOS:000605993700001}
}

@article{jara_Tracing_2018,
  title = {Tracing {{Mapuche Exclusion}} from {{Post}}-{{Dictatorial Truth Commissions}} in {{Chile}}: {{Official}} and {{Grassroots Initiatives}}},
  shorttitle = {Tracing {{Mapuche Exclusion}} from {{Post}}-{{Dictatorial Truth Commissions}} in {{Chile}}},
  author = {Jara, Daniela and Badilla, Manuela and Figueiredo, Ana and Cornejo, Marcela and Riveros, Victoria},
  year = {2018},
  month = nov,
  journal = {International Journal of Transitional Justice},
  volume = {12},
  number = {3},
  pages = {479--498},
  issn = {1752-7716},
  doi = {10.1093/ijtj/ijy025},
  abstract = {This article critically examines the official misrecognition of Mapuche experiences of violence during Augusto Pinochet's dictatorship (1973\textendash 1990) in state-sponsored truth commissions in Chile. We examine official post-dictatorial truth commission politics, narratives and procedures, analyzing how they envisioned the Mapuche as a political (absent) subject and how specific and homogenizing notions of victimhood were produced. We draw attention to three forms of cultural response by the Mapuche toward the official practices of the truth commissions from a bottom-up perspective: indifference, ambivalence and cultural resistance. We then draw attention to unofficial initiatives by nongovernmental organizations (NGOs) and grassroots groups that have aimed to tackle this gap in the transitional justice mechanisms by creating oppositional knowledge. We see in these counter initiatives a valuable knowledge that could allow the creation of bridges between Mapuche communities, mechanisms of transitional justice, grassroots and NGO activism and the Chilean state.}
}

@article{jerolmack_Ethical_2019,
  title = {The {{Ethical Dilemmas}} and {{Social Scientific Trade}}-Offs of {{Masking}} in {{Ethnography}}},
  author = {Jerolmack, Colin and Murphy, Alexandra K.},
  year = {2019},
  month = nov,
  journal = {Sociological Methods \& Research},
  volume = {48},
  number = {4},
  pages = {801--827},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124117701483},
  abstract = {Masking, the practice of hiding or distorting identifying information about people, places, and organizations, is usually considered a requisite feature of ethnographic research and writing. This is justified both as an ethical obligation to one's subjects and as a scientifically neutral position (as readers are enjoined to treat a case's idiosyncrasies as sociologically insignificant). We question both justifications, highlighting potential ethical dilemmas and obstacles to constructing cumulative social science that can arise through masking. Regarding ethics, we show, on the one hand, how masking may give subjects a false sense of security because it implies a promise of confidentiality that it often cannot guarantee and, on the other hand, how naming may sometimes be what subjects want and expect. Regarding scientific tradeoffs, we argue that masking can reify ethnographic authority, exaggerate the universality of the case (e.g., ``Middletown''), and inhibit replicability (or ``revisits'') and sociological comparison. While some degree of masking is ethically and practically warranted in many cases and the value of disclosure varies across ethnographies, we conclude that masking should no longer be the default option that ethnographers unquestioningly choose.},
  language = {en},
  keywords = {disclosure,ethics,ethnography,generalizability,masking,pseudonyms}
}

@article{jl_It_2017,
  title = {It's {{Time}} to {{Broaden}} the {{Replicability Conversation}}: {{Thoughts}} for and {{From Clinical Psychological Science}}},
  shorttitle = {It's {{Time}} to {{Broaden}} the {{Replicability Conversation}}},
  author = {Jl, Tackett and So, Lilienfeld and Cj, Patrick and Sl, Johnson and Rf, Krueger and Jd, Miller and Tf, Oltmanns and Pe, Shrout},
  year = {2017},
  month = sep,
  journal = {Perspectives on psychological science : a journal of the Association for Psychological Science},
  volume = {12},
  number = {5},
  publisher = {{Perspect Psychol Sci}},
  issn = {1745-6924},
  doi = {10.1177/1745691617690042},
  abstract = {Psychology is in the early stages of examining a crisis of replicability stemming from several high-profile failures to replicate studies in experimental psychology. This important conversation has largely been focused on social psychology, with some active participation from cognitive psychology. N \ldots},
  language = {en},
  pmid = {28972844}
}

@article{john_Measuring_2012,
  title = {Measuring the {{Prevalence}} of {{Questionable Research Practices With Incentives}} for {{Truth Telling}}},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = {2012},
  month = may,
  journal = {Psychological Science},
  volume = {23},
  number = {5},
  pages = {524--532},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797611430953},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  language = {en},
  keywords = {disclosure,judgment,methodology,practices,professional standards}
}

@article{johnson_Hunting_2014,
  title = {Hunting for {{Artifacts The Perils}} of {{Dismissing Inconsistent Replication Results}}},
  author = {Johnson, David J. and Cheung, Felix and Donnellan, M. Brent},
  year = {2014},
  journal = {Social Psychology},
  volume = {45},
  number = {4},
  pages = {318--320},
  publisher = {{Hogrefe \& Huber Publishers}},
  address = {{Gottingen}},
  issn = {1864-9335},
  abstract = {We attempted high-powered direct replications of the two experiments in Schnall, Benton, and Harvey (2008) and did not duplicate the original results. We therefore concluded that more research was needed to establish the size and robustness of the original effects and to evaluate potential moderators. Schnall (2014) suggests that our conclusions were invalid because of potential psychometric artifacts in our data. We present evidence that undermines concerns about artifacts and defend the utility of preregistered replication studies for advancing research in psychological science.},
  language = {English},
  keywords = {cleanliness,moral judgment,preregistration,replication},
  annotation = {WOS:000341226100015}
}

@article{kapiszewski_Openness_2019,
  title = {Openness in {{Practice}} in {{Qualitative Research}}},
  author = {Kapiszewski, Diana and Karcher, Sebastian},
  year = {2019},
  month = jun,
  doi = {10.33774/apsa-2019-if2he},
  abstract = {The discipline of political science has been engaged in discussion about when, why, and how to make scholarship more open for at least three decades.This piece argues that the best way to resolve our differences and develop appropriate norms and guidelines for making different types of qualitative research more open is to move from ``if'' to ``how'' \textendash{} for individual political scientists to make their work more open \textendash{} generating examples from which we can learn and on which we can build. We begin by articulating a series of principles that underlie our views on openness. We then consider the ``state of the debate,'' briefly outlining the contours of the scholarship on openness in political and other social sciences, highlighting the fractured nature of the discussion. The heart of the piece considers various strategies, illustrated by exemplary applications, for making qualitative research more open.},
  language = {en}
}

@article{kapiszewski_Transparency_2021,
  title = {Transparency in {{Practice}} in {{Qualitative Research}}},
  author = {Kapiszewski, Diana and Karcher, Sebastian},
  year = {2021},
  month = apr,
  journal = {PS: Political Science \& Politics},
  volume = {54},
  number = {2},
  pages = {285--291},
  publisher = {{Cambridge University Press}},
  issn = {1049-0965, 1537-5935},
  doi = {10.1017/S1049096520000955},
  abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS1049096520000955/resource/name/firstPage-S1049096520000955a.jpg},
  language = {en},
  keywords = {transparency}
}

@article{kathawalla_Easing_2021,
  title = {Easing {{Into Open Science}}: {{A Guide}} for {{Graduate Students}} and {{Their Advisors}}},
  shorttitle = {Easing {{Into Open Science}}},
  author = {Kathawalla, Ummul-Kiram and Silverstein, Priya and Syed, Moin},
  year = {2021},
  month = jan,
  journal = {Collabra-Psychology},
  volume = {7},
  number = {1},
  publisher = {{Univ California Press}},
  address = {{Oakland}},
  issn = {2474-7394},
  doi = {10.1525/collabra.18684},
  abstract = {This article provides a roadmap to assist graduate students and their advisors to engage in open science practices. We suggest eight open science practices that novice graduate students could begin adopting today. The topics we cover include journal clubs, project workflow, preprints, reproducible code, data sharing, transparent writing, preregistration, and registered reports. To address concerns about not knowing how to engage in open science practices, we provide a difficulty rating of each behavior (easy, medium, difficult), present them in order of suggested adoption, and follow the format of what, why, how, and worries. We give graduate students ideas on how to approach conversations with their advisors/collaborators, ideas on how to integrate open science practices within the graduate school framework, and specific resources on how to engage with each behavior. We emphasize that engaging in open science behaviors need not be an all or nothing approach, but rather graduate students can engage with any number of the behaviors outlined.},
  language = {English},
  keywords = {advising,apa publications,article reporting standards,graduate students,open science,preregistration,psychology,reproducibility,tutorial},
  annotation = {WOS:000655169700001}
}

@article{kerr_HARKing_1998,
  title = {{{HARKing}}: {{Hypothesizing After}} the {{Results}} Are {{Known}}},
  shorttitle = {{{HARKing}}},
  author = {Kerr, Norbert L.},
  year = {1998},
  month = aug,
  journal = {Personality and Social Psychology Review},
  volume = {2},
  number = {3},
  pages = {196--217},
  publisher = {{SAGE Publications Inc}},
  issn = {1088-8683},
  doi = {10.1207/s15327957pspr0203_4},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing's costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  language = {en}
}

@article{khan_pHacking_2019,
  title = {P-{{Hacking}} in {{Experimental Audit Research}}},
  author = {Khan, Mohammad Jahanzeb and Tonnes, Per Christen},
  year = {SPR 2019},
  journal = {Behavioral Research in Accounting},
  volume = {31},
  number = {1},
  pages = {119--131},
  publisher = {{Amer Accounting Assoc}},
  address = {{Sarasota}},
  issn = {1050-4753},
  doi = {10.2308/bria-52183},
  abstract = {A focus on novel, confirmatory, and statistically significant results by journals that publish experimental audit research may result in substantial bias in the literature. We explore one type of bias known as p-hacking: a practice where researchers, whether knowingly or unknowingly, adjust their collection, analysis, and reporting of data and results, until nonsignificant results become significant. Examining experimental audit literature published in eight accounting and audit journals within the last three decades, we find an overabundance of p-values at or just below the conventional thresholds for statistical significance. The finding of too many "just significant" results is an indication that some of the results published in the experimental audit literature are potentially a consequence of p-hacking. We discuss potential remedies that, if adopted, may to some extent alleviate concerns regarding p-hacking and the publication of false positive results.},
  language = {English},
  keywords = {audit,audit experiment,curve,design,file-drawer problem,p-hacking,p-values,publication bias,statistics,tests},
  annotation = {WOS:000478766300009}
}

@article{kiese-himmel_Pitfalls_2020,
  title = {{Pitfalls in the statistical world}},
  author = {{Kiese-Himmel}, C. and Plontke, S. K.},
  year = {2020},
  month = jan,
  journal = {Hno},
  volume = {68},
  number = {1},
  pages = {3--7},
  publisher = {{Springer}},
  address = {{New York}},
  issn = {0017-6192},
  doi = {10.1007/s00106-019-00750-x},
  abstract = {Mistakes in the acquisition, evaluation, analysis and reporting of data often occur because there is a lack of awareness of the methodological problem. They can have far-reaching consequences for the reception of results of scientific primary studies and therefore for the clinical practice and healthcare. This article aims at raising awareness for a responsible handling of study data and for avoiding questionable or incorrect practices. It presents some examples of statistical pitfalls in empirical research practice, which increase the probability of false positive results and shows possibilities to avoid such risks.},
  language = {German},
  keywords = {audiometry,clinical-trials,false discovery rate,False positive findings,HARKing,hearing-loss,missing data,p-hacking,parameters,Preregistration,Replicability},
  annotation = {WOS:000512057900001}
}

@article{klein_Practical_2018,
  title = {A {{Practical Guide}} for {{Transparency}} in {{Psychological Science}}},
  author = {Klein, Olivier and Hardwicke, Tom E. and Aust, Frederik and Breuer, Johannes and Danielsson, Henrik and Mohr, Alicia Hofelich and IJzerman, Hans and Nilsonne, Gustav and Vanpaemel, Wolf and Frank, Michael C.},
  editor = {Nuijten, Mich{\'e}le and Vazire, Simine},
  year = {2018},
  month = jun,
  journal = {Collabra: Psychology},
  volume = {4},
  number = {1},
  issn = {2474-7394},
  doi = {10.1525/collabra.158},
  abstract = {The credibility of scientific claims depends upon the transparency of the research products upon which they are based (e.g., study protocols, data, materials, and analysis scripts). As psychology navigates a period of unprecedented introspection, user-friendly tools and services that support open science have flourished. However, the plethora of decisions and choices involved can be bewildering. Here we provide a practical guide to help researchers navigate the process of preparing and sharing the products of their research (e.g., choosing a repository, preparing their research products for sharing, structuring folders, etc.). Being an open scientist means adopting a few straightforward research management practices, which lead to less error prone, reproducible research workflows. Further, this adoption can be piecemeal \textendash{} each incremental step towards complete transparency adds positive value. Transparent research practices not only improve the efficiency of individual researchers, they enhance the credibility of the knowledge generated by the scientific community.},
  keywords = {herramienta}
}

@techreport{lareferencia.consejodirectivo_Comunicacion_2019,
  title = {{Comunicaci\'on Acad\'emica y Acceso Abierto: Acciones para un Pol\'itica P\'ublica en Am\'erica Latina}},
  shorttitle = {{Comunicaci\'on Acad\'emica y Acceso Abierto}},
  author = {LA Referencia. Consejo Directivo},
  year = {2019},
  month = may,
  institution = {{Zenodo}},
  doi = {10.5281/ZENODO.3229410},
  abstract = {Documento redactado como insumo  para las autoridades regionales que asistieron a la reuni\'on anual del Global Research Council con acuerdo del Consejo Directivo.   La publicaci\'on y difusi\'on del mismo se realiza con el fin de favorecer el di\'alogo y la construcci\'on de una visi\'on conjunta sobre la cual se debe profundizar y actualizar a la luz de los desaf\'ios del Acceso Abierto en la regi\'on en el corto y mediano plazo. La comunicaci\'on cient\'ifica y el cambio del modelo; la situaci\'on de Am\'erica Latina; el sistema de comunicaci\'on acad\'emica de la regi\'on, principios y acciones y recomendaciones para repositorios, consorcios y revistas son los ejes tem\'aticos que se abordan a lo largo de sus p\'aginas. El art\'iculo  refuerza la premisa de que se deben tomar acciones decididas para que los resultados financiados total o parcialmente con fondos p\'ublicos est\'en en Acceso Abierto y reafirma el rol central de los organismos de CyT para lograrlo. Basado en la realidad regional, propone principios generales y acciones para los repositorios de Acceso Abierto, consorcios y revistas con una mirada m\'as sist\'emica desde las pol\'iticas p\'ublicas. Concluye con la necesidad de un di\'alogo con iniciativas como el ``Plan S'' se\~nalando los puntos de acuerdo, as\'i como diferencias, dado el contexto regional, en temas como el APC o una valorizaci\'on del rol de los repositorios. Presentado en la reuni\'on  de   COAR. 2019.  Technical and Strategic Meeting of Repository Networks. Mayo 21, 2019 - Lyon, France. Alberto Cabezas Bullemore, Secretario Ejecutivo,   LA Referencia.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  language = {es},
  keywords = {Acceso Abierto,Ciencia Abierta,Financiadores,ONCYTs,Plan S,Repositorios,revisado}
}

@article{lewandowsky_Research_2016,
  title = {Research Integrity: {{Don}}'t Let Transparency Damage Science},
  shorttitle = {Research Integrity},
  author = {Lewandowsky, Stephan and Bishop, Dorothy},
  year = {2016},
  month = jan,
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {459--461},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/529459a},
  abstract = {Stephan Lewandowsky and Dorothy Bishop explain how the research community should protect its members from harassment, while encouraging the openness that has become essential to science.},
  copyright = {2016 Nature Publishing Group},
  language = {en},
  keywords = {transparency}
}

@article{leys_How_2019,
  title = {How to {{Classify}}, {{Detect}}, and {{Manage Univariate}} and {{Multivariate Outliers}}, {{With Emphasis}} on {{Pre}}-{{Registration}}},
  author = {Leys, Christophe and Delacre, Marie and Mora, Youri L. and Lakens, Daniel and Ley, Christophe},
  year = {2019},
  month = apr,
  journal = {International Review of Social Psychology},
  volume = {32},
  number = {1},
  pages = {5},
  publisher = {{Ubiquity Press Ltd}},
  address = {{London}},
  issn = {2397-8570},
  doi = {10.5334/irsp.289},
  abstract = {Researchers often lack knowledge about how to deal with outliers when analyzing their data. Even more frequently, researchers do not pre-specify how they plan to manage outliers. In this paper we aim to improve research practices by outlining what you need to know about outliers. We start by providing a functional definition of outliers. We then lay down an appropriate nomenclature/classification of outliers. This nomenclature is used to understand what kinds of outliers can be encountered and serves as a guideline to make appropriate decisions regarding the conservation, deletion, or recoding of outliers. These decisions might impact the validity of statistical inferences as well as the reproducibility of our experiments. To be able to make informed decisions about outliers you first need proper detection tools. We remind readers why the most common outlier detection methods are problematic and recommend the use of the median absolute deviation to detect univariate outliers, and of the Mahalanobis-MCD distance to detect multivariate outliers. An R package was created that can be used to easily perform these detection tests. Finally, we promote the use of pre-registration to avoid flexibility in data analysis when handling outliers.},
  language = {English},
  keywords = {Malahanobis distance,median absolute deviation,minimum covariance determinant,outliers,preregistration,psychology,robust detection,transparency},
  annotation = {WOS:000468350500001}
}

@article{lilienfeld_Psychology_2017,
  title = {Psychology's {{Replication Crisis}} and the {{Grant Culture}}: {{Righting}} the {{Ship}}},
  shorttitle = {Psychology's {{Replication Crisis}} and the {{Grant Culture}}},
  author = {Lilienfeld, Scott O.},
  year = {2017},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {12},
  number = {4},
  pages = {660--664},
  publisher = {{Sage Publications Ltd}},
  address = {{London}},
  issn = {1745-6916},
  doi = {10.1177/1745691616687745},
  abstract = {The past several years have been a time for soul searching in psychology, as we have gradually come to grips with the reality that some of our cherished findings are less robust than we had assumed. Nevertheless, the replication crisis highlights the operation of psychological science at its best, as it reflects our growing humility. At the same time, institutional variables, especially the growing emphasis on external funding as an expectation or de facto requirement for faculty tenure and promotion, pose largely unappreciated hazards for psychological science, including (a) incentives for engaging in questionable research practices, (b) a single-minded focus on programmatic research, (c) intellectual hyperspecialization, (d) disincentives for conducting direct replications, (e) stifling of creativity and intellectual risk taking, (f) researchers promising more than they can deliver, and (g) diminished time for thinking deeply. Preregistration should assist with (a), but will do little about (b) through (g). Psychology is beginning to right the ship, but it will need to confront the increasingly deleterious impact of the grant culture on scientific inquiry.},
  language = {English},
  keywords = {confirmation bias,grants,preregistration,replication},
  annotation = {WOS:000406027000008}
}

@article{linder_Unfolding_2020,
  title = {Unfolding the {{Black Box}} of {{Questionable Research Practices}}: {{Where Is}} the {{Line Between Acceptable}} and {{Unacceptable Practices}}?},
  shorttitle = {Unfolding the {{Black Box}} of {{Questionable Research Practices}}},
  author = {Linder, Christian and Farahbakhsh, Siavash},
  year = {2020},
  month = jul,
  journal = {Business Ethics Quarterly},
  volume = {30},
  number = {3},
  pages = {335--360},
  publisher = {{Cambridge Univ Press}},
  address = {{New York}},
  issn = {1052-150X},
  doi = {10.1017/beq.2019.52},
  abstract = {Despite the extensive literature on what questionable research practices (QRPs) are and how to measure them, the normative underpinnings of such practices have remained less explored. QRPs often fall into a grey area of justifiable and unjustifiable practices. Where to precisely draw the line between such practices challenges individual scholars and this harms science. We investigate QRPs from a normative perspective using the theory of communicative action. We highlight the role of the collective in assessing individual behaviours. Our contribution is a framework that allows identification of when particular actions cross over from acceptable to unacceptable practice. Thus, this article provides grounds for developing scientific standards to raise the quality of scientific research.},
  language = {English},
  keywords = {academic standards,academic-freedom,business,deontology,discourse ethics,ethics,file-drawer problem,management,metaanalysis,norms,p-hacking,publication bias,questionable research practice,science,tests},
  annotation = {WOS:000549379800003}
}

@article{lindsay_Seven_2020,
  title = {Seven Steps toward Transparency and Replicability in Psychological Science.},
  author = {Lindsay, D. Stephen},
  year = {2020},
  month = nov,
  journal = {Canadian Psychology/Psychologie canadienne},
  volume = {61},
  number = {4},
  pages = {310--317},
  issn = {1878-7304, 0708-5591},
  doi = {10.1037/cap0000222},
  language = {en},
  keywords = {forrt,transparency}
}

@article{linton_Publish_2011,
  title = {Publish or {{Perish}}: {{How Are Research}} and {{Reputation Related}}?},
  shorttitle = {Publish or {{Perish}}},
  author = {Linton, Jonathan D. and Tierney, Robert and Walsh, Steven T.},
  year = {2011},
  month = dec,
  journal = {Serials Review},
  volume = {37},
  number = {4},
  pages = {244--257},
  publisher = {{Routledge}},
  issn = {0098-7913},
  doi = {10.1080/00987913.2011.10765398},
  abstract = {A study of twenty-seven fields in 350 highly ranked universities examines the relationship between reputation and rank. We find that many metrics associated with research prowess significantly correlate to university reputation. However, the next logical step\textendash{} looking at the relationship that links different academic fields with the reputation of the university\textendash did not always offer the expected results. The phrase ``publish or perish'' clearly has very different meanings in different fields.},
  keywords = {Academic reputation,institutional,Interdisciplinary studies,Publish or perish,University research reputation},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00987913.2011.10765398}
}

@misc{loredo_derecho_2012,
  title = {Derecho {{Comparado}}: {{Derecho}} de {{Autor}} y {{Copyright}}. {{Dos}} Caminos Que Se Encuentran},
  author = {Loredo, Alejandro},
  year = {2012},
  journal = {Portal de gobierno electr\'onico, inclusi\'on digital y sociedad del conocimiento}
}

@article{loredo_mexico_2006,
  title = {M\'exico: {{Derecho Comparado}}: {{Derecho}} de {{Autor}} y {{Copyright}}. {{Dos}} Caminos Que Se Encuentran},
  shorttitle = {M\'exico},
  author = {Loredo, Alejandro},
  year = {2006},
  journal = {AR: Revista de Derecho Inform\'atico},
  number = {91},
  pages = {2},
  publisher = {{Alfa-Redi}}
}

@article{lucas_Introduction_2021,
  title = {Introduction to the {{Special Issue}}: {{Preregistered Studies}} of {{Personality Development}} and {{Aging Using Existing Data}}},
  shorttitle = {Introduction to the {{Special Issue}}},
  author = {Lucas, Richard E. and Donnellan, M. Brent},
  year = {2021},
  month = jan,
  journal = {Journals of Gerontology Series B-Psychological Sciences and Social Sciences},
  volume = {76},
  number = {1},
  pages = {1--5},
  publisher = {{Oxford Univ Press Inc}},
  address = {{Cary}},
  issn = {1079-5014},
  doi = {10.1093/geronb/gbaa192},
  language = {English},
  keywords = {registered-reports},
  annotation = {WOS:000649390900001}
}

@techreport{luke_Epistemological_2019,
  type = {{{SSRN Scholarly Paper}}},
  title = {Epistemological and {{Ontological Priors}}: {{Explicating}} the {{Perils}} of {{Transparency}}},
  shorttitle = {Epistemological and {{Ontological Priors}}},
  author = {Luke, Timothy W. and {V{\'a}zquez-Arroyo}, Antonio and Hawkesworth, Mary},
  year = {2019},
  month = feb,
  number = {ID 3332878},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  doi = {10.2139/ssrn.3332878},
  abstract = {The discipline of political science encompasses multiple research communities, which have grown out of and rely upon different epistemological and ontological presuppositions.  Recent debates about transparency raise important questions about which of these research communities will be accredited within the discipline, whose values, norms, and methods of knowledge production will gain ascendency and whose will be marginalized.  Although the language of "transparency" makes it appear that these debates are apolitical, simply elaborating standards that all political scientists share, the intensity and content of recent contestations about DA-RT, JETS, and QTD attest to the profoundly political nature of these methodological discussions. This report traces the epistemological and ontological assumptions that have shaped diverse research communities within the discipline, situating "transparency" in relation to classical (Aristotelian), modern (Baconian) and twentieth-century (positivist, critical rationalist, and postpositivist) versions of empiricism.  It shows how recent discussions of transparency accredit certain empirical approaches by collapsing the scope of empirical investigation and the parameters of the knowable.  And it argues that "transparency" is inappropriate as a regulative ideal for political science because it misconstrues the roles of theory, social values, and critique in scholarly investigation.},
  language = {en},
  keywords = {epistemology,ontology,philosophy of science,qualitative methods,Qualitative Transparency Deliberations,research transparency}
}

@article{makel_Both_2021,
  title = {Both {{Questionable}} and {{Open Research Practices Are Prevalent}} in {{Education Research}}},
  author = {Makel, Matthew C. and Hodges, Jaret and Cook, Bryan G. and Plucker, Jonathan A.},
  year = {2021},
  month = mar,
  journal = {Educational Researcher},
  pages = {0013189X211001356},
  publisher = {{American Educational Research Association}},
  issn = {0013-189X},
  doi = {10.3102/0013189X211001356},
  abstract = {Concerns about the conduct of research are pervasive in many fields, including education. In this preregistered study, we replicated and extended previous studies from other fields by asking education researchers about 10 questionable research practices and five open research practices. We asked them to estimate the prevalence of the practices in the field, to self-report their own use of such practices, and to estimate the appropriateness of these behaviors in education research. We made predictions under four umbrella categories: comparison to psychology, geographic location, career stage, and quantitative orientation. Broadly, our results suggest that both questionable and open research practices are used by many education researchers. This baseline information will be useful as education researchers seek to understand existing social norms and grapple with whether and how to improve research practices.},
  language = {en},
  keywords = {ethics,globalization,open science,psychology,questionable research practices,replication,research methodology,survey research}
}

@article{mardones_usos_2018,
  title = {{Usos del dise\~no metodol\'ogico cualitativo en art\'iculos de acceso abierto de alto impacto en ciencias sociales}},
  shorttitle = {{Dise\~nos metodol\'ogicos en art\'iculos de acceso abierto}},
  author = {Mardones, Rodolfo and Ulloa, Jorge and Salas, Gonz{\'a}lo},
  year = {2018},
  journal = {Forum: Qualitative Social Research},
  volume = {19, n\textdegree 1},
  pages = {1--18},
  issn = {1438-5627},
  abstract = {Las definiciones de dise\~no metodol\'ogico en la perspectiva cualitativa son variadas y sus diversos usos dejan entrever una multiplicidad de perspectivas. El presente trabajo tiene por objeto describir los usos del dise\~no metodol\'ogico cualitativo en art\'iculos de ciencias sociales de alto impacto. Se realiza una revisi\'on de 186 art\'iculos de resultados de investigaci\'on y propuestas metodol\'ogicas, publicados en revistas open access indexadas en Scopus en el periodo 2013-2015. Los resultados muestran que el 75\% de los art\'iculos declara su dise\~no metodol\'ogico. El uso de este se clasific\'o en tres categor\'ias: organizaci\'on de la investigaci\'on, elecci\'on paradigm\'atica y tipo de estudio. El 51\% de los art\'iculos utilizan dise\~no cualitativo basado en el tipo de estudio. Mientras tanto, la organizaci\'on de la investigaci\'on (37\%) y la elecci\'on paradigm\'atica (11\%) quedan en segundo lugar. Se concluye que los usos del dise\~no metodol\'ogico cualitativo se basa en supuestos te\'oricos y pr\'acticas propias de la categor\'ia definida como tipo de estudio. Esto puede facilitar la elecci\'on de un dise\~no cualitativo, sensible y flexible al contexto estudiado.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  language = {es}
}

@article{marsden_Introducing_2018,
  title = {Introducing {{Registered Reports}} at {{Language Learning}}: {{Promoting Transparency}}, {{Replication}}, and a {{Synthetic Ethic}} in the {{Language Sciences}}},
  shorttitle = {Introducing {{Registered Reports}} at {{Language Learning}}},
  author = {Marsden, Emma and {Morgan-Short}, Kara and Trofimovich, Pavel and Ellis, Nick C.},
  year = {2018},
  month = jun,
  journal = {Language Learning},
  volume = {68},
  number = {2},
  pages = {309--320},
  publisher = {{Wiley}},
  address = {{Hoboken}},
  issn = {0023-8333},
  doi = {10.1111/lang.12284},
  abstract = {The past few years have seen growing interest in open science practices, which include initiatives to increase transparency in research methods, data collection, and analysis; enhance accessibility to data and materials; and improve the dissemination of findings to broader audiences. Language Learning is enhancing its participation in the open science movement by launching Registered Reports as an article category as of January 1, 2018. Registered Reports allow authors to submit the conceptual justifications and the full method and analysis protocol of their study to peer review prior to data collection. High-quality submissions then receive provisional, in-principle acceptance. Provided that data collection, analyses, and reporting follow the proposed and accepted methodology and analysis protocols, the article is subsequently publishable whatever the findings. We outline key concerns leading to the development of Registered Reports, describe its core features, and discuss some of its benefits and weaknesses.},
  language = {English},
  keywords = {open science,peer review,preregistration,publication bias,registered report,replication,transparency},
  annotation = {WOS:000434147700001}
}

@article{martinson_Scientists_2005,
  title = {Scientists Behaving Badly},
  author = {Martinson, Brian C. and Anderson, Melissa S. and {de Vries}, Raymond},
  year = {2005},
  month = jun,
  journal = {Nature},
  volume = {435},
  number = {7043},
  pages = {737--738},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/435737a},
  abstract = {To protect the integrity of science, we must look beyond falsification, fabrication and plagiarism, to a wider range of questionable research practices, argue Brian C. Martinson, Melissa S. Anderson and Raymond de Vries.},
  copyright = {2005 Nature Publishing Group},
  language = {en},
  keywords = {practices},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Comments \& Opinion}
}

@article{martiny_Degrees_2017,
  title = {Degrees of {{Openness}}, {{Embodiment}}, {{Circularity}}, and {{Invariance Response}}},
  author = {Martiny, Kristian Moltke},
  year = {2017},
  month = nov,
  journal = {Constructivist Foundations},
  volume = {13},
  number = {1},
  pages = {83--90},
  publisher = {{Alexander Riegler}},
  address = {{Brussels}},
  issn = {1782-348X},
  language = {English},
  keywords = {consciousness,dynamics,enaction,experience,incentives,neurophenomenology,open science,phenomenology,registered-reports,synchrony},
  annotation = {WOS:000415144800023}
}

@article{matthes_Questionable_2015,
  title = {Questionable {{Research Practices}} in {{Experimental Communication Research}}: {{A Systematic Analysis From}} 1980 to 2013},
  shorttitle = {Questionable {{Research Practices}} in {{Experimental Communication Research}}},
  author = {Matthes, J{\"o}rg and Marquart, Franziska and Naderer, Brigitte and Arendt, Florian and Schmuck, Desir{\'e}e and Adam, Karoline},
  year = {2015},
  month = oct,
  journal = {Communication Methods and Measures},
  volume = {9},
  number = {4},
  pages = {193--207},
  publisher = {{Routledge}},
  issn = {1931-2458},
  doi = {10.1080/19312458.2015.1096334},
  abstract = {Questionable research practices (QRPs) pose a major threat to any scientific discipline. This article analyzes QRPs with a content analysis of more than three decades of published experimental research in four flagship communication journals: Journal of Communication, Communication Research, Journalism \& Mass Communication Quarterly, and Media Psychology. Findings reveal indications of small and insufficiently justified sample sizes, a lack of reported effect sizes, an indiscriminate removal of cases and items, an increasing inflation of p-values directly below p {$<$} .05, and a rising share of verified (as opposed to falsified) hypotheses. Implications for authors, reviewers, and editors are discussed.},
  annotation = {\_eprint: https://doi.org/10.1080/19312458.2015.1096334}
}

@article{mazor_novel_2019,
  title = {A Novel Tool for Time-Locking Study Plans to Results},
  author = {Mazor, Matan and Mazor, Noam and Mukamel, Roy},
  year = {2019},
  month = may,
  journal = {European Journal of Neuroscience},
  volume = {49},
  number = {9},
  pages = {1149--1156},
  publisher = {{Wiley}},
  address = {{Hoboken}},
  issn = {0953-816X},
  doi = {10.1111/ejn.14278},
  abstract = {Often researchers wish to mark an objective line between study plans that were specified before data acquisition and decisions that were made following data exploration. Contrary to common perception, registering study plans to an online platform prior to data collection does not by itself provide such an objective distinction, even when the registration is time-stamped. Here, we adapt a method from the field of cryptography to allow encoding of study plans and predictions within random aspects of the data acquisition process. Doing so introduces a causal link between the preregistered content and objective attributes of the acquired data, such as the timing and location of brain activations. This guarantees that the preregistered plans and predictions are indeed specified prior to data collection. Our time-locking system does not depend on any external party and can be performed entirely in-lab. We provide code for easy implementation and a detailed example from the field of functional Magnetic Resonance Imaging (fMRI).},
  language = {English},
  keywords = {neuroimaging,preregistration,replicability,science},
  annotation = {WOS:000473619700008}
}

@article{mcgrail_Publish_2006,
  title = {Publish or Perish: A Systematic Review of Interventions to Increase Academic Publication Rates},
  shorttitle = {Publish or Perish},
  author = {McGrail, Matthew R. and Rickard, Claire M. and Jones, Rebecca},
  year = {2006},
  month = feb,
  journal = {Higher Education Research \& Development},
  volume = {25},
  number = {1},
  pages = {19--35},
  publisher = {{Routledge}},
  issn = {0729-4360},
  doi = {10.1080/07294360500453053},
  abstract = {Academics are expected to publish. In Australia universities receive extra funding based on their academic publication rates and academic promotion is difficult without a good publication record. However, the reality is that only a small percentage of academics are actively publishing. To fix this problem, a number of international universities and other higher education institutions have implemented interventions with the main aim being to increase the number of publications. A comprehensive literature search identified 17 studies published between 1984 and 2004, which examined the effects of these interventions. Three key types of interventions were identified: writing courses, writing support groups and writing coaches. The resulting publication output varied, but all interventions led to an increase in average publication rates for the participants.},
  keywords = {institutional},
  annotation = {\_eprint: https://doi.org/10.1080/07294360500453053}
}

@article{mcintosh_three_2020,
  title = {The Three {{R}}'s of Scientific Integrity: {{Replicability}}, Reproducibility, and Robustness},
  shorttitle = {The Three {{R}}'s of Scientific Integrity},
  author = {McIntosh, Robert D. and Chambers, Christopher D.},
  year = {2020},
  month = aug,
  journal = {Cortex},
  volume = {129},
  pages = {A4-A7},
  publisher = {{Elsevier Masson, Corp Off}},
  address = {{Paris}},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2020.04.019},
  language = {English},
  keywords = {fear,reconsolidation,registered-reports,retrieval},
  annotation = {WOS:000552946000003}
}

@article{mckiernan_How_2016,
  title = {How Open Science Helps Researchers Succeed},
  author = {McKiernan, Erin C and Bourne, Philip E and Brown, C Titus and Buck, Stuart and Kenall, Amye and Lin, Jennifer and McDougall, Damon and Nosek, Brian A and Ram, Karthik and Soderberg, Courtney K and Spies, Jeffrey R and Thaney, Kaitlin and Updegrove, Andrew and Woo, Kara H and Yarkoni, Tal},
  editor = {Rodgers, Peter},
  year = {2016},
  month = jul,
  journal = {eLife},
  volume = {5},
  pages = {e16800},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.16800},
  abstract = {Open access, open data, open source and other open scholarship practices are growing in popularity and necessity. However, widespread adoption of these practices has not yet been achieved. One reason is that researchers are uncertain about how sharing their work will affect their careers. We review literature demonstrating that open research is associated with increases in citations, media attention, potential collaborators, job opportunities and funding opportunities. These findings are evidence that open research practices bring significant benefits to researchers relative to more traditional closed practices.},
  keywords = {open access,open data,open science,open source,research}
}

@article{mcvay_Transparency_2021,
  title = {Transparency and Openness in Behavioral Medicine Research},
  author = {McVay, Megan A and Conroy, David E},
  year = {2021},
  month = jan,
  journal = {Translational Behavioral Medicine},
  volume = {11},
  number = {1},
  pages = {287--290},
  issn = {1869-6716},
  doi = {10.1093/tbm/ibz154},
  abstract = {Behavioral medicine aims to improve the health of individuals and communities by addressing behavioral, psychosocial, and environmental contributors to health. Succeeding in this endeavor requires rigorous research and effective communication of this research to relevant stakeholders and the public at large [1]. Both research rigor and effective communication of research may benefit from adopting transparent and open research practices [2\textendash 4], sometimes called ``open science.'' Such practices include preregistering designs, hypotheses, and data analysis plans; making publically available study materials, data, and analytic code; sharing preprints (works-in-progress) of articles; and publishing open access [2]. In this commentary, we describe the evolving pressures to increase the transparency and openness of research, examine the status of open science practices in behavioral medicine, and recommend a path forward to find the right fit for these practices in behavioral medicine research.},
  keywords = {transparency}
}

@article{melero_revistas_2008,
  title = {Revistas {{Open Access}}: Caracter\'isticas, Modelos Econ\'omicos y Tendencias},
  author = {Melero, Remedios and Abad, Mar{\'i}a Francisca},
  year = {2008},
  volume = {20},
  issn = {1575-5886}
}

@article{mertens_Preregistration_2019,
  title = {Preregistration of {{Analyses}} of {{Preexisting Data}}},
  author = {Mertens, Ga{\"e}tan and Krypotos, Angelos-Miltiadis},
  year = {2019},
  journal = {Psychologica Belgica},
  volume = {59},
  number = {1},
  pages = {338--352},
  issn = {0033-2879},
  doi = {10.5334/pb.493},
  abstract = {The preregistration of a study's hypotheses, methods, and data-analyses steps is becoming a popular psychological research practice. To date, most of the discussion on study preregistration has focused on the preregistration of studies that include the collection of original data. However, much of the research in psychology relies on the (re-)analysis of preexisting data. Importantly, this type of studies is different from original studies as researchers cannot change major aspects of the study (e.g., experimental manipulations, sample size). Here, we provide arguments as to why it is useful to preregister analyses of preexisting data, discuss practical considerations, consider potential concerns, and introduce a preregistration template tailored for studies focused on the analyses of preexisting data. We argue that the preregistration of hypotheses and data-analyses for analyses of preexisting data is an important step towards more transparent psychological research.},
  pmcid = {PMC6706998},
  pmid = {31497308}
}

@book{merton_sociology_1973,
  title = {The {{Sociology}} of {{Science}}: {{Theorerical}} and {{Empirical Investigations}}},
  author = {Merton, Robert K.},
  year = {1973},
  publisher = {{The University of Chicago Press}},
  address = {{Chicago and London}}
}

@article{miguel_Promoting_2014,
  title = {Promoting {{Transparency}} in {{Social Science Research}}},
  author = {Miguel, E. and Camerer, C. and Casey, K. and Cohen, J. and Esterling, K. M. and Gerber, A. and Glennerster, R. and Green, D. P. and Humphreys, M. and Imbens, G. and Laitin, D. and Madon, T. and Nelson, L. and Nosek, B. A. and Petersen, M. and Sedlmayr, R. and Simmons, J. P. and Simonsohn, U. and der Laan, M. Van},
  year = {2014},
  month = jan,
  journal = {Science},
  volume = {343},
  number = {6166},
  pages = {30--31},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1245317},
  abstract = {There is growing appreciation for the advantages of experimentation in the social sciences. Policy-relevant claims that in the past were backed by theoretical arguments and inconclusive correlations are now being investigated using more credible methods. Changes have been particularly pronounced in development economics, where hundreds of randomized trials have been carried out over the last decade. When experimentation is difficult or impossible, researchers are using quasi-experimental designs. Governments and advocacy groups display a growing appetite for evidence-based policy-making. In 2005, Mexico established an independent government agency to rigorously evaluate social programs, and in 2012, the U.S. Office of Management and Budget advised federal agencies to present evidence from randomized program evaluations in budget requests (1, 2). Social scientists should adopt higher transparency standards to improve the quality and credibility of research. Social scientists should adopt higher transparency standards to improve the quality and credibility of research.},
  chapter = {Policy Forum},
  copyright = {Copyright \textcopyright{} 2014, American Association for the Advancement of Science},
  language = {en},
  pmid = {24385620},
  keywords = {transparency}
}

@article{monti_acceso_2020,
  title = {{Acceso a la literatura cient\'ifica desde Sci-Hub: An\'alisis y reflexi\'on de las descargas en Argentina}},
  author = {Monti, Carolina and Unzurrunzaga, Carolina},
  year = {2020},
  journal = {Revista Hipertextos},
  volume = {8 (14)},
  pages = {111--116},
  doi = {10.24215/23143924e022},
  abstract = {La comercializaci\'on de la informaci\'on cient\'ifica y las barreras de pago para su acceso se han convertido en una de las problem\'aticas m\'as debatidas en las \'ultimas d\'ecadas. El movimiento internacional de acceso abierto ha propuesto v\'ias para favorecer su acceso y, a su vez, distintos actores han impulsado otras estrategias como sitios piratas para la descarga. En este art\'iculo estudiamos el uso que se hace de Sci-Hub en Argentina a partir de un an\'alisis cuantitativo de las estad\'isticas de descargas disponibles hasta 2017. Detectamos patrones de uso generales, editoriales y tem\'aticos de los documentos e indagamos aspectos comunes y diferenciadores con respecto al uso mundial y otros pa\'ises de Am\'erica Latina. Asimismo, determinamos, a partir de distintas muestras, si los art\'iculos accedidos adem\'as est\'an disponibles en acceso abierto y/o su acceso es posible a trav\'es de las suscripciones pagadas por el estado nacional a trav\'es de la Biblioteca Electr\'onica de Ciencia y Tecnolog\'ia (BECyT). Encontramos que las descargas realizadas desde Argentina representan poco m\'as del 1\% de las registradas a nivel mundial, que existe una gran dispersi\'on de t\'itulos solicitados y que hay un aumento significativo en el uso respecto a los mismos per\'iodos de 2015 y 2016. Con las distintas muestras pudimos observar que se est\'an descargando mayormente art\'iculos de acceso restringido publicados por las empresas editoriales que manejan el sector como oligopolio, en revistas indexadas en la llamada ``corriente principal'' y que corresponden al \'area de la medicina (oncolog\'ia, pediatr\'ia y medicina cardiovascular). Tambi\'en, detectamos un n\'umero significativo de descargas de art\'iculos que ya estaban disponibles en acceso abierto, evidenciando un posible desconocimiento de estos recursos y calculamos un incremento importante respecto al uso de BECyT. Concluimos que las descargas desde Argentina tienen patrones similares al resto del mundo y que el aumento de su uso muestra que un cambio m\'as radical para garantizar el derecho de acceso a la informaci\'on es necesario. Mientras la informaci\'on cient\'ifica siga siendo una mercanc\'ia es primordial profundizar el estudio de los distintos proyectos que permiten reapropiarnos del conocimiento.},
  language = {es}
}

@article{moore_Preregister_2016,
  title = {Preregister If You Want To},
  author = {Moore, Don A.},
  year = {2016},
  month = apr,
  journal = {The American Psychologist},
  volume = {71},
  number = {3},
  pages = {238--239},
  issn = {1935-990X},
  doi = {10.1037/a0040195},
  abstract = {Prespecification of confirmatory hypothesis tests is a useful tool that makes our statistical tests informative. On the other hand, selectively reporting studies, measures, or statistical tests renders the probability of false positives higher than the p values would imply. The bad news is that it is usually difficult to tell how much higher the probability is. Fortunately, there are enormous opportunities to improve the quality of our science by preregistering our research plans. Preregistration is a highly distinctive strength that should increase our faith in the veracity and replicability of a research result.},
  language = {eng},
  pmid = {27042885},
  keywords = {Clinical Trials as Topic,Humans,Information Dissemination,Reproducibility of Results,Research Design,Science}
}

@article{motta_Dynamics_2018,
  title = {The {{Dynamics}} and {{Political Implications}} of {{Anti}}-{{Intellectualism}} in the {{United States}}},
  author = {Motta, Matthew},
  year = {2018},
  month = may,
  journal = {American Politics Research},
  volume = {46},
  number = {3},
  pages = {465--498},
  publisher = {{SAGE Publications Inc}},
  issn = {1532-673X},
  doi = {10.1177/1532673X17719507},
  abstract = {Recently, Americans have become increasingly likely to hold anti-intellectual attitudes (i.e., negative affect toward scientists and other experts). However, few have investigated the political implications of anti-intellectualism, and much empirical uncertainty surrounds whether or not these attitudes can be mitigated. Drawing on cross-sectional General Social Survey (GSS) data and a national election panel in 2016, I find that anti-intellectualism is associated with not only the rejection of policy-relevant matters of scientific consensus but support for political movements (e.g., ``Brexit'') and politicians (e.g., George Wallace, Donald Trump) who are skeptical of experts. Critically, though, I show that these effects can be mitigated. Verbal intelligence plays a strong role in mitigating anti-intellectual sympathies, compared with previously studied potential mitigators. I conclude by discussing how scholars might build on this research to study the political consequences of anti-intellectualism in the future.},
  language = {en},
  keywords = {anti-intellectualism,antiscience attitudes,political psychology,public opinion,verbal intelligence}
}

@article{motyl_state_2017,
  title = {The State of Social and Personality Science: {{Rotten}} to the Core, Not so Bad, Getting Better, or Getting Worse?},
  shorttitle = {The State of Social and Personality Science},
  author = {Motyl, Matt and Demos, Alexander P. and Carsel, Timothy S. and Hanson, Brittany E. and Melton, Zachary J. and Mueller, Allison B. and Prims, J. P. and Sun, Jiaqing and Washburn, Anthony N. and Wong, Kendal M. and Yantis, Caitlyn and Skitka, Linda J.},
  year = {2017},
  month = jul,
  journal = {Journal of Personality and Social Psychology},
  volume = {113},
  number = {1},
  pages = {34--58},
  issn = {1939-1315},
  doi = {10.1037/pspa0000084},
  abstract = {The scientific quality of social and personality psychology has been debated at great length in recent years. Despite research on the prevalence of Questionable Research Practices (QRPs) and the replicability of particular findings, the impact of the current discussion on research practices is unknown. The current studies examine whether and how practices have changed, if at all, over the last 10 years. In Study 1, we surveyed 1,166 social and personality psychologists about how the current debate has affected their perceptions of their own and the field's research practices. In Study 2, we coded the research practices and critical test statistics from social and personality psychology articles published in 2003-2004 and 2013-2014. Together, these studies suggest that (a) perceptions of the current state of the field are more pessimistic than optimistic; (b) the discussion has increased researchers' intentions to avoid QRPs and adopt proposed best practices, (c) the estimated replicability of research published in 2003-2004 may not be as bad as many feared, and (d) research published in 2013-2014 shows some improvement over research published in 2003-2004, a result that suggests the field is evolving in a positive direction. (PsycINFO Database Record},
  language = {eng},
  pmid = {28447837},
  keywords = {Attitude of Health Personnel,Ethics; Research,Female,Humans,Male,Personality,Psychology,Psychology; Social,Research,Research Design,Surveys and Questionnaires}
}

@article{munafo_manifesto_2017,
  title = {A Manifesto for Reproducible Science},
  author = {Munaf{\`o}, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and {Percie du Sert}, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
  year = {2017},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {1},
  number = {1},
  pages = {1--9},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-016-0021},
  abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
  copyright = {2017 Macmillan Publishers Limited},
  language = {en},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Social sciences Subject\_term\_id: social-sciences}
}

@misc{nassi-calo_Open_2013,
  title = {Open {{Access}} and a Call to Prevent the Looming Crisis in Science | {{SciELO}} in {{Perspective}}},
  author = {{Nassi-Cal{\`o}}, Lilian},
  year = {2013},
  month = jul,
  abstract = {The number of retracted articles has recently been on the rise. Bj\"orn Brembs identifies this tendency as a reflection of an imminent crisis in science whose},
  language = {en-US},
  keywords = {crisis}
}

@misc{nassi-calo_reproducibilidad_2014,
  title = {La Reproducibilidad En Los Resultados de Investigaci\'on: La Mirada Subjetiva | {{SciELO}} En {{Perspectiva}}},
  shorttitle = {La Reproducibilidad En Los Resultados de Investigaci\'on},
  author = {{Nassi-Cal{\`o}}, Lilian},
  year = {2014},
  month = feb,
  abstract = {En una \'epoca en que las discusiones sobre \'etica en la experimentaci\'on y la publicaci\'on cient\'ifica traspasan los laboratorios y ambientes acad\'emicos para},
  language = {en-US}
}

@article{naturehumanbehaviour_Tell_2020,
  title = {Tell It like It Is},
  author = {{Nature human behaviour}},
  year = {2020},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {4},
  number = {1},
  pages = {1--1},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-0818-9},
  abstract = {Every research paper tells a story, but the pressure to provide `clean' narratives is harmful for the scientific endeavour.},
  copyright = {2020 Springer Nature Limited},
  language = {en},
  keywords = {forrt,transparency}
}

@article{nosek_preregistration_2018,
  title = {The Preregistration Revolution},
  author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
  year = {2018},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {11},
  pages = {2600--2606},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708274114},
  abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes\textemdash a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
  chapter = {Colloquium Paper},
  copyright = {\textcopyright{} 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  language = {en},
  pmid = {29531091},
  keywords = {confirmatory analysis,exploratory analysis,methodology,open science,preregistration,reports,revisado}
}

@article{nosek_Preregistration_2019,
  title = {Preregistration {{Is Hard}}, {{And Worthwhile}}},
  author = {Nosek, Brian A. and Beck, Emorie D. and Campbell, Lorne and Flake, Jessica K. and Hardwicke, Tom E. and Mellor, David T. and {van 't Veer}, Anna E. and Vazire, Simine},
  year = {2019},
  month = oct,
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {10},
  pages = {815--818},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.07.009},
  abstract = {Preregistration clarifies the distinction between planned and unplanned research by reducing unnoticed flexibility. This improves credibility of findings and calibration of uncertainty. However, making decisions before conducting analyses requires practice. During report writing, respecting both what was planned and what actually happened requires good judgment and humility in making claims.},
  language = {en},
  keywords = {confirmatory research,exploratory research,preregistration,reproducibility,transparency}
}

@article{nosek_Promoting_2015,
  title = {Promoting an Open Research Culture},
  author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and {Mayo-Wilson}, E. and McNutt, M. and Miguel, E. and Paluck, E. Levy and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
  year = {2015},
  month = jun,
  journal = {Science},
  volume = {348},
  number = {6242},
  pages = {1422--1425},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab2374},
  abstract = {Author guidelines for journals could help to promote transparency, openness, and reproducibility Author guidelines for journals could help to promote transparency, openness, and reproducibility},
  chapter = {Policy Forum},
  copyright = {Copyright \textcopyright{} 2015, American Association for the Advancement of Science},
  language = {en},
  pmid = {26113702},
  keywords = {revisado}
}

@article{nosek_Registered_2014,
  title = {Registered {{Reports}}: {{A Method}} to {{Increase}} the {{Credibility}} of {{Published Results}}},
  shorttitle = {Registered {{Reports}}},
  author = {Nosek, Brian A. and Lakens, Dani{\"e}l},
  year = {2014},
  month = may,
  journal = {Social Psychology},
  volume = {45},
  number = {3},
  pages = {137--141},
  issn = {1864-9335, 2151-2590},
  doi = {10.1027/1864-9335/a000192},
  language = {en},
  keywords = {forrt,reports}
}

@article{nosek_Transparency_2014,
  title = {Transparency and {{Openness Promotion}} ({{TOP}}) {{Guidelines}}},
  author = {Nosek, Brian A. and Alter, George and Banks, George Christopher and Borsboom, Denny and Bowman, Sara and Breckler, Steven and Buck, Stuart and Chambers, Chris and Chin, Gilbert and Christensen, Garret},
  year = {2014},
  month = aug,
  publisher = {{OSF}},
  abstract = {The Transparency and Openness Promotion (TOP) Committee met in November 2014 to address one important element of the incentive systems - journals' procedures and  policies for publication. The outcome of the effort is the TOP Guidelines. There are eight standards in the TOP guidelines; each move scientific communication toward greater openness.  These standards are modular, facilitating adoption in whole or in part. However, they also complement each other, in that commitment to one standard may facilitate adoption of others. Moreover, the guidelines are sensitive to barriers to openness by articulating, for example, a process for exceptions to sharing because of ethical issues, intellectual property concerns, or availability of necessary resources.      Hosted on the Open Science Framework},
  language = {en},
  keywords = {(,transparency}
}

@misc{nw_Trust_2019,
  title = {Trust and {{Mistrust}} in {{Americans}}' {{Views}} of {{Scientific Experts}}},
  author = {NW, 1615 L. St and Suite 800Washington and Inquiries, DC 20036USA202-419-4300 | Main202-857-8562 | Fax202-419-4372 | Media},
  year = {2019},
  month = aug,
  journal = {Pew Research Center Science \& Society},
  abstract = {Public confidence in scientists is on the upswing, and six-in-ten Americans say scientists should play an active role in policy debates about scientific issues, according to a new Pew Research Center survey.},
  language = {en-US}
}

@article{nyhan_Increasing_2015,
  title = {Increasing the {{Credibility}} of {{Political Science Research}}: {{A Proposal}} for {{Journal Reforms}}},
  shorttitle = {Increasing the {{Credibility}} of {{Political Science Research}}},
  author = {Nyhan, Brendan},
  year = {2015},
  month = sep,
  journal = {Ps-Political Science \& Politics},
  volume = {48},
  pages = {78--83},
  publisher = {{Cambridge Univ Press}},
  address = {{New York}},
  issn = {1049-0965},
  doi = {10.1017/S1049096515000463},
  language = {English},
  keywords = {gender,medicaid,publication bias,registered-reports,replication,transparency},
  annotation = {WOS:000359291900014}
}

@article{oboyle_Chrysalis_2017,
  title = {The {{Chrysalis Effect}}: {{How Ugly Initial Results Metamorphosize Into Beautiful Articles}}},
  shorttitle = {The {{Chrysalis Effect}}},
  author = {O'Boyle, Ernest Hugh and Banks, George Christopher and {Gonzalez-Mul{\'e}}, Erik},
  year = {2017},
  month = feb,
  journal = {Journal of Management},
  volume = {43},
  number = {2},
  pages = {376--399},
  publisher = {{SAGE Publications Inc}},
  issn = {0149-2063},
  doi = {10.1177/0149206314527133},
  abstract = {The issue of a published literature not representative of the population of research is most often discussed in terms of entire studies being suppressed. However, alternative sources of publication bias are questionable research practices (QRPs) that entail post hoc alterations of hypotheses to support data or post hoc alterations of data to support hypotheses. Using general strain theory as an explanatory framework, we outline the means, motives, and opportunities for researchers to better their chances of publication independent of rigor and relevance. We then assess the frequency of QRPs in management research by tracking differences between dissertations and their resulting journal publications. Our primary finding is that from dissertation to journal article, the ratio of supported to unsupported hypotheses more than doubled (0.82 to 1.00 versus 1.94 to 1.00). The rise in predictive accuracy resulted from the dropping of statistically nonsignificant hypotheses, the addition of statistically significant hypotheses, the reversing of predicted direction of hypotheses, and alterations to data. We conclude with recommendations to help mitigate the problem of an unrepresentative literature that we label the ``Chrysalis Effect.''},
  language = {en},
  keywords = {ethics,morality and moral behavior,philosophy of science,revisado,statistical methods,transparency}
}

@misc{omatos_aspectos_2013,
  title = {Aspectos {{Legales}} En La {{Educaci\'on}}},
  author = {Omatos, Antonio},
  year = {2013}
}

@article{opensciencecollaboration_Estimating_2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716-aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  language = {en}
}

@article{ospina_problemas_2014,
  title = {Problemas de Propiedad Intelectual En El Entorno Universitario. {{Un}} Acercamiento General},
  author = {Ospina, Adriana Mar{\'i}a Restrepo},
  year = {2014},
  journal = {Estudios de Derecho},
  volume = {71},
  number = {158},
  pages = {69--96}
}

@article{patil_visual_2019,
  title = {A Visual Tool for Defining Reproducibility and Replicability},
  author = {Patil, Prasad and Peng, Roger D. and Leek, Jeffrey T.},
  year = {2019},
  month = jul,
  journal = {Nature Human Behaviour},
  volume = {3},
  number = {7},
  pages = {650--652},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0629-z},
  abstract = {Reproducibility and replicability are fundamental requirements of scientific studies. Disagreements over universal definitions for these terms have affected the interpretation of large-scale replication attempts. We provide a visual tool for representing definitions and use it to re-examine these attempts.},
  copyright = {2019 Springer Nature Limited},
  language = {en},
  keywords = {herramienta},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Comments \& Opinion Subject\_term: Software;Statistics Subject\_term\_id: software;statistics}
}

@article{paul_Making_2021,
  title = {Making {{ERP}} Research More Transparent: {{Guidelines}} for Preregistration},
  shorttitle = {Making {{ERP}} Research More Transparent},
  author = {Paul, Mariella and Govaart, Gisela H. and Schettino, Antonio},
  year = {2021},
  month = jun,
  journal = {International Journal of Psychophysiology},
  volume = {164},
  pages = {52--63},
  publisher = {{Elsevier}},
  address = {{Amsterdam}},
  issn = {0167-8760},
  doi = {10.1016/j.ijpsycho.2021.02.016},
  abstract = {A combination of confirmation bias, hindsight bias, and pressure to publish may prompt the (unconscious) exploration of various methodological options and reporting only the ones that lead to a (statistically) significant outcome. This undisclosed analytic flexibility is particularly relevant in EEG research, where a myriad of preprocessing and analysis pipelines can be used to extract information from complex multidimensional data. One solution to limit confirmation and hindsight bias by disclosing analytic choices is preregistration: researchers write a time-stamped, publicly accessible research plan with hypotheses, data collection plan, and the intended preprocessing and statistical analyses before the start of a research project. In this manuscript, we present an overview of the problems associated with undisclosed analytic flexibility, discuss why and how EEG researchers would benefit from adopting preregistration, provide guidelines and examples on how to preregister data preprocessing and analysis steps in typical ERP studies, and conclude by discussing possibilities and limitations of this open science practice.},
  language = {English},
  keywords = {eeg,eeg-data,erp,error-related negativity,incentives,independent   components,meg,null,Open science,potentials,Preregistration,processing pipeline,publication,randomized clinical-trials},
  annotation = {WOS:000645667300007}
}

@misc{pena_declaracion_2003,
  title = {{Declaraci\'on de Bethesda sobre Publicaci\'on de Acceso Abierto}},
  author = {Pe{\~n}a, Ismaes},
  year = {20 de Junio, 2003},
  language = {Traducido}
}

@article{penders_Rinse_2019,
  title = {Rinse and {{Repeat}}: {{Understanding}} the {{Value}} of {{Replication}} across {{Different Ways}} of {{Knowing}}},
  shorttitle = {Rinse and {{Repeat}}},
  author = {Penders, Bart and Holbrook, J. Britt and {de Rijcke}, Sarah},
  year = {2019},
  month = sep,
  journal = {Publications},
  volume = {7},
  number = {3},
  pages = {52},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/publications7030052},
  abstract = {The increasing pursuit of replicable research and actual replication of research is a political project that articulates a very specific technology of accountability for science. This project was initiated in response to concerns about the openness and trustworthiness of science. Though applicable and valuable in many fields, here we argue that this value cannot be extended everywhere, since the epistemic content of fields, as well as their accountability infrastructures, differ. Furthermore, we argue that there are limits to replicability across all fields; but in some fields, including parts of the humanities, these limits severely undermine the value of replication to account for the value of research.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  language = {en},
  keywords = {accountability,epistemic pluralism,humanities,Replicability,replication,reproducibility,reproduction}
}

@article{peng_reproducibility_2015,
  title = {The Reproducibility Crisis in Science: {{A}} Statistical Counterattack},
  shorttitle = {The Reproducibility Crisis in Science},
  author = {Peng, Roger},
  year = {2015},
  journal = {Significance},
  volume = {12},
  number = {3},
  pages = {30--32},
  issn = {1740-9713},
  doi = {10.1111/j.1740-9713.2015.00827.x},
  abstract = {More people have more access to data than ever before. But a comparative lack of analytical skills has resulted in scientific findings that are neither replicable nor reproducible. It is time to invest in statistics education, says Roger Peng},
  copyright = {\textcopyright{} 2015 The Royal Statistical Society},
  language = {en},
  keywords = {crisis},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2015.00827.x}
}

@article{peng_Reproducible_2011,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  author = {Peng, R. D.},
  year = {2011},
  month = dec,
  journal = {Science},
  volume = {334},
  number = {6060},
  pages = {1226--1227},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1213847},
  language = {en}
}

@article{petousi_Contextualising_2020,
  title = {Contextualising Harm in the Framework of Research Misconduct. {{Findings}} from Discourse Analysis of Scientific Publications},
  author = {Petousi, Vasiliki and Sifaki, Eirini},
  year = {2020},
  month = jan,
  journal = {International Journal of Sustainable Development},
  volume = {23},
  number = {3-4},
  pages = {149--174},
  publisher = {{Inderscience Publishers}},
  issn = {0960-1406},
  doi = {10.1504/IJSD.2020.115206},
  abstract = {This article reports on research, which deals with dimensions of harm resulting from research misconduct, in articles published in scientific journals. An appropriate sample of publications retrieved from Pubmed, Scopus and WOS was selected across various disciplines and topics. Implementing discourse analysis, articles were classified according to the narratives of 'individual impurity', 'institutional failure' and 'structural crisis'. Most of the articles analysed fall within the narrative of structural crisis. The main argument advanced is that research misconduct harms the scientific enterprise as a whole. Harm is narrated in the context of institutional characteristics, policies, procedures, guidelines, and work environment. Mainly, however, harm is narrated in the context of structural characteristics of contemporary scientific practices, which result in normative dissonance for scientists and loss of trust in science in the relation between science and society and within the scientific enterprise itself. We conclude that new grounds for building trust and confidence in science are needed.},
  keywords = {practices}
}

@article{pham_Not_2021,
  title = {On {{Not Confusing}} the {{Tree}} of {{Trustworthy Statistics}} with the {{Greater Forest}} of {{Good Science}}: {{A Comment}} on {{Simmons}} et al.'s {{Perspective}} on {{Pre}}-Registration},
  shorttitle = {On {{Not Confusing}} the {{Tree}} of {{Trustworthy Statistics}} with the {{Greater Forest}} of {{Good Science}}},
  author = {Pham, Michel Tuan and Oh, Travis Tae},
  year = {2021},
  month = jan,
  journal = {Journal of Consumer Psychology},
  volume = {31},
  number = {1},
  pages = {181--185},
  publisher = {{John Wiley \& Sons Ltd}},
  address = {{Chichester}},
  issn = {1057-7408},
  doi = {10.1002/jcpy.1213},
  abstract = {In this commentary on Simmons, Nelson, and Simonsohn (this issue), we examine their rationale for pre-registration within the broader perspective of what good science is. We agree that there is potential benefit in a system of pre-registration if implemented selectively. However, we believe that other tools of open science such as the full sharing of study materials and open access to underlying data, provide most of the same benefits-and more (i.e., the prevention of outright fraud)-without risking the potential adverse consequences of a system of pre-registration. This is why we favor these other means of controlling type I error and fostering transparency. Direct replication, as opposed to conceptual replication, should be encouraged as well.},
  language = {English},
  keywords = {consumer   research,open science,philosophy of science,preregistration,reproducibility crisis},
  annotation = {WOS:000618078000015}
}

@article{pham_Preregistration_2021,
  title = {Preregistration {{Is Neither Sufficient}} nor {{Necessary}} for {{Good Science}}},
  author = {Pham, Michel Tuan and Oh, Travis Tae},
  year = {2021},
  month = jan,
  journal = {Journal of Consumer Psychology},
  volume = {31},
  number = {1},
  pages = {163--176},
  publisher = {{John Wiley \& Sons Ltd}},
  address = {{Chichester}},
  issn = {1057-7408},
  doi = {10.1002/jcpy.1209},
  abstract = {To address widespread perceptions of a reproducibility crisis in the social sciences, a growing number of scholars recommend the systematic preregistration of empirical studies. The purpose of this article is to contribute to an epistemological dialogue on the value of preregistration in consumer research by identifying the limitations, drawbacks, and potential adverse effects of a preregistration system. After a brief review of some of the implementation challenges that commonly arise with preregistration, we raise three levels of issues with a system of preregistration. First, we identify its limitations as a means of advancing consumer knowledge, thus questioning the sufficiency of preregistration in promoting good consumer science. Second, we elaborate on why consumer science can progress even in the absence of preregistration, thereby also questioning the necessity of preregistration in promoting good consumer science. Third, we discuss serious potential adverse effects of preregistration, both at the individual researcher level and at the level of the field as a whole. We conclude by offering a broader perspective on the narrower role that preregistration can play within the general pursuit of building robust and useful knowledge about consumers.},
  language = {English},
  keywords = {Consumer research,Open science,Preregistration,Reproducibility crisis},
  annotation = {WOS:000618078000013}
}

@article{piwowar_future_2019,
  title = {The {{Future}} of {{OA}}: {{A}} Large-Scale Analysis Projecting {{Open Access}} Publication and Readership},
  shorttitle = {The {{Future}} of {{OA}}},
  author = {Piwowar, Heather and Priem, Jason and Orr, Richard},
  year = {2019},
  journal = {BioRxiv},
  pages = {795310},
  publisher = {{Cold Spring Harbor Laboratory}}
}

@article{piwowar_state_2018,
  title = {The State of {{OA}}: A Large-Scale Analysis of the Prevalence and Impact of {{Open Access}} Articles},
  shorttitle = {The State of {{OA}}},
  author = {Piwowar, Heather and Priem, Jason and Larivi{\`e}re, Vincent and Alperin, Juan Pablo and Matthias, Lisa and Norlander, Bree and Farley, Ashley and West, Jevin and Haustein, Stefanie},
  year = {2018},
  journal = {PeerJ},
  volume = {6},
  pages = {e4375},
  publisher = {{PeerJ Inc.}}
}

@incollection{poumadere_Credibility_1991,
  title = {The {{Credibility Crisis}}},
  booktitle = {Chernobyl: {{A Policy Response Study}}},
  author = {Poumad{\`e}re, Marc},
  editor = {Segerst{\aa}hl, Boris},
  year = {1991},
  series = {Springer {{Series}} on {{Environmental Management}}},
  pages = {149--171},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-84367-9_8},
  abstract = {Those who said that the twentieth century is the century of the atom didn't know how right they were. Certainly, the scientific discovery of a particularly powerful new energy source had produced hopes and applications in both the civil and military sectors. In the final equation, though, it is the major accident at Chernobyl that dramatically and suddenly brought the reality of the atom's presence home to many persons and groups around the world.},
  isbn = {978-3-642-84367-9},
  language = {en},
  keywords = {Chernobyl Accident,crisis,Nuclear Energy,Nuclear Power Plant,Social Defense,Social Distance}
}

@article{price_Problem_2020,
  title = {Problem with p Values: Why p Values Do Not Tell You If Your Treatment Is Likely to Work},
  shorttitle = {Problem with p Values},
  author = {Price, Robert and Bethune, Rob and Massey, Lisa},
  year = {2020},
  month = jan,
  journal = {Postgraduate Medical Journal},
  volume = {96},
  number = {1131},
  pages = {1--3},
  issn = {0032-5473, 1469-0756},
  doi = {10.1136/postgradmedj-2019-137079},
  language = {en},
  keywords = {forrt,practices}
}

@article{rabelo_Questionable_2020,
  title = {Questionable Research Practices among {{Brazilian}} Psychological Researchers: {{Results}} from a Replication Study and an International Comparison},
  shorttitle = {Questionable Research Practices among {{Brazilian}} Psychological Researchers},
  author = {Rabelo, Andr{\'e} L. A. and Farias, J{\'e}ssica E. M. and Sarmet, Maur{\'i}cio M. and Joaquim, Teresa C. R. and Hoersting, Raquel C. and Victorino, Luiz and Modesto, Jo{\~a}o G. N. and Pilati, Ronaldo},
  year = {2020},
  journal = {International Journal of Psychology},
  volume = {55},
  number = {4},
  pages = {674--683},
  issn = {1464-066X},
  doi = {10.1002/ijop.12632},
  abstract = {Research on scientific integrity is growing in psychology, and questionable research practices (QRPs) have received more attention due to its harmful effect on science. By replicating the procedures of previous research, the present study aimed at describing the use of QRPs among Brazilian psychological researchers and to make an international comparison with previous studies in other countries\textemdash the US and Italy. Two hundred and thirty-two Brazilian researchers in the field of psychology answered questions related to 10 different QRPs. Brazilian researchers indicated a lower tendency to engage in two QRPs (failing to report all of a study's dependent measures; deciding whether to collect more data after looking to see whether the results were significant) when compared to their Italian and North American counterparts, but indicated a higher tendency to engage in two other QRPs (selectively reporting studies that ``worked''; not reporting all of a study's conditions). Most of the sample did not admit integrity conflicts in their own research but indicated that others have integrity problems, as observed in previous studies. Those discrepancies could be attributed to contextual and systemic factors regarding different publication demands among the different nations. Further studies should focus on identifying the antecedents of QRPs.},
  language = {en},
  keywords = {Bias,Meta-research,Questionable research practices,Replicability,Scientific integrity},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ijop.12632}
}

@article{raj_PHacking_2018,
  title = {P-{{Hacking}}: {{A Wake}}-{{Up Call}} for the {{Scientific Community}}},
  shorttitle = {P-{{Hacking}}},
  author = {Raj, A. Thirumal and Patil, Shankargouda and Sarode, Sachin and Salameh, Ziad},
  year = {2018},
  month = dec,
  journal = {Science and Engineering Ethics},
  volume = {24},
  number = {6},
  pages = {1813--1814},
  publisher = {{Springer}},
  address = {{Dordrecht}},
  issn = {1353-3452},
  doi = {10.1007/s11948-017-9984-1},
  abstract = {P-hacking or data dredging involves manipulation of the research data in order to obtain a statistically significant result. The reasons behind P-hacking and the consequences of the same are discussed in the present manuscript.},
  language = {English},
  keywords = {Financing,Journal impact factor,Organized,Publications,Research design},
  annotation = {WOS:000451715300010}
}

@misc{rapp_what_2019,
  title = {What {{bioRxiv}}'s First 30,000 Preprints Reveal about Biologists},
  author = {Rapp, Joshua},
  year = {2019},
  journal = {nature},
  abstract = {More than 1 million studies are now downloaded from the site every month, mostly in neuroscience, bioinformatics and genomics.},
  language = {English}
}

@article{redish_Opinion_2018,
  title = {Opinion: {{Reproducibility}} Failures Are Essential to Scientific Inquiry},
  shorttitle = {Opinion},
  author = {Redish, A. David and Kummerfeld, Erich and Morris, Rebecca Lea and Love, Alan C.},
  year = {2018},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {20},
  pages = {5042--5046}
}

@article{restrepo_problemas_2014,
  title = {Problemas de Propiedad Intelectual En El Entorno Universitario. {{Un}} Acercamiento General},
  author = {Restrepo, Adriana Mar{\'i}a},
  year = {30 de Junio, 2014},
  volume = {71},
  number = {158},
  pages = {69--96}
}

@article{rice_Curtailing_2019,
  title = {Curtailing the {{Use}} of {{Preregistration}}: {{A Misused Term}}},
  shorttitle = {Curtailing the {{Use}} of {{Preregistration}}},
  author = {Rice, Danielle B. and Moher, David},
  year = {2019},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {14},
  number = {6},
  pages = {1105--1108},
  publisher = {{Sage Publications Ltd}},
  address = {{London}},
  issn = {1745-6916},
  doi = {10.1177/1745691619858427},
  abstract = {Improving the usability of psychological research has been encouraged through practices such as prospectively registering research plans. Registering research aligns with the open-science movement, as the registration of research protocols in publicly accessible domains can result in reduced research waste and increased study transparency. In medicine and psychology, two different terms, registration and preregistration, have been used to refer to study registration, but applying inconsistent terminology to represent one concept can complicate both educational outreach and epidemiological investigation. Consistently using one term across disciplines to refer to the concept of study registration may improve the understanding and uptake of this practice, thereby supporting the movement toward improving the reliability and reproducibility of research through study registration. We recommend encouraging use of the original term, registration, given its widespread and long-standing use, including in national registries.},
  language = {English},
  keywords = {open science,preregistration,protocols,Registered Reports,registration,trial registration},
  annotation = {WOS:000483904100001}
}

@misc{rinke_Probabilistic_2018,
  title = {Probabilistic {{Misconceptions Are Pervasive Among Communication Researchers}}},
  author = {Rinke, Eike Mark and Schneider, Frank M.},
  year = {2018},
  month = sep,
  institution = {{SocArXiv}},
  doi = {10.31235/osf.io/h8zbe},
  abstract = {Across all areas of communication research, the most popular approach to generating insights about communication is the classical significance test (also called null hypothesis significance testing, NHST). The predominance of NHST in communication research is in spite of serious concerns about the ability of researchers to properly interpret its results. We draw on data from a survey of the ICA membership to assess the evidential basis of these concerns. The vast majority of communication researchers misinterpreted NHST (91\%) and the most prominent alternative, confidence intervals (96\%), while overestimating their competence. Academic seniority and statistical experience did not predict better interpretation outcomes. These findings indicate major problems regarding the generation of knowledge in the field of communication research.},
  keywords = {Communication,confidence intervals,data analysis,misconceptions,practices,significance testing,Social and Behavioral Sciences,statistical inference,statistics}
}

@article{rodriguez-sanchez_Ciencia_2016,
  title = {{Ciencia reproducible: qu\'e, por qu\'e, c\'omo}},
  shorttitle = {{Ciencia reproducible}},
  author = {{Rodriguez-Sanchez}, Francisco and {P{\'e}rez-Luque}, Antonio Jes{\'u}s and Bartomeus, Ignasi and Varela, Sara},
  year = {2016},
  month = jul,
  journal = {Ecosistemas},
  volume = {25},
  number = {2},
  pages = {83--92},
  issn = {1697-2473},
  doi = {10.7818/ECOS.2016.25-2.11},
  copyright = {Derechos de autor},
  language = {es},
  keywords = {reproducibilidad,revisado}
}

@article{rosenthal_file_1979,
  title = {The File Drawer Problem and Tolerance for Null Results.},
  author = {Rosenthal, Robert},
  year = {1979},
  journal = {Psychological Bulletin},
  volume = {86},
  number = {3},
  pages = {638--641},
  issn = {0033-2909},
  doi = {10.1037/0033-2909.86.3.638},
  language = {en}
}

@misc{rowe_Preview_2018,
  title = {Preview My New Book: {{Introduction}} to {{Reproducible Science}} in {{R}} | {{R}}-Bloggers},
  shorttitle = {Preview My New Book},
  author = {Rowe, Brian Lee Yung},
  year = {2018},
  month = nov,
  abstract = {I'm pleased to share Part I of my new book ``Introduction to Reproducible Science in R``. The purpose of this \ldots Continue reading \textrightarrow},
  language = {en-US}
}

@article{rubin_Does_2020,
  title = {Does Preregistration Improve the Credibility of Research Findings?},
  author = {Rubin, Mark},
  year = {2020},
  journal = {Quantitative Methods for Psychology},
  volume = {16},
  number = {4},
  pages = {376--390},
  publisher = {{Univ Montreal, Dept Psychologie}},
  address = {{Montreal}},
  issn = {1913-4126},
  doi = {10.20982/tqmp.16.4.p376},
  abstract = {Preregistration entails researchers registering their planned research hypotheses, methods, and analyses in a time-stamped document before they undertake their data collection and analyses. This document is then made available with the published research report to allow readers to identify discrepancies between what the researchers originally planned to do and what they actually ended up doing. This historical transparency is supposed to facilitate judgments about the credibility of the research findings. The present article provides a critical review of 17 of the reasons behind this argument. The article covers issues such as HARKing, multiple testing, p-hacking, forking paths, optional stopping, researchers' biases, selective reporting, test severity, publication bias, and replication rates. It is concluded that preregistration's historical transparency does not facilitate judgments about the credibility of research findings when researchers provide contemporary transparency in the form of (a) clear rationales for current hypotheses and analytical approaches, (b) public access to research data, materials, and code, and (c) demonstrations of the robustness of research conclusions to alternative interpretations and analytical approaches.},
  language = {English},
  keywords = {bias,error,forking paths,harking,HARKing,inference,multiple testing,optional stopping,p-hacking,preregistration,publication bias,science},
  annotation = {WOS:000573887700010}
}

@article{rubin_Evaluation_2017,
  title = {An {{Evaluation}} of {{Four Solutions}} to the {{Forking Paths Problem}}: {{Adjusted Alpha}}, {{Preregistration}}, {{Sensitivity Analyses}}, and {{Abandoning}} the {{Neyman}}-{{Pearson Approach}}},
  shorttitle = {An {{Evaluation}} of {{Four Solutions}} to the {{Forking Paths Problem}}},
  author = {Rubin, Mark},
  year = {2017},
  month = dec,
  journal = {Review of General Psychology},
  volume = {21},
  number = {4},
  pages = {321--329},
  publisher = {{Sage Publications Inc}},
  address = {{Thousand Oaks}},
  issn = {1089-2680},
  doi = {10.1037/gpr0000135},
  abstract = {Gelman and Loken (2013, 2014) proposed that when researchers base their statistical analyses on the idiosyncratic characteristics of a specific sample (e.g., a nonlinear transformation of a variable because it is skewed), they open up alternative analysis paths in potential replications of their study that are based on different samples (i.e., no transformation of the variable because it is not skewed). These alternative analysis paths count as additional (multiple) tests and, consequently, they increase the probability of making a Type I error during hypothesis testing. The present article considers this forking paths problem and evaluates four potential solutions that might be used in psychology and other fields: (a) adjusting the prespecified alpha level, (b) preregistration, (c) sensitivity analyses, and (d) abandoning the Neyman-Pearson approach. It is concluded that although preregistration and sensitivity analyses are effective solutions to p-hacking, they are ineffective against result-neutral forking paths, such as those caused by transforming data. Conversely, although adjusting the alpha level cannot address p-hacking, it can be effective for result-neutral forking paths. Finally, abandoning the Neyman-Pearson approach represents a further solution to the forking paths problem.},
  language = {English},
  keywords = {confusion,familywise error,forking paths,null hypothesis significance testing,okeefes,p-values,preregistration,registered reports,replication crisis,sensitivity analyses,statistical-methods},
  annotation = {WOS:000417900400004}
}

@article{rytchkov_Information_2020,
  title = {Information {{Aggregation}} and {{P}}-{{Hacking}}},
  author = {Rytchkov, Oleg and Zhong, Xun},
  year = {2020},
  month = apr,
  journal = {Management Science},
  volume = {66},
  number = {4},
  pages = {1605--1626},
  publisher = {{Informs}},
  address = {{Catonsville}},
  issn = {0025-1909},
  doi = {10.1287/mnsc.2018.3259},
  abstract = {This paper studies the interplay between information aggregation and p-hacking in the context of predicting stock returns. The standard information-aggregation techniques exacerbate p-hacking by increasing the probability of the type I error. We propose an aggregation technique that is a simple modification of three-pass regression filter/ partial least squares regression with an opposite property: the predictability tests applied to the combined predictor become more conservative in the presence of p-hacking. Using simulations, we quantify the advantages of our approach relative to the standard information-aggregation techniques. We also apply our aggregation technique to three sets of return predictors proposed in the literature and find that the forecasting ability of combined predictors in two cases cannot be explained by p-hacking.},
  language = {English},
  keywords = {3prf,book-to-market,cross-section,false discoveries,forecast combination,p-hacking,performance,pls,predictability,predictability of returns,prediction,premium,presidential-address,stock returns,tests},
  annotation = {WOS:000531064000007}
}

@article{sadaba_acceso_2014,
  title = {{{EL ACCESO ABIERTO EN CIENCIAS SOCIALES}}: {{NOTAS SOCIOL\'OGICAS SOBRE PUBLICACIONES}}, {{COMUNIDADES Y CAMPOS}}},
  author = {Sadaba, Igor},
  year = {2014},
  volume = {17},
  pages = {93--113},
  issn = {1139-3327},
  abstract = {En el presente art\'iculo proponemos evitar las caracterizaciones abstractas y pol\'iticas del Open Access para pasar a evaluar emp\'iricamente su funcionamiento. Solo apart\'andonos de los manifiestos program\'aticos y los listados de beneficios te\'oricos de dichas pr\'acticas podremos valorar en su justa medida las resistencias existentes y aprovechar sus potencialidades reales. En concreto, se propone estudiar el Open Access en las Ciencias Sociales (en comparaci\'on con las Ciencias Naturales) y entender que todav\'ia estamos ante un proceso desigual de difusi\'on del conocimiento acad\'emico debido, en parte, a dos nociones sociol\'ogicas centrales (de dos autores tambi\'en centrales en las propias Ciencias Sociales): i) la arquitectura diferencial de sus ``comunidades cient\'ificas'' (Merton) y ii) las diferentes reglas de ``campo acad\'emico'' (Bourdieu) configuradas a partir del dominio de los \'indices de impacto en las ciencias contempor\'aneas.}
}

@article{schindler_investigating_2021,
  title = {Investigating {{Software Usage}} in the {{Social Sciences}}: {{A Knowledge Graph Approach}}},
  shorttitle = {Investigating {{Software Usage}} in the {{Social Sciences}}},
  author = {Schindler, David and Zapilko, Benjamin and Kr{\"u}ger, Frank},
  year = {2021},
  month = aug,
  journal = {arXiv:2003.10715 [cs]},
  eprint = {2003.10715},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Knowledge about the software used in scientific investigations is necessary for different reasons, including provenance of the results, measuring software impact to attribute developers, and bibliometric software citation analysis in general. Additionally, providing information about whether and how the software and the source code are available allows an assessment about the state and role of open source software in science in general. While such analyses can be done manually, large scale analyses require the application of automated methods of information extraction and linking. In this paper, we present SoftwareKG - a knowledge graph that contains information about software mentions from more than 51,000 scientific articles from the social sciences. A silver standard corpus, created by a distant and weak supervision approach, and a gold standard corpus, created by manual annotation, were used to train an LSTM based neural network to identify software mentions in scientific articles. The model achieves a recognition rate of .82 F-score in exact matches. As a result, we identified more than 133,000 software mentions. For entity disambiguation, we used the public domain knowledge base DBpedia. Furthermore, we linked the entities of the knowledge graph to other knowledge bases such as the Microsoft Academic Knowledge Graph, the Software Ontology, and Wikidata. Finally, we illustrate, how SoftwareKG can be used to assess the role of software in the social sciences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval}
}

@article{schnell_Reproducible_2018,
  title = {``{{Reproducible}}'' {{Research}} in {{Mathematical Sciences Requires Changes}} in Our {{Peer Review Culture}} and {{Modernization}} of Our {{Current Publication Approach}}},
  author = {Schnell, Santiago},
  year = {2018},
  month = dec,
  journal = {Bulletin of Mathematical Biology},
  volume = {80},
  number = {12},
  pages = {3095--3105},
  issn = {1522-9602},
  doi = {10.1007/s11538-018-0500-9},
  abstract = {The nature of scientific research in mathematical and computational biology allows editors and reviewers to evaluate the findings of a scientific paper. Replication of a research study should be the minimum standard for judging its scientific claims and considering it for publication. This requires changes in the current peer review practice and a strict adoption of a replication policy similar to those adopted in experimental fields such as organic synthesis. In the future, the culture of replication can be easily adopted by publishing papers through dynamic computational notebooks combining formatted text, equations, computer algebra and computer code.},
  language = {en}
}

@article{schoenbrodt_Fostering_2018,
  title = {{Fostering Research Transparency as a Key Property of Science: Concrete Actions for Psychological Departments}},
  shorttitle = {{Fostering Research Transparency as a Key Property of Science}},
  author = {Schoenbrodt, Felix D. and Maier, Markus and Heene, Moritz and Buehner, Markus},
  year = {2018},
  month = jan,
  journal = {Psychologische Rundschau},
  volume = {69},
  number = {1},
  pages = {37--44},
  publisher = {{Hogrefe \& Huber Publishers}},
  address = {{Gottingen}},
  issn = {0033-3042},
  doi = {10.1026/0033-3042/a000386},
  abstract = {Recent large-scale replication projects suggest an amount of nonreplicable results in the scientific literature, in psychology but also in other sciences, which is concerning from our point of view. We analyze some causes for this situation, and argue that the change toward more research transparency ("open science") must be one consequence that should be drawn from the credibility crisis. We call for feasible changes in the local research units and departments and show as an example the steps that have been taken at the Department of Psychology of the Ludwig-Maximilians-Universitat Munchen. These changes concern incentive structures, research culture, teaching, and a close integration with the local ethics committee. The goal is to foster a more credible and more reproducible research output without generating unnecessary bureaucratic overhead.},
  language = {German},
  keywords = {credibility crisis,game,incentives,registered reports,replicability,replication crisis,research quality,rules,standards,truth},
  annotation = {WOS:000419334000004}
}

@misc{serevicionacionaldelpatrimoniocultural_tratados_,
  title = {Tratados {{Inernacionales}}},
  author = {{Serevicio Nacional del Patrimonio Cultural}},
  journal = {Departamento de Derechos Intelectuales}
}

@article{serra-garcia_Nonreplicable_2021,
  title = {Nonreplicable Publications Are Cited More than Replicable Ones},
  author = {{Serra-Garcia}, Marta and Gneezy, Uri},
  year = {2021},
  month = may,
  journal = {Science Advances},
  volume = {7},
  number = {21},
  pages = {eabd1705},
  issn = {2375-2548},
  doi = {10.1126/sciadv.abd1705},
  abstract = {Published papers that fail to replicate are cited more than those that replicate, even after the failure is published., We use publicly available data to show that published papers in top psychology, economics, and general interest journals that fail to replicate are cited more than those that replicate. This difference in citation does not change after the publication of the failure to replicate. Only 12\% of postreplication citations of nonreplicable findings acknowledge the replication failure. Existing evidence also shows that experts predict well which papers will be replicated. Given this prediction, why are nonreplicable papers accepted for publication in the first place? A possible answer is that the review team faces a trade-off. When the results are more ``interesting,'' they apply lower standards regarding their reproducibility.},
  pmcid = {PMC8139580},
  pmid = {34020944}
}

@article{sijtsma_Steps_2021,
  title = {Steps toward Preregistration of Research on Research Integrity},
  author = {Sijtsma, Klaas and Emons, Wilco H. M. and Steneck, Nicholas H. and Bouter, Lex M.},
  year = {2021},
  month = mar,
  journal = {Research Integrity and Peer Review},
  volume = {6},
  number = {1},
  pages = {5},
  publisher = {{Bmc}},
  address = {{London}},
  doi = {10.1186/s41073-021-00108-4},
  abstract = {Background: A proposal to encourage the preregistration of research on research integrity was developed and adopted as the Amsterdam Agenda at the 5th World Conference on Research Integrity (Amsterdam, 2017). This paper reports on the degree to which abstracts of the 6th World Conference in Research Integrity (Hong Kong, 2019) reported on preregistered research. Methods: Conference registration data on participants presenting a paper or a poster at 6th WCRI were made available to the research team. Because the data set was too small for inferential statistics this report is limited to a basic description of results and some recommendations that should be considered when taking further steps to improve preregistration. Results: 19\% of the 308 presenters preregistered their research. Of the 56 usable cases, less than half provided information on the six key elements of the Amsterdam Agenda. Others provided information that invalidated their data, such as an uninformative URL. There was no discernable difference between qualitative and quantitative research. Conclusions: Some presenters at the WCRI have preregistered their research on research integrity, but further steps are needed to increase frequency and completeness of preregistration. One approach to increase preregistration would be to make it a requirement for research presented at the World Conferences on Research Integrity.},
  language = {English},
  keywords = {Open Science framework,Paper and poster presenters,Preregistration,Registry for research on the responsible conduct of research,Responsible conduct of research,Study protocol,World conference on   research integrity},
  annotation = {WOS:000624023800001}
}

@article{simmons_FalsePositive_2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  language = {en},
  keywords = {practices}
}

@article{simmons_FalsePositive_2018,
  title = {False-{{Positive Citations}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2018},
  month = mar,
  journal = {Perspectives on Psychological Science},
  volume = {13},
  number = {2},
  pages = {255--259},
  publisher = {{Sage Publications Ltd}},
  address = {{London}},
  issn = {1745-6916},
  doi = {10.1177/1745691617698146},
  abstract = {We describe why we wrote False-Positive Psychology, analyze how it has been cited, and explain why the integrity of experimental psychology hinges on the full disclosure of methods, the sharing of materials and data, and, especially, the preregistration of analyses.},
  language = {English},
  keywords = {p-hacking,preregistration,replicability,research methods},
  annotation = {WOS:000429909000024}
}

@article{simonsohn_Pcurve_2014,
  title = {P-Curve: {{A}} Key to the File-Drawer},
  shorttitle = {P-Curve},
  author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
  year = {2014},
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  number = {2},
  pages = {534--547},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/a0033242},
  abstract = {Because scientists tend to report only studies (publication bias) or analyses (p-hacking) that ``work,'' readers must ask, ``Are these effects true, or do they merely reflect selective reporting?'' We introduce p-curve as a way to answer this question. P-curve is the distribution of statistically significant p values for a set of studies (ps p-curves\textemdash containing more low (.01s) than high (.04s) significant p values\textemdash only right-skewed p-curves are diagnostic of evidential value. By telling us whether we can rule out selective reporting as the sole explanation for a set of findings, p-curve offers a solution to the age-old inferential problems caused by file-drawers of failed studies and analyses. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {herramienta,Hypothesis Testing,practices,Psychology,Scientific Communication,Statistics}
}

@misc{socha_cuanto_2018,
  title = {{\textquestiondown Cu\'anto cobran los principales editores comerciales por tener un art\'iculo en acceso abierto?}},
  author = {Socha, Beata},
  year = {2018},
  journal = {Universo Abierto},
  abstract = {Los cuatro grandes actores de la industria editorial, Elsevier, Springer, Wiley y Taylor \& Francis, han adoptado el acceso abierto (Open Access, OA), a trav\'es de la modalidad ``El autor paga'' aunque en distintos grados. Tambi\'en han empleado estrategias muy diferentes en cuanto a cu\'anto cobran a sus autores. Para cualquier autor que desee publicar su investigaci\'on en Acceso Abierto en alguna de estas revistas probablemente necesita conocer lo que el mercado editorial tiene para ofrecer y qu\'e gama de precios existe. Los datos primarios proceden de las listas de precios oficiales de los editores disponibles en sus sitios web.},
  language = {Traducido}
}

@article{sociedadmaxplanck_declaracion_2003,
  title = {La {{Declaraci\'on}} de {{Berl\'in}} Sobre Acceso Abierto.},
  author = {Sociedad Max Planck},
  year = {2003},
  volume = {1},
  number = {2},
  pages = {152--154}
}

@techreport{soderberg_Initial_2020,
  type = {Preprint},
  title = {Initial {{Evidence}} of {{Research Quality}} of {{Registered Reports Compared}} to the {{Traditional Publishing Model}}},
  author = {Soderberg, Courtney K. and Errington, Timothy M. and Schiavone, Sarah R. and Bottesini, Julia G. and Singleton Thorn, Felix and Vazire, Simine and Esterling, Kevin and Nosek, Brian A.},
  year = {2020},
  month = nov,
  institution = {{MetaArXiv}},
  doi = {10.31222/osf.io/7x9vy},
  abstract = {In Registered Reports (RRs), initial peer review and in-principle acceptance occurs before knowing the research outcomes. This combats publication bias and distinguishes planned and unplanned research. How RRs could improve the credibility of research findings is straightforward, but there is little empirical evidence. Also, there could be unintended costs such as reducing novelty. 353 researchers peer reviewed a pair of papers from 29 published RRs from psychology and neuroscience and 57 non-RR comparison papers. RRs outperformed comparison papers on all 19 criteria (mean difference=0.46; Scale range -4 to +4) with effects ranging from little improvement in novelty (0.13, 95\% credible interval [-0.24, 0.49]) and creativity (0.22, [-0.14, 0.58]) to larger improvements in rigor of methodology (0.99, [0.62, 1.35]) and analysis (0.97, [0.60, 1.34]) and overall paper quality (0.66, [0.30, 1.02]). RRs could improve research quality while reducing publication bias and ultimately improve the credibility of the published literature.},
  keywords = {reports,transparency}
}

@misc{spinak_revistas_2019,
  title = {{Revistas que han aumentado el valor del APC han recibido m\'as art\'iculos}},
  author = {Spinak, Ernesto},
  year = {2019},
  journal = {Scielo en Perspectiva},
  abstract = {El Acceso Abierto (AA) a las publicaciones cient\'ificas online surgi\'o hace dos d\'ecadas. Entre las expectativas de su amplia adopci\'on, se consideraba la superaci\'on de la crisis presupuestal que afrontaban las universidades y otras instituciones educativas y de investigaci\'on debido al aumento constante de los precios de suscripci\'on por encima de la inflaci\'on. Es en este contexto internacional que surge la Red SciELO hace ya 20 a\~nos, proyecto pionero en su momento y hoy d\'ia el m\'as importante en el \'ambito de los pa\'ises en v\'ias de desarrollo, con m\'as de 1.650 revistas y 812.000 art\'iculos publicados en texto completo al momento de escribir este post. SciELO adopt\'o el Acceso Abierto con el objetivo de maximizar la visibilidad de las revistas y de las investigaciones que comunican.},
  language = {es}
}

@article{stamkou_Cultural_2019,
  title = {Cultural {{Collectivism}} and {{Tightness Moderate Responses}} to {{Norm Violators}}: {{Effects}} on {{Power Perception}}, {{Moral Emotions}}, and {{Leader Support}}},
  shorttitle = {Cultural {{Collectivism}} and {{Tightness Moderate Responses}} to {{Norm Violators}}},
  author = {Stamkou, Eftychia and {van Kleef}, Gerben A. and Homan, Astrid C. and Gelfand, Michele J. and {van de Vijver}, Fons J. R. and {van Egmond}, Marieke C. and Boer, Diana and Phiri, Natasha and Ayub, Nailah and Kinias, Zoe and Cantarero, Katarzyna and Efrat Treister, Dorit and Figueiredo, Ana and Hashimoto, Hirofumi and Hofmann, Eva B. and Lima, Renata P. and Lee, I-Ching},
  year = {2019},
  month = jun,
  journal = {Personality and Social Psychology Bulletin},
  volume = {45},
  number = {6},
  pages = {947--964},
  publisher = {{SAGE Publications Inc}},
  issn = {0146-1672},
  doi = {10.1177/0146167218802832},
  abstract = {Responses to norm violators are poorly understood. On one hand, norm violators are perceived as powerful, which may help them to get ahead. On the other hand, norm violators evoke moral outrage, which may frustrate their upward social mobility. We addressed this paradox by considering the role of culture. Collectivistic cultures value group harmony and tight cultures value social order. We therefore hypothesized that collectivism and tightness moderate reactions to norm violators. We presented 2,369 participants in 19 countries with a norm violation or a norm adherence scenario. In individualistic cultures, norm violators were considered more powerful than norm abiders and evoked less moral outrage, whereas in collectivistic cultures, norm violators were considered less powerful and evoked more moral outrage. Moreover, respondents in tighter cultures expressed a stronger preference for norm followers as leaders. Cultural values thus influence responses to norm violators, which may have downstream consequences for violators' hierarchical positions.},
  language = {en},
  keywords = {collectivism,leadership,moral emotions,norm violation,tightness}
}

@article{steneck_Fostering_2006,
  title = {Fostering Integrity in Research: {{Definitions}}, Current Knowledge, and Future Directions},
  shorttitle = {Fostering Integrity in Research},
  author = {Steneck, Nicholas H.},
  year = {2006},
  month = mar,
  journal = {Science and Engineering Ethics},
  volume = {12},
  number = {1},
  pages = {53--74},
  issn = {1471-5546},
  doi = {10.1007/PL00022268},
  abstract = {Over the last 25 years, a small but growing body of research on research behavior has slowly provided a more complete and critical understanding of research practices, particularly in the biomedical and behavioral sciences. The results of this research suggest that some earlier assumptions about irresponsible conduct are not reliable, leading to the conclusion that there is a need to change the way we think about and regulate research behavior. This paper begins with suggestions for more precise definitions of the terms ``responsible conduct of research,'' ``research ethics,'' and ``research integrity.'' It then summarizes the findings presented in some of the more important studies of research behavior, looking first at levels of occurrence and then impact. Based on this summary, the paper concludes with general observations about priorities and recommendations for steps to improve the effectiveness of efforts to respond to misconduct and foster higher standards for integrity in research.},
  language = {en},
  keywords = {practices}
}

@misc{stewart_Preregistration_2020,
  title = {Pre-Registration and {{Registered Reports}}: A {{Primer}} from {{UKRN}}},
  shorttitle = {Pre-Registration and {{Registered Reports}}},
  author = {Stewart, Suzanne and Rinke, Eike Mark and McGarrigle, Ronan and Lynott, Dermot and Lunny, Carole and Lautarescu, Alexandra and Galizzi, Matteo M. and Farran, Emily K. and Crook, Zander},
  year = {2020},
  month = oct,
  institution = {{OSF Preprints}},
  doi = {10.31219/osf.io/8v2n7},
  abstract = {Help reduce questionable research practices, and prevent selective reporting.},
  keywords = {Architecture,Arts and Humanities,Business,Education,Engineering,Law,Life Sciences,Medicine and Health Sciences,Physical Sciences and Mathematics,pre-analysis plan,pre-registration,preregistration,primer,primers,prospective registration,registered reports,registration,reproducibility,Social and Behavioral Sciences,UK Reproducibility Network,UKRN}
}

@article{stodden_Trust_2011,
  title = {Trust {{Your Science}}? {{Open Your Data}} and {{Code}}},
  shorttitle = {Trust {{Your Science}}?},
  author = {Stodden, Victoria C.},
  year = {2011},
  volume = {409},
  pages = {21--22},
  doi = {10.7916/D8CJ8Q0P},
  abstract = {This is a view on the reproducibility of computational sciences by Victoria Stodden. It contains information on the Reproducibility, Replicability, and Repeatability of code created by the other sciences. Stodden also talks about the rising prominence of computational sciences as we are in the digital age and what that means for the future of science and collecting data.},
  language = {en},
  keywords = {forrt}
}

@book{swan_directrices_2013,
  title = {Directrices Para Pol\'iticasde Desarrollo y Promoci\'on Del Acceso Abierto},
  author = {Swan, Alma},
  year = {2013},
  publisher = {{UNESCO}},
  isbn = {978-959-18-0928- 5}
}

@article{syed_Registered_2020,
  title = {Registered {{Reports With Developmental}} and {{Secondary Data}}: {{Some Brief Observations}} and {{Introduction}} to the {{Special Issue}}},
  shorttitle = {Registered {{Reports With Developmental}} and {{Secondary Data}}},
  author = {Syed, Moin and Donnellan, M. Brent},
  year = {2020},
  month = aug,
  journal = {Emerging Adulthood},
  volume = {8},
  number = {4},
  pages = {255--258},
  publisher = {{Sage Publications Inc}},
  address = {{Thousand Oaks}},
  issn = {2167-6968},
  doi = {10.1177/2167696820938529},
  abstract = {Registered reports are a relatively new type of journal article format in which the decision to publish an article is based on sound conceptualization, methods, and planned analyses rather than the specific nature of the results. Registered reports are becoming increasingly instituted in journals across the sciences but mostly in experimental contexts. Relatively few of these journals pertain directly to developmental research with adolescents, emerging adults, and adults, which tend to use more complex methods, or at least methods that involve a greater degree of flexibility. This article describes lessons learned through editing a special issue focused on registered reports based on analyses of a single existing data set, the Emerging Adulthood Measured and Multiple Institutions 2 project. These observations should be helpful for researchers interested in preparing registered report submissions using developmental and secondary data.},
  language = {English},
  keywords = {credibility,emerging   adulthood,open science,preregistration,registered reports,replication,revolution},
  annotation = {WOS:000548582500001}
}

@article{szollosi_Arrested_2021,
  title = {Arrested {{Theory Development}}: {{The Misguided Distinction Between Exploratory}} and {{Confirmatory Research}}},
  shorttitle = {Arrested {{Theory Development}}},
  author = {Szollosi, Aba and Donkin, Chris},
  year = {2021},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {16},
  number = {4},
  pages = {717--724},
  publisher = {{Sage Publications Ltd}},
  address = {{London}},
  issn = {1745-6916},
  doi = {10.1177/1745691620966796},
  abstract = {Science progresses by finding and correcting problems in theories. Good theories are those that help facilitate this process by being hard to vary: They explain what they are supposed to explain, they are consistent with other good theories, and they are not easily adaptable to explain anything. Here we argue that, rather than a lack of distinction between exploratory and confirmatory research, an abundance of flexible theories is a better explanation for the current replicability problems of psychology. We also explain why popular methods-oriented solutions fail to address the real problem of flexibility. Instead, we propose that a greater emphasis on theory criticism by argument might improve replicability.},
  language = {English},
  keywords = {confirmatory research,direct replication,exploratory research,philosophy of science,preregistration,theory development},
  annotation = {WOS:000620008500001}
}

@article{szollosi_Preregistration_2020,
  title = {Is {{Preregistration Worthwhile}}?},
  author = {Szollosi, Aba and Kellen, David and Navarro, Danielle J. and Shiffrin, Richard and {van Rooij}, Iris and Van Zandt, Trisha and Donkin, Chris},
  year = {2020},
  month = feb,
  journal = {Trends in Cognitive Sciences},
  volume = {24},
  number = {2},
  pages = {94--95},
  issn = {13646613},
  doi = {10.1016/j.tics.2019.11.009},
  language = {en},
  keywords = {forrt,reports}
}

@article{tackett_Bringing_2020a,
  title = {Bringing the ({{Pre}}){{Registration Revolution}} to {{Graduate Training}}},
  author = {Tackett, Jennifer L. and Brandes, Cassandra M. and Dworak, Elizabeth M. and Shields, Allison N.},
  year = {2020},
  month = nov,
  journal = {Canadian Psychology-Psychologie Canadienne},
  volume = {61},
  number = {4},
  pages = {299--309},
  publisher = {{Canadian Psychological Assoc}},
  address = {{Ottawa}},
  issn = {0708-5591},
  doi = {10.1037/cap0000221},
  abstract = {Preregistration, which involves documentation of hypotheses, methods, and plans for data analysis prior to data collection or analysis, has been lauded as 1 potential solution to the replication crisis in psychological science. Yet, many researchers have been slow to adopt preregistration, and the next generation of researchers is offered little formalized instruction in creating comprehensive preregistrations. In this article, we describe a collaborative workshop-based preregistration course designed and taught by Jennifer L. Tackett. We provide a brief overview of preregistration, including resources available, common concerns with preregistration, and responses to these concerns. We then describe the goals, structure, and evolution of our preregistration course and provide examples of enrolled students' final research products. We conclude with reflections on the strengths and opportunities for growth for the 1st iteration of this course and suggestions for others who are interested in implementing similar open science-focused courses in their training programs.},
  language = {English},
  keywords = {error,guide,incentives,open science,personality,preregistration,psychopathology,publication,registered-reports,registration,science,workshop-based learning},
  annotation = {WOS:000592853200005}
}

@article{tackett_Leveraging_2019,
  title = {Leveraging the {{Open Science Framework}} in {{Clinical Psychological Assessment Research}}},
  author = {Tackett, Jennifer L. and Brandes, Cassandra M. and Reardon, Kathleen W.},
  year = {2019},
  month = dec,
  journal = {Psychological Assessment},
  volume = {31},
  number = {12},
  pages = {1386--1394},
  publisher = {{Amer Psychological Assoc}},
  address = {{Washington}},
  issn = {1040-3590},
  doi = {10.1037/pas0000583},
  abstract = {The last decade has seen enormous advances in research transparency in psychology. One of these advances has been the creation of a common interface for openness across the sciences-the Open Science Framework (OSF). While social, personality, and cognitive psychologists have been at the fore in participating in open practices on the OSF, clinical psychology has trailed behind. In this article, we discuss the advantages and special considerations for clinical assessment researchers' participation in open science broadly, and specifically in using the OSF for these purposes. We use several studies from our lab to illustrate the uses of the OSF for psychological studies, as well as the process of implementing this tool in assessment research. Among these studies are an archival assessment study, a project using an extensive unpublished assessment battery. and one in which we developed a short-form assessment instrument.},
  language = {English},
  keywords = {childhood,metascience,Open Science Framework,personality,power,preregistration,replicability,step,traits,transparency,youth},
  annotation = {WOS:000498808600002}
}

@article{taylor_altmetric_2020,
  title = {An Altmetric Attention Advantage for Open Access Books in the Humanities and Social Sciences},
  author = {Taylor, Michael},
  year = {2020},
  journal = {Scientometrics},
  volume = {125},
  pages = {2523--2543},
  doi = {10.1007/s11192-020-03735-8},
  abstract = {The last decade has seen two significant phenomena emerge in research communication: the rise of open access (OA) publishing, and the easy availability of evidence of online sharing in the form of altmetrics. There has been limited examination of the effect of OA on online sharing for journal articles, and little for books. This paper examines the altmetrics of a set of 32,222 books (of which 5\% are OA) and a set of 220,527 chapters (of which 7\% are OA) indexed by the scholarly database Dimensions in the Social Sciences and Humanities. Both OA books and chapters have significantly higher use on social networks, higher coverage in the mass media and blogs, and evidence of higher rates of social impact in policy documents. OA chapters have higher rates of coverage on Wikipedia than their non-OA equivalents, and are more likely to be shared on Mendeley. Even within the Humanities and Social Sciences, disciplinary differences in altmetric activity are evident. The effect is confirmed for chapters, although sampling issues prevent the strong conclusion that OA facilitates extra attention at the whole book level, the apparent OA altmetrics advantage suggests that the move towards OA is increasing social sharing and broader impact.},
  language = {English}
}

@article{tennant_Ten_2019,
  title = {Ten {{Hot Topics}} around {{Scholarly Publishing}}},
  author = {Tennant, Jonathan P. and Crane, Harry and Crick, Tom and Davila, Jacinto and Enkhbayar, Asura and Havemann, Johanna and Kramer, Bianca and Martin, Ryan and Masuzzo, Paola and Nobes, Andy and Rice, Curt and {Rivera-L{\'o}pez}, B{\'a}rbara and {Ross-Hellauer}, Tony and Sattler, Susanne and Thacker, Paul D. and Vanholsbeeck, Marc},
  year = {2019},
  month = jun,
  journal = {Publications},
  volume = {7},
  number = {2},
  pages = {34},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/publications7020034},
  abstract = {The changing world of scholarly communication and the emerging new wave of \&lsquo;Open Science\&rsquo; or \&lsquo;Open Research\&rsquo; has brought to light a number of controversial and hotly debated topics. Evidence-based rational debate is regularly drowned out by misinformed or exaggerated rhetoric, which does not benefit the evolving system of scholarly communication. This article aims to provide a baseline evidence framework for ten of the most contested topics, in order to help frame and move forward discussions, practices, and policies. We address issues around preprints and scooping, the practice of copyright transfer, the function of peer review, predatory publishers, and the legitimacy of \&lsquo;global\&rsquo; databases. These arguments and data will be a powerful tool against misinformation across wider academic research, policy and practice, and will inform changes within the rapidly evolving scholarly publishing system.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  language = {en},
  keywords = {copyright,impact factor,open access,open science,peer review,research evaluation,scholarly communication,Scopus,web of science}
}

@article{thibodeaux_Production_2016,
  title = {Production as Social Change: {{Policy}} Sociology as a Public Good},
  shorttitle = {Production as Social Change},
  author = {Thibodeaux, Jarrett},
  year = {2016},
  month = may,
  journal = {Sociological Spectrum},
  volume = {36},
  number = {3},
  pages = {183--190},
  publisher = {{Routledge}},
  issn = {0273-2173},
  doi = {10.1080/02732173.2015.1102666},
  abstract = {Burawoy described two ways sociology can aid the public, through: (1) instrumental (policy) sociology and (2) reflexive (public) sociology. This article elaborates the different assumptions of how social change occurs according to policy and public sociology (and how sociology effects social change). Policy sociology assumes social change occurs through the scientific elaboration of the best means to achieve goals. However, policy sociology largely takes the public as an object of power rather than subjects who can utilize scientific knowledge. Public sociology assumes that social change occurs through the exposure of contradictions in goals, which elaborates better goals. However, the elaboration of contradictions assumes that there is a fundamental thesis/antithesis in society. If there are multiple goals/theses, public sociology fails in at least three ways. Policy sociology, when reflexively selecting its public, provides the best way sociology can aid the public.},
  annotation = {\_eprint: https://doi.org/10.1080/02732173.2015.1102666}
}

@article{tijdink_Publication_2014,
  title = {Publication {{Pressure}} and {{Scientific Misconduct}} in {{Medical Scientists}}},
  author = {Tijdink, Joeri K. and Verbeke, Reinout and Smulders, Yvo M.},
  year = {2014},
  month = dec,
  journal = {Journal of Empirical Research on Human Research Ethics},
  volume = {9},
  number = {5},
  pages = {64--71},
  publisher = {{Sage Publications Inc}},
  address = {{Thousand Oaks}},
  issn = {1556-2646},
  doi = {10.1177/1556264614552421},
  abstract = {There is increasing evidence that scientific misconduct is more common than previously thought. Strong emphasis on scientific productivity may increase the sense of publication pressure. We administered a nationwide survey to Flemish biomedical scientists on whether they had engaged in scientific misconduct and whether they had experienced publication pressure. A total of 315 scientists participated in the survey; 15\% of the respondents admitted they had fabricated, falsified, plagiarized, or manipulated data in the past 3 years. Fraud was more common among younger scientists working in a university hospital. Furthermore, 72\% rated publication pressure as too high. Publication pressure was strongly and significantly associated with a composite scientific misconduct severity score.},
  language = {English},
  keywords = {ethics,ethics in publishing,fraud,impact,integrity,publication pressure,questionable   research practice,science,scientific misconduct},
  annotation = {WOS:000344691700008}
}

@article{toth_Study_2021,
  title = {Study {{Preregistration}}: {{An Evaluation}} of a {{Method}} for {{Transparent Reporting}}},
  shorttitle = {Study {{Preregistration}}},
  author = {Toth, Allison A. and Banks, George C. and Mellor, David and O'Boyle, Ernest H. and Dickson, Ashleigh and Davis, Daniel J. and DeHaven, Alex and Bochantin, Jaime and Borns, Jared},
  year = {2021},
  month = aug,
  journal = {Journal of Business and Psychology},
  volume = {36},
  number = {4},
  pages = {553--571},
  publisher = {{Springer}},
  address = {{New York}},
  issn = {0889-3268},
  doi = {10.1007/s10869-020-09695-3},
  abstract = {Study preregistration promotes transparency in scientific research by making a clear distinction between a priori and post hoc procedures or analyses. Management and applied psychology have not embraced preregistration in the way other closely related social science fields have. There may be concerns that preregistration does not add value and prevents exploratory data analyses. Using a mixed-method approach, in Study 1, we compared published preregistered samples against published non-preregistered samples. We found that preregistration effectively facilitated more transparent reporting based on criteria (i.e., confirmed hypotheses and a priori analysis plans). Moreover, consistent with concerns that the published literature contains elevated type I error rates, preregistered samples had fewer statistically significant results (48\%) than non-preregistered samples (66\%). To learn about the perceived advantages, disadvantages, and misconceptions of study preregistration, in Study 2, we surveyed authors of preregistered studies and authors who had never preregistered a study. Participants in both samples had positive inclinations towards preregistration yet expressed concerns about the process. We conclude with a review of best practices for management and applied psychology stakeholders.},
  language = {English},
  keywords = {bias,incentives,management,Methodology,Open science,Preregistration,Questionable research   practices,questionable research practices,Reproducibility,science,truth},
  annotation = {WOS:000539933800001}
}

@article{ulloa_tendencias_2017,
  title = {Tendencias Paradigm\'aticas y T\'ecnicas Conversacionales En Investigaci\'on {{Cualitativa}} En {{Ciencias Sociales}}},
  author = {Ulloa, Jorge and Mardones, Rodolfo},
  year = {2017},
  journal = {Perspectivas de la Comunicaci\'on},
  series = {Universidad de La {{Frontera}}},
  volume = {10, n\textdegree 1},
  pages = {213--235},
  issn = {0718-4867},
  abstract = {El art\'iculo presenta los resultados de una investigaci\'on emp\'irica que busca describir el quehacer cient\'ifico cualitativo de alto impacto a trav\'es del an\'alisis de tres revistas indexadas en la base de datos Scopus entre los a\~nos 2013-2015. Para ello se construy\'o un corpus de 186 publicaciones reportando propuestas de investigaci\'on y/o resultados emp\'iricos de investigaci\'on. Se puso \'enfasis en el an\'alisis de la posici\'on paradigm\'atica de los autores, la explicitaci\'on y justificaci\'on de los objetivos de estudio y la utilizaci\'on de t\'ecnicas cualitativas conversacionales en su amplio rango, como las entrevistas en profundidad, historias de vida, grupos focales y grupos de discusi\'on. Los resultados dan cuenta que la mayor\'ia de los art\'iculos revisados no declaran abiertamente el paradigma desde el cual se posicionan, adem\'as se observa una multiplicidad de t\'ecnicas conversacionales, en las cuales destaca el uso de entrevistas. Se concluye que la adscripci\'on a alg\'un paradigma no es un asunto determinante a la hora de informar resultados, por cuanto estos forman parte de estudios m\'as amplios o, por su estructura son m\'as pragm\'aticos. Adem\'as, entre quienes declaran paradigmas de investigaci\'on, predomina una posici\'on constructivista. Por otro lado, el uso de t\'ecnicas conversacionales var\'ia en funci\'on de los objetivos de cada investigaci\'on, present\'andose en un amplio espectro, predomina el uso de entrevistas, y los grupos de discusi\'on y grupos focales son usados indistintamente.}
}

@inproceedings{unesco_declaracion_1999,
  title = {Declaraci\'on Sobre La Ciencia y El Uso Del Saber Cinet\'ifico},
  booktitle = {Conferencia Mundial Sobre La Ciencia},
  author = {UNESCO},
  year = {1999},
  address = {{Hungry - Budapest}}
}

@misc{universoabierto_rutas_2019,
  title = {Las 5 Rutas Para Llegar al Acceso Abierto: Verde, Dorada, Bronce, H\'ibrida y Diamante},
  author = {Universo Abierto},
  year = {2019},
  journal = {Blog de la biblioteca de Traducci\'on y Documentaci\'on de la Universidad de Salamanca}
}

@article{vanderzee_Open_2018,
  title = {Open {{Education Science}}},
  author = {{van der Zee}, Tim and Reich, Justin},
  year = {2018},
  month = jul,
  journal = {Aera Open},
  volume = {4},
  number = {3},
  publisher = {{Sage Publications Inc}},
  address = {{Thousand Oaks}},
  doi = {10.1177/2332858418787466},
  abstract = {Scientific progress is built on research that is reliable, accurate, and verifiable. The methods and evidentiary reasoning that underlie scientific claims must be available for scrutiny. Like other fields, the education sciences suffer from problems such as failure to replicate, validity and generalization issues, publication bias, and high costs of access to publications-all of which are symptoms of a nontransparent approach to research. Each aspect of the scientific cycle-research design, data collection, analysis, and publication-can and should be made more transparent and accessible. Open Education Science is a set of practices designed to increase the transparency of evidentiary reasoning and access to scientific research in a domain characterized by diverse disciplinary traditions and a commitment to impact in policy and practice. Transparency and accessibility are functional imperatives that come with many benefits for the individual researcher, scientific community, and society at large Open Education Science is the way forward.},
  language = {English},
  keywords = {articles,availability,bias,journals,open access,open science,preregistration,qualitative research,registered report,registered-reports,replication},
  annotation = {WOS:000509663300004}
}

@article{vantveer_Preregistration_2016,
  title = {Pre-Registration in Social Psychology\textemdash{{A}} Discussion and Suggested Template},
  author = {{van 't Veer}, Anna Elisabeth and {Giner-Sorolla}, Roger},
  year = {2016},
  month = nov,
  journal = {Journal of Experimental Social Psychology},
  series = {Special {{Issue}}: {{Confirmatory}}},
  volume = {67},
  pages = {2--12},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2016.03.004},
  abstract = {Pre-registration of studies before they are conducted has recently become more feasible for researchers, and is encouraged by an increasing number of journals. However, because the practice of pre-registration is relatively new to psychological science, specific guidelines for the content of registrations are still in a formative stage. After giving a brief history of pre-registration in medical and psychological research, we outline two different models that can be applied\textemdash reviewed and unreviewed pre-registration\textemdash and discuss the advantages of each model to science as a whole and to the individual scientist, as well as some of their drawbacks and limitations. Finally, we present and justify a proposed standard template that can facilitate pre-registration. Researchers can use the template before and during the editorial process to meet article requirements and enhance the robustness of their scholarly efforts.},
  language = {en},
  keywords = {Pre-registration,Research methods,Reviewed pre-registration (RPR),Solid science,Unreviewed pre-registration (UPR)}
}

@misc{velterop_suscripciones_2018,
  type = {{Scientific Blog}},
  title = {{De suscripciones y Tasas de Procesamiento de Art\'iculos}},
  author = {Velterop, Jan},
  year = {2018},
  journal = {Scielo en Perspectiva},
  abstract = {Estoy viendo m\'as y m\'as cr\'iticas a las Tasas de Procesamiento de Art\'iculos (Article Processing Charges \textendash{} APCs). Mientras que la mayor\'ia de ellas se preocupan por el monto de las tasas, tambi\'en hay algo de cr\'iticas al concepto mismo de las APC. Naturalmente, las tasas son a menudo altas. Demasiado altas, en particular en el caso de las revistas llamadas ``h\'ibridas''. M\'as sobre esto a continuaci\'on.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  language = {es}
}

@article{vermeulen_Blinded_2015,
  title = {Blinded by the {{Light}}: {{How}} a {{Focus}} on {{Statistical}} ``{{Significance}}'' {{May Cause}} p-{{Value Misreporting}} and an {{Excess}} of p-{{Values Just Below}} .05 in {{Communication Science}}},
  shorttitle = {Blinded by the {{Light}}},
  author = {Vermeulen, Ivar and Beukeboom, Camiel J. and Batenburg, Anika and Avramiea, Arthur and Stoyanov, Dimo and {van de Velde}, Bob and Oegema, Dirk},
  year = {2015},
  month = oct,
  journal = {Communication Methods and Measures},
  volume = {9},
  number = {4},
  pages = {253--279},
  publisher = {{Routledge}},
  issn = {1931-2458},
  doi = {10.1080/19312458.2015.1096333},
  abstract = {Publication bias promotes papers providing ``significant'' findings, thus incentivizing researchers to produce such findings. Prior studies suggested that researchers' focus on ``p {$<$} .05'' yields\textemdash intentional or unintentional\textemdash p-value misreporting, and excess p-values just below .05. To assess whether similar distortions occur in communication science, we extracted 5,834 test statistics from 693 recent communication science ISI papers, and assessed prevalence of p-values (1) misreported, and (2) just below .05. Results show 8.8\% of p-values were misreported (74.5\% too low). 1.3\% of p-values were critically misreported, stating p {$<$} .05 while in fact p {$>$} .05 (88.3\%) or vice versa (11.7\%). Analyzing p-value frequencies just below .05 using a novel method did not unequivocally demonstrate ``p-hacking''\textemdash excess p-values could be alternatively explained by (severe) publication bias. Results for 19,830 p-values from social psychology were strikingly similar. We conclude that publication bias, publication pressure, and verification bias distort the communication science knowledge base, and suggest solutions to this problem.},
  annotation = {\_eprint: https://doi.org/10.1080/19312458.2015.1096333}
}

@techreport{viscusi_Role_2014,
  type = {Working {{Paper}}},
  title = {The {{Role}} of {{Publication Selection Bias}} in {{Estimates}} of the {{Value}} of a {{Statistical Life}}},
  author = {Viscusi, W. Kip},
  year = {2014},
  month = may,
  series = {Working {{Paper Series}}},
  number = {20116},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w20116},
  abstract = {Meta-regression estimates of the value of a statistical life (VSL) controlling for publication selection bias yield bias-corrected estimates of VSL that are higher for labor market studies using the more recent Census of Fatal Occupational Injuries (CFOI) data. These results are borne out by the findings for four meta-analysis data sets and different formulations of the variable used to capture publication bias effects. Meta-regression estimates for a large sample of VSL estimates consisting only of results of labor market studies using the CFOI fatality data indicate publication selection bias effects that are not statistically significant in either fixed effects or random effects models with clustered standard errors. The confidence intervals of the publication bias-corrected estimates of the value of a statistical life sometimes include the sample mean estimates and always include the values that are currently used by government agencies.}
}

@article{vivalt_Heterogeneous_2015,
  title = {Heterogeneous {{Treatment Effects}} in {{Impact Evaluation}}},
  author = {Vivalt, Eva},
  year = {2015},
  month = may,
  journal = {American Economic Review},
  volume = {105},
  number = {5},
  pages = {467--470},
  issn = {0002-8282},
  doi = {10.1257/aer.p20151015},
  abstract = {It is very important to know how much we can extrapolate from a study's results. This paper examines the issue using data from impact evaluations in development.},
  language = {en},
  keywords = {Quantile Regressions,Single Equation Models,Single Variables: Cross-Sectional Models,Spatial Models,Treatment Effect Models}
}

@article{wagenmakers_CreativityVerification_2018,
  title = {The {{Creativity}}-{{Verification Cycle}} in {{Psychological Science}}: {{New Methods}} to {{Combat Old Idols}}},
  shorttitle = {The {{Creativity}}-{{Verification Cycle}} in {{Psychological Science}}},
  author = {Wagenmakers, Eric-Jan and Dutilh, Gilles and Sarafoglou, Alexandra},
  year = {2018},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {13},
  number = {4},
  pages = {418--427},
  publisher = {{Sage Publications Ltd}},
  address = {{London}},
  issn = {1745-6916},
  doi = {10.1177/1745691618771357},
  abstract = {Over the years, researchers in psychological science have documented and investigated a host of powerful cognitive fallacies, including hindsight bias and confirmation bias. Researchers themselves may not be immune to these fallacies and may unwittingly adjust their statistical analysis to produce an outcome that is more pleasant or better in line with prior expectations. To shield researchers from the impact of cognitive fallacies, several methodologists are now advocating preregistration-that is, the creation of a detailed analysis plan before data collection or data analysis. One may argue, however, that preregistration is out of touch with academic reality, hampering creativity and impeding scientific progress. We provide a historical overview to show that the interplay between creativity and verification has shaped theories of scientific inquiry throughout the centuries; in the currently dominant theory, creativity and verification operate in succession and enhance one another's effectiveness. From this perspective, the use of preregistration to safeguard the verification stage will help rather than hinder the generation of fruitful new ideas.},
  language = {English},
  keywords = {empirical cycle,philosophy of science,preregistration,replication},
  annotation = {WOS:000438605100002}
}

@article{warren_How_2019,
  title = {How {{Much Do You Have}} to {{Publish}} to {{Get}} a {{Job}} in a {{Top Sociology Department}}? {{Or}} to {{Get Tenure}}? {{Trends}} over a {{Generation}}},
  shorttitle = {How {{Much Do You Have}} to {{Publish}} to {{Get}} a {{Job}} in a {{Top Sociology Department}}?},
  author = {Warren, John Robert},
  year = {2019},
  month = feb,
  journal = {Sociological Science},
  volume = {6},
  pages = {172--196},
  issn = {2330-6696},
  doi = {10.15195/v6.a7},
  abstract = {Many sociologists suspect that publication expectations have risen over time\textemdash that how much graduate students have published to get assistant professor jobs and how much assistant professors have published to be promoted have gone up. Using information about faculty in 21 top sociology departments from the American Sociological Association's Guide to Graduate Departments of Sociology, online curricula vitae, and other public records, I provide empirical evidence to support this suspicion. On the day they start their first jobs, new assistant professors in recent years have already published roughly twice as much as their counterparts did in the early 1990s. Trends for promotion to associate professor are not as dramatic but are still remarkable. I evaluate several potential explanations for these trends and conclude that they are driven mainly by changes over time in the fiscal and organizational realities of universities and departments.},
  language = {en-US}
}

@article{weston_Recommendations_2019,
  title = {Recommendations for {{Increasing}} the {{Transparency}} of {{Analysis}} of {{Preexisting Data Sets}}},
  author = {Weston, Sara J. and Ritchie, Stuart J. and Rohrer, Julia M. and Przybylski, Andrew K.},
  year = {2019},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {3},
  pages = {214--227},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245919848684},
  abstract = {Secondary data analysis, or the analysis of preexisting data, provides a powerful tool for the resourceful psychological scientist. Never has this been more true than now, when technological advances enable both sharing data across labs and continents and mining large sources of preexisting data. However, secondary data analysis is easily overlooked as a key domain for developing new open-science practices or improving analytic methods for robust data analysis. In this article, we provide researchers with the knowledge necessary to incorporate secondary data analysis into their methodological toolbox. We explain that secondary data analysis can be used for either exploratory or confirmatory work, and can be either correlational or experimental, and we highlight the advantages and disadvantages of this type of research. We describe how transparency-enhancing practices can improve and alter interpretations of results from secondary data analysis and discuss approaches that can be used to improve the robustness of reported results. We close by suggesting ways in which scientific subfields and institutions could address and improve the use of secondary data analysis.},
  language = {en},
  keywords = {bias,file drawer,p-hacking,panel design,preexisting data,preregistration,reproducibility,secondary analysis,transparency}
}

@article{wilkinson_fair_2016,
  title = {The {{FAIR Guiding Principles}} for Scientific Data Management and Stewardship},
  author = {Wilkinson, Mark and Dumontier, Michel},
  year = {2016},
  volume = {3},
  number = {160018},
  doi = {10.1038/sdata.2016.18},
  abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders\textemdash representing academia, industry, funding agencies, and scholarly publishers\textemdash have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
  collaborator = {Aalbersberg, Ijsbrand and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Nikklas and Boiten, Jan-Willem and Bonino, Luiz and Bourne, Philip E and Bouwman, Jildau and Brookes, Anthony and Clark, Tim and Crosas, Merc{\`e} and Dillo, Ingrid and Dumon, Oliver and Edmunds, Scott and Evelo, Chris},
  copyright = {Creative Commons 3.0},
  language = {English}
}

@article{wilson_Replication_1973,
  title = {The {{Replication Problem}} in {{Sociology}}: {{A Report}} and a {{Suggestion}}*},
  shorttitle = {The {{Replication Problem}} in {{Sociology}}},
  author = {Wilson, Franklin D. and Smoke, Gale L. and Martin, J. David},
  year = {1973},
  journal = {Sociological Inquiry},
  volume = {43},
  number = {2},
  pages = {141--149},
  issn = {1475-682X},
  doi = {10.1111/j.1475-682X.1973.tb00711.x},
  abstract = {The deleterious effects of joint bias in favor of statistical inference and against replication are becoming well known. The acceptance of numerous Type I errors into the literature is by far the most serious of these. Data on the contents of three major journals support the contention that a joint bias for statistical significance tests, for rejections, and against replication exists in modern sociology. This finding replicates that of Sterling (1959) for psychology. A speculative analysis of the dynamics of publication decisions suggests that a compact format for reporting replications might make their publication more attractive to editors, and thus increase their frequency in the literature. A possible format for briefly reporting replication studies is suggested.},
  language = {en},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1475-682X.1973.tb00711.x}
}

@article{wingen_No_2020,
  title = {No {{Replication}}, {{No Trust}}? {{How Low Replicability Influences Trust}} in {{Psychology}}},
  shorttitle = {No {{Replication}}, {{No Trust}}?},
  author = {Wingen, Tobias and Berkessel, Jana B. and Englich, Birte},
  year = {2020},
  month = may,
  journal = {Social Psychological and Personality Science},
  volume = {11},
  number = {4},
  pages = {454--463},
  publisher = {{SAGE Publications Inc}},
  issn = {1948-5506},
  doi = {10.1177/1948550619877412},
  abstract = {In the current psychological debate, low replicability of psychological findings is a central topic. While the discussion about the replication crisis has a huge impact on psychological research, we know less about how it impacts public trust in psychology. In this article, we examine whether low replicability damages public trust and how this damage can be repaired. Studies 1\textendash 3 provide correlational and experimental evidence that low replicability reduces public trust in psychology. Additionally, Studies 3\textendash 5 evaluate the effectiveness of commonly used trust-repair strategies such as information about increased transparency (Study 3), explanations for low replicability (Study 4), or recovered replicability (Study 5). We found no evidence that these strategies significantly repair trust. However, it remains possible that they have small but potentially meaningful effects, which could be detected with larger samples. Overall, our studies highlight the importance of replicability for public trust in psychology.},
  language = {en},
  keywords = {crisis,open science,public trust,replicability,replication crisis}
}

@misc{wipo_frequently_2020,
  type = {Science {{Organization}}},
  title = {Frequently {{Asked Questions}}: {{IP Policies}} for {{Universities}} and {{Research Institutions}}},
  shorttitle = {{{FAQ}}},
  author = {WIPO},
  year = {2020},
  journal = {World Intelectual Property Organization},
  abstract = {WIPO is the global forum for intellectual property (IP) services, policy, information and cooperation. We are a self-funding agency of the United Nations, with 193 member states. Our mission is to lead the development of a balanced and effective international IP system that enables innovation and creativity for the benefit of all. Our mandate, governing bodies and procedures are set out in the WIPO Convention, which established WIPO in 1967},
  copyright = {Creative Commons 3.0},
  language = {English}
}

@book{worldintelectualpropietyorganization_what_2020,
  title = {What Is Intellectual Property?},
  shorttitle = {Whats Is {{IP}}?},
  author = {World Intelectual Propiety Organization, WIPO},
  year = {2020},
  edition = {WIPO},
  address = {{Switzerland}},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  isbn = {978-92-805-3176-3},
  language = {English}
}

@article{yamada_How_2018,
  title = {How to {{Crack Pre}}-Registration: {{Toward Transparent}} and {{Open Science}}},
  shorttitle = {How to {{Crack Pre}}-Registration},
  author = {Yamada, Yuki},
  year = {2018},
  month = sep,
  journal = {Frontiers in Psychology},
  volume = {9},
  pages = {1831},
  publisher = {{Frontiers Media Sa}},
  address = {{Lausanne}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2018.01831},
  language = {English},
  keywords = {academic publishing,misconduct in research,open   science,preregistration,qrp},
  annotation = {WOS:000445687200001}
}

@article{zenk-moltgen_Factors_2018,
  title = {Factors Influencing the Data Sharing Behavior of Researchers in Sociology and Political Science},
  author = {{Zenk-M{\"o}ltgen}, Wolfgang and Akdeniz, Esra and Katsanidou, Alexia and Na{\ss}hoven, Verena and Balaban, Ebru},
  year = {2018},
  month = jan,
  journal = {Journal of Documentation},
  volume = {74},
  number = {5},
  pages = {1053--1073},
  publisher = {{Emerald Publishing Limited}},
  issn = {0022-0418},
  doi = {10.1108/JD-09-2017-0126},
  abstract = {Purpose Open data and data sharing should improve transparency of research. The purpose of this paper is to investigate how different institutional and individual factors affect the data sharing behavior of authors of research articles in sociology and political science. Design/methodology/approach Desktop research analyzed attributes of sociology and political science journals (n=262) from their websites. A second data set of articles (n=1,011; published 2012-2014) was derived from ten of the main journals (five from each discipline) and stated data sharing was examined. A survey of the authors used the Theory of Planned Behavior to examine motivations, behavioral control, and perceived norms for sharing data. Statistical tests (Spearman's {$\rho$}, {$\chi$}2) examined correlations and associations. Findings Although many journals have a data policy for their authors (78 percent in sociology, 44 percent in political science), only around half of the empirical articles stated that the data were available, and for only 37 percent of the articles could the data be accessed. Journals with higher impact factors, those with a stated data policy, and younger journals were more likely to offer data availability. Of the authors surveyed, 446 responded (44 percent). Statistical analysis indicated that authors' attitudes, reported past behavior, social norms, and perceived behavioral control affected their intentions to share data. Research limitations/implications Less than 50 percent of the authors contacted provided responses to the survey. Results indicate that data sharing would improve if journals had explicit data sharing policies but authors also need support from other institutions (their universities, funding councils, and professional associations) to improve data management skills and infrastructures. Originality/value This paper builds on previous similar research in sociology and political science and explains some of the barriers to data sharing in social sciences by combining journal policies, published articles, and authors' responses to a survey.},
  keywords = {Data availability,Data policy,Data sharing,Political science,practices,Replication,Research data management,Research transparency,Sociology,Theory of Planned Behaviour}
}


